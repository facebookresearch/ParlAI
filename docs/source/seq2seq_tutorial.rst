..
  Copyright (c) 2017-present, Facebook, Inc.
  All rights reserved.
  This source code is licensed under the BSD-style license found in the
  LICENSE file in the root directory of this source tree. An additional grant
  of patent rights can be found in the PATENTS file in the same directory.

Creating an Agent
=================

In this tutorial, we'll be setting up an agent which learns from the data it
sees to produce the right answers.

For this agent, we'll be implementing a simple GRU Seq2Seq agent based on
Sequence to Sequence Learning with Neural Networks (Sutskever et al. 2014) and
Sean Robertson's `Seq2Seq PyTorch tutorial <http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>`_.


Part 1: Naming Things
^^^^^^^^^^^^^^^^^^^^^^^^^
    *"There are two hard problems in computer science:
    cache invalidation, naming things, and off-by-one errors."*

In order to make programmatic importing easier, we use a simple naming scheme
for our models, so that on the command line we can just type "--model seq2seq"
to load up the seq2seq model.

To this end, we create a folder under parlai/agents with the name seqseq, and
then put an empty __init__.py file there along with seq2seq.py.
Then, we name our agent "Seq2seqAgent".

This way, "--model seq2seq" can translate to "parlai.agents.seq2seq.seq2seq:Seq2seqAgent".
Underscores in the name become capitals in the class name: "--model local_human"
resides at "parlai.agents.local_human.local_human:LocalHumanAgent".
If you need to put a model at a different path, you can specify the full path
on the command line in the format above (with a colon in front of the class name).
For example, "--model parlai.agents.remote_agent.remote_agent:ParsedRemoteAgent".

Part 2: Main Agent Methods
^^^^^^^^^^^^^^^^^^^^^
First off, generally we should inherit from the Agent class in parlai.core.agents.
This provides us with some default implementations (often, ``pass``) of some utility
functions like "shutdown".

First let's focus on the primary functions to implement: ``__init__``, ``observe``, and ``act``.

The standard initialization parameters for agents are a dict of command-line parameters `opt`
and an optional dict of shared parameters called `shared`.

For our Seq2Seq model we'll call our parent init method, which does a few basic operations
like setting self.observation to None and creating a deep copy of the `opt` dict.

Then, we do a check to see if the `shared` parameter is set.
When it is not None, it's telling this instance to initialize with this particular
state, as this instance will be used either for batched or hogwild training
(depending on your preference). We'll take a quick digression to describe how
batching is set up:

Batching Example
----------------

Let's say we are training our seq2seq model on babi:task10k:1. What happens
behind the scenes for a batch size of 4 is that we actually create four shared
versions of the bAbI Task10k teacher, and four shared versions of the seq2seq
agent. These shared versions are initialized from the originals: for the bAbI
teachers, they inherit the data from their parent agent, but they each have
their own local state such as the current example they're showing or how far
through a bAbI episode they are (bAbI task 1 has five examples per episode).
For the seq2seq agent, each shared agent is keeping track of the previous
examples they've seen in this same episode, since each observation does not
repeat previously seen but related information--the agent has to remember it.

Observations are generated by calling the ``act`` function on each teacher, then
passing those observations to each agent by calling the ``observe`` function of the
shared agents. The agents are free to transform the previous observation
(for example, prepending previously seen text from the same episode, if applicable).
These transformed observations are packed into a list, which is then passed to
``batch_act`` function our agent implements. We implement ``batch_act`` differently
from the simple ``act`` function to take advantage of the effects of batching
over multiple examples when executing or updating our model.

Thus, since our  agent's shared-instances will only be used to keep track
of state particular to their sequence of examples in the batch, we have
barely anything when setting these shared instances up: we just initialize the
``self.episodeDone`` flag so we know whether we are in the middle of an episode or not.

The full initialization of the model is included further below, but is very
particular to this particular implementation. Let's talk more about the primary
agent functions we need to define first.

Observing and Acting
--------------------
Let's take a look at the ``observe`` function first. Here, we can modify the
observation dict if necessary, and then return it to be queued for batching.

In this version, we first make a deep copy of the observation. Then, if this is
not the first entry in an episode (some datasets like SQuAD have only one entry
for every episode, but others like bAbI have multiple), then we prepend the
previous text to the current text. We use a newline to separate them in case the
model wants to recognize the difference between different lines.

Then, we store whether this is the last entry in the episode so that we'll be
ready to reset next time if we need to.

.. code-block:: python

    def observe(self, observation):
        observation = copy.deepcopy(observation)
        if not self.episode_done:
            # if the last example wasn't the end of an episode, then we need to
            # recall what was said in that example
            prev_dialogue = self.observation['text']
            observation['text'] = prev_dialogue + '\n' + observation['text']
        self.observation = observation
        self.episode_done = observation['episode_done']
        return observation


Next up is the ``act`` function. Since we are going to implement a batched
version, we'll just call the batched version from our single-example act to
reduce code duplication. The performance hit here won't matter much since we'll
only use a batch size of one when debugging.

.. code-block:: python

    def act(self):
        # call batch_act with this batch of one
        return self.batch_act([self.observation])[0]


Now it's time for the batch_act function. This function gets a list of length
batchsize of observations and returns a list of the same length with this
agent's replies.

We'll follow this loose format:

1. Set up our list of dicts to send back as replies, with the agent's ID set.

2. Convert the incoming observations into tensors to feed into our model.

3. Produce predictions on the input text using the model. If labels were provided, update the model as well.

4. Unpack the predictions into the reply dicts and return them.

.. code-block:: python

    def batch_act(self, observations):
        batchsize = len(observations)
        # initialize a table of replies with this agent's id
        batch_reply = [{'id': self.getID()} for _ in range(batchsize)]

        # convert the observations into batches of inputs and targets
        # valid_inds tells us the indices of all valid examples
        # e.g. for input [{}, {'text': 'hello'}, {}, {}], valid_inds is [1]
        # since the other three elements had no 'text' field
        xs, ys, valid_inds = self.batchify(observations)

        if len(xs) == 0:
            # no valid examples, just return the empty responses we set up
            return batch_reply

        # produce prodictions either way, but use the targets if available
        if ys is not None:
            predictions = self.update(xs, ys)
        else:
            predictions = self.predict(xs)

        for i in range(len(predictions)):
            # map the predictions back to non-empty examples in the batch
            # we join with spaces since we produce tokens one at a time
            batch_reply[valid_inds[i]]['text'] = ' '.join(
                c for c in predictions[i] if c != self.EOS)

        return batch_reply

Since the implementation of ``batchify``, ``update``, and ``predict`` are
particular to our model, we'll table those for now. Next up, we'll cover some of
the other methods in the Agent API.

Part 3: Extended Agent API
^^^^^^^^^^^^^^^^^^^^^^^^^^

There are a few other useful methods you may want to define in your agent to
take of additional functionality one might want during training. Many of these
functions will be automatically called if you use our example training function
to train your model.

save()
------

This function saves a copy of your model. We recommended implementing it with
an optional parameter (path=None)--then, you can provide a specific path in
calls to the function if you want, but if this is None then check for a
'model_file' parameter in the `opt` dict.

This allows all models in an environment to save when the validation triggers
in the train_model function--by calling world.save(), all contained agents'
save() functions will be called, so they know to save their parameters if they
want to.

Our seq2seq agent defines the following:

.. code-block:: python

    def save(self, path=None):
        path = self.opt.get('model_file', None) if path is None else path

        if path:
            model = {}
            model['lt'] = self.lt.state_dict()
            model['encoder'] = self.encoder.state_dict()
            model['decoder'] = self.decoder.state_dict()
            model['d2o'] = self.d2o.state_dict()
            model['longest_label'] = self.longest_label

            with open(path, 'wb') as write:
                torch.save(model, write)
            torch.save(model)

shutdown()
----------
This function allows your model to do any final wrapup, such as writing any last
logging info, saving an end-state version of the model if desired, or closing
any open connections.

Our seq2seq model doesn't implement this, but the agents in parlai/agents/remote_agent
use this to close their open TCP connection after sending a shutdown signal through.


Part 4: Finishing the Seq2Seq model
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Here we'll take a look at the full details of ``__init__``, ``batchify``,
``update``, ``predict``, and more.

Full __init__()
-------------------

Here's the full code of the initialization of our model. While you might define
the model as a separate class if you prefer, we're going to define its modules
in-line here, since it's such a simple model.

.. code-block:: python

    class Seq2seqAgent(Agent):

        def __init__(self, opt, shared=None):
            # initialize defaults first
            super().__init__(opt, shared)
            if not shared:
                # this is not a shared instance of this class, so do full
                # initialization. if shared is set, only set up shared members.
                self.dict = DictionaryAgent(opt)
                self.id = 'Seq2Seq'
                # we use EOS markers to break input and output and end our output
                self.EOS = self.dict.eos_token
                self.observation = {'text': self.EOS, 'episode_done': True}
                self.EOS_TENSOR = torch.LongTensor(self.dict.parse(self.EOS))

                # store important params directly
                hsz = opt['hiddensize']
                self.hidden_size = hsz
                self.num_layers = opt['numlayers']
                self.learning_rate = opt['learningrate']
                self.longest_label = 1

                # set up modules
                self.criterion = nn.NLLLoss()
                # lookup table stores word embeddings
                self.lt = nn.Embedding(len(self.dict), hsz, padding_idx=0,
                                       scale_grad_by_freq=True)
                # encoder captures the input text
                self.encoder = nn.GRU(hsz, hsz, opt['numlayers'])
                # decoder produces our output states
                self.decoder = nn.GRU(hsz, hsz, opt['numlayers'])
                # linear layer helps us produce outputs from final decoder state
                self.d2o = nn.Linear(hsz, len(self.dict))
                # droput on the linear layer helps us generalize
                self.dropout = nn.Dropout(opt['dropout'])
                # softmax maps output scores to probabilities
                self.softmax = nn.LogSoftmax()

                # set up optims for each module
                lr = opt['learningrate']
                self.optims = {
                    'lt': optim.SGD(self.lt.parameters(), lr=lr),
                    'encoder': optim.SGD(self.encoder.parameters(), lr=lr),
                    'decoder': optim.SGD(self.decoder.parameters(), lr=lr),
                    'd2o': optim.SGD(self.d2o.parameters(), lr=lr),
                }

                # check for cuda
                self.use_cuda = not opt.get('no_cuda') and torch.cuda.is_available()
                if self.use_cuda:
                    print('[ Using CUDA ]')
                    torch.cuda.set_device(opt['gpu'])
                if self.use_cuda:
                    self.cuda()

                # load model parameters if available
                if opt.get('model_file') and os.path.isfile(opt['model_file']):
                    print('Loading existing model parameters from ' + opt['model_file'])
                    self.load(opt['model_file'])

            self.episode_done = True
