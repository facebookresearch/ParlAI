<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.16"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ParlAI: Engaging Image Captioning via Personality</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ParlAI
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.16 -->
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',false,false,'search.php','Search');
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Engaging Image Captioning via Personality </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>Kurt Shuster, Samuel Humeau, Hexiang Hu, Antoine Bordes, Jason Weston</p>
<p>Please see <a href="https://arxiv.org/abs/1810.10665">Shuster et al. (CVPR 2019)</a> for more details.</p>
<h1><a class="anchor" id="autotoc_md157"></a>
Abstract</h1>
<p>Standard image captioning tasks such as COCO and Flickr30k are factual, neutral in tone and (to a human) state the obvious (e.g., "a man playing a guitar"). While such tasks are useful to verify that a machine understands the content of an image, they are not engaging to humans as captions. With this in mind we define a new task, Personality-Captions, where the goal is to be as engaging to humans as possible by incorporating controllable style and personality traits. We collect and release a large dataset of 201,858 of such captions conditioned over 215 possible traits. We build models that combine existing work from (i) sentence representations (Mazare et al., 2018) with Transformers trained on 1.7 billion dialogue examples; and (ii) image representations (Mahajan et al., 2018) with ResNets trained on 3.5 billion social media images. We obtain state-of-the-art performance on Flickr30k and COCO, and strong performance on our new task. Finally, online evaluations validate that our task and models are engaging to humans, with our best model close to human performance.</p>
<h1><a class="anchor" id="autotoc_md158"></a>
Dataset</h1>
<p>The Personality-Captions dataset can be accessed via ParlAI, with <code>-t personality_captions</code>. See the <a href="http://www.parl.ai/static/docs/tutorial_quick.html">ParlAI quickstart for help</a>.</p>
<p>Additionally, the ParlAI MTurk tasks for data collection and human evaluation are <a href="https://github.com/facebookresearch/ParlAI/tree/master/parlai/mturk/tasks/personality_captions">made available</a> in ParlAI.</p>
<h1><a class="anchor" id="autotoc_md159"></a>
Leaderboards for Personality-Captions Task</h1>
<h2><a class="anchor" id="autotoc_md160"></a>
Retrieval Models</h2>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Model  </th><th class="markdownTableHeadNone">Paper  </th><th class="markdownTableHeadNone">Test R@1   </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">TransResNet, ResNeXt-IG-3.5B  </td><td class="markdownTableBodyNone"><a href="https://arxiv.org/abs/1810.10665">Shuster et al. (2019)</a>  </td><td class="markdownTableBodyNone">77.5   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">TransResNet, ResNet152  </td><td class="markdownTableBodyNone"><a href="https://arxiv.org/abs/1810.10665">Shuster et al. (2019)</a>  </td><td class="markdownTableBodyNone">51.7   </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">TransResNet, No Images  </td><td class="markdownTableBodyNone"><a href="https://arxiv.org/abs/1810.10665">Shuster et al. (2019)</a>  </td><td class="markdownTableBodyNone">25.8   </td></tr>
</table>
<h2><a class="anchor" id="autotoc_md161"></a>
Generative Models</h2>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Model  </th><th class="markdownTableHeadNone">Paper  </th><th class="markdownTableHeadNone">BLEU1  </th><th class="markdownTableHeadNone">BLEU4  </th><th class="markdownTableHeadNone">ROUGE-L  </th><th class="markdownTableHeadNone">CIDEr  </th><th class="markdownTableHeadNone">SPICE   </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">UpDown, ResNeXt-IG-3.5B  </td><td class="markdownTableBodyNone"><a href="https://arxiv.org/abs/1810.10665">Shuster et al. (2019)</a>  </td><td class="markdownTableBodyNone">44.0  </td><td class="markdownTableBodyNone">8.0  </td><td class="markdownTableBodyNone">27.4  </td><td class="markdownTableBodyNone">16.5  </td><td class="markdownTableBodyNone">5.2   </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">ShowAttTell, ResNeXt-IG-3.5B  </td><td class="markdownTableBodyNone"><a href="https://arxiv.org/abs/1810.10665">Shuster et al. (2019)</a>  </td><td class="markdownTableBodyNone">43.3  </td><td class="markdownTableBodyNone">7.1  </td><td class="markdownTableBodyNone">27.0  </td><td class="markdownTableBodyNone">12.6  </td><td class="markdownTableBodyNone">3.6   </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">ShotTell, ResNeXt-IG-3.5B  </td><td class="markdownTableBodyNone"><a href="https://arxiv.org/abs/1810.10665">Shuster et al. (2019)</a>  </td><td class="markdownTableBodyNone">38.4  </td><td class="markdownTableBodyNone">7.3  </td><td class="markdownTableBodyNone">24.3  </td><td class="markdownTableBodyNone">9.6  </td><td class="markdownTableBodyNone">1.6   </td></tr>
</table>
<h1><a class="anchor" id="autotoc_md162"></a>
Pretrained Models</h1>
<p>We provide our best model trained with ResNet152 image features. To evaluate the model, specify the following command: </p><pre class="fragment">  python examples/eval_model.py \
      -bs 128 -t personality_captions
      -mf models:personality_captions/transresnet/model
      --num-test-labels 5 -dt test
</pre><p>Which yields the following results: </p><pre class="fragment">  {'exs': 10000, 'accuracy': 0.5113, 'f1': 0.5951, 'hits@1': 0.511, 'hits@5': 0.816,
  'hits@10': 0.903, 'hits@100': 0.998, 'bleu': 0.4999, 'hits@1/100': 1.0,
  'loss': -0.002, 'med_rank': 1.0}
</pre><p>Additionally, we provide an interactive script that you can use to view outputs of our pretrained model. Simply run the following command: </p><pre class="fragment">  python projects/personality_captions/interactive.py \
  -mf models:personality_captions/transresnet/model
</pre><p>Which will allow you to upload an image and choose a personality for the model to use.</p>
<h1><a class="anchor" id="autotoc_md163"></a>
Model Examples</h1>
<p><img src="Examples.png" alt="" width="85%" class="inline"/></p>
<h1><a class="anchor" id="autotoc_md164"></a>
Citation</h1>
<p>If you use the dataset or models in your own work, please cite with the following BibText entry: </p><pre class="fragment">    @InProceedings{Shuster_2019_CVPR,
    author = {Shuster, Kurt and Humeau, Samuel and Hu, Hexiang and Bordes, Antoine and Weston, Jason},
    title = {Engaging Image Captioning via Personality},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2019}
    }
</pre> </div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.16
</small></address>
</body>
</html>
