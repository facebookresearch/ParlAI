<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.16"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ParlAI: README</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ParlAI
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.16 -->
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',false,false,'search.php','Search');
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">README </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This directory contains:</p><ul>
<li>pretrained transformers on Reddit and Wikipedia + Toronto Books that can be used as a base for pretraining</li>
<li>pretrained models fine tuned on ConvAI2 for the bi-encoder and poly-encoder The polyencoder scores 89+ on convai2 valid set and is fast enough to interact in real time with 100k candidates (which are provided.)</li>
</ul>
<h2><a class="anchor" id="autotoc_md90"></a>
Interacting with a pretrained model on Convai2</h2>
<p>Run this command: (assumes your model zoo is in the default ./data/models) </p><div class="fragment"><div class="line">python examples/interactive.py -m transformer/polyencoder \</div>
<div class="line">    -mf zoo:pretrained_transformers/model_poly/model \</div>
<div class="line">    --encode-candidate-vecs true \</div>
<div class="line">    --eval-candidates fixed  \</div>
<div class="line">    --fixed-candidates-path data/models/pretrained_transformers/convai_trainset_cands.txt</div>
</div><!-- fragment --><p>Example output: </p><div class="fragment"><div class="line">Enter Your Message: your persona: i love to drink fancy tea.\nyour persona: i have a big library at home.\nyour persona: i&#39;m a museum tour guide.\nhi how are you doing ?</div>
<div class="line">[Polyencoder]: i am alright . i am back from the library .</div>
<div class="line">Enter Your Message: oh, what do you do for a living?</div>
<div class="line">[Polyencoder]: i work at the museum downtown . i love it there .</div>
<div class="line">Enter Your Message: what is your favorite drink?</div>
<div class="line">[Polyencoder]: i am more of a tea guy . i get my tea from china .</div>
</div><!-- fragment --><p>Note the polyencoder gives 89+ hits@1/20 on convai2, however, it expects data that is close to the dataset. If you do not include the multiple 'your persona: ...<br  />
' at the beginning it will answer nonsense.</p>
<h2><a class="anchor" id="autotoc_md91"></a>
Fine tuning on your own tasks</h2>
<h3><a class="anchor" id="autotoc_md92"></a>
bi-encoder</h3>
<p>Execute this to train a biencoder scoring 86+ on Convai2 valid set (requires 8 x GPU 32GB., If you don't have this, reduce the batch size )</p>
<div class="fragment"><div class="line">python -u examples/train_model.py \</div>
<div class="line">    --init-model zoo:pretrained_transformers/bi_model_huge_reddit/model \</div>
<div class="line">    --batchsize 512 -pyt convai2 \</div>
<div class="line">    --shuffle true --model transformer/biencoder --eval-batchsize 6 \</div>
<div class="line">    --warmup_updates 100 --lr-scheduler-patience 0 \</div>
<div class="line">    --lr-scheduler-decay 0.4 -lr 5e-05 --data-parallel True \</div>
<div class="line">    --history-size 20 --label-truncate 72 --text-truncate 360 \</div>
<div class="line">    --num-epochs 10.0 --max_train_time 200000 -veps 0.5 -vme 8000 \</div>
<div class="line">    --validation-metric accuracy --validation-metric-mode max \</div>
<div class="line">    --save-after-valid True --log_every_n_secs 20 --candidates batch \</div>
<div class="line">    --dict-tokenizer bpe --dict-lower True --optimizer adamax \</div>
<div class="line">    --output-scaling 0.06 \</div>
<div class="line">     --variant xlm --reduction-type mean --share-encoders False \</div>
<div class="line">     --learn-positional-embeddings True --n-layers 12 --n-heads 12 \</div>
<div class="line">     --ffn-size 3072 --attention-dropout 0.1 --relu-dropout 0.0 --dropout 0.1 \</div>
<div class="line">     --n-positions 1024 --embedding-size 768 --activation gelu \</div>
<div class="line">     --embeddings-scale False --n-segments 2 --learn-embeddings True \</div>
<div class="line">     --share-word-embeddings False --dict-endtoken __start__ --fp16 True \</div>
<div class="line">     --model-file &lt;YOUR MODEL FILE&gt;</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md93"></a>
poly-encoder</h3>
<p>Execute this to train a poly-encoder scoring 89+ on Convai2 valid set (requires 8 x GPU 32GB., If you don't have this, reduce the batch size )</p>
<div class="fragment"><div class="line">python -u examples/train_model.py \</div>
<div class="line">  --init-model zoo:pretrained_transformers/poly_model_huge_reddit/model \</div>
<div class="line">  -pyt convai2 --shuffle true \</div>
<div class="line">  --model transformer/polyencoder --batchsize 256 --eval-batchsize 10 \</div>
<div class="line">  --warmup_updates 100 --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 \</div>
<div class="line">  -lr 5e-05 --data-parallel True --history-size 20 --label-truncate 72 \</div>
<div class="line">  --text-truncate 360 --num-epochs 8.0 --max_train_time 200000 -veps 0.5 \</div>
<div class="line">  -vme 8000 --validation-metric accuracy --validation-metric-mode max \</div>
<div class="line">  --save-after-valid True --log_every_n_secs 20 --candidates batch --fp16 True \</div>
<div class="line">  --dict-tokenizer bpe --dict-lower True --optimizer adamax --output-scaling 0.06 \</div>
<div class="line">  --variant xlm --reduction-type mean --share-encoders False \</div>
<div class="line">  --learn-positional-embeddings True --n-layers 12 --n-heads 12 --ffn-size 3072 \</div>
<div class="line">  --attention-dropout 0.1 --relu-dropout 0.0 --dropout 0.1 --n-positions 1024 \</div>
<div class="line">  --embedding-size 768 --activation gelu --embeddings-scale False --n-segments 2 \</div>
<div class="line">  --learn-embeddings True --polyencoder-type n_first --poly-n-codes 64 \</div>
<div class="line">  --poly-attention-type basic --dict-endtoken __start__ \</div>
<div class="line">  --model-file &lt;YOUR MODEL FILE&gt;</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md94"></a>
cross-encoder</h3>
<p>Execute this to train a cross-encoder scoring 90+ on Convai2 valid set (requires 8 x GPU 32GB., If you don't have this, reduce the batch size )</p>
<div class="fragment"><div class="line">python -u examples/train_model.py \</div>
<div class="line">  --init-model zoo:pretrained_transformers/cross_model_huge_reddit/model \</div>
<div class="line">  -pyt convai2 --shuffle true \</div>
<div class="line">  --model transformer/crossencoder --batchsize 16 --eval-batchsize 10 \</div>
<div class="line">  --warmup_updates 1000 --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 \</div>
<div class="line">  -lr 5e-05 --data-parallel True --history-size 20 --label-truncate 72 \</div>
<div class="line">  --text-truncate 360 --num-epochs 12.0 --max_train_time 200000 -veps 0.5 \</div>
<div class="line">  -vme 2500 --validation-metric accuracy --validation-metric-mode max \</div>
<div class="line">  --save-after-valid True --log_every_n_secs 20 --candidates inline --fp16 True \</div>
<div class="line">  --dict-tokenizer bpe --dict-lower True --optimizer adamax --output-scaling 0.06 \</div>
<div class="line">  --variant xlm --reduction-type first --share-encoders False \</div>
<div class="line">  --learn-positional-embeddings True --n-layers 12 --n-heads 12 --ffn-size 3072 \</div>
<div class="line">  --attention-dropout 0.1 --relu-dropout 0.0 --dropout 0.1 --n-positions 1024 \</div>
<div class="line">  --embedding-size 768 --activation gelu --embeddings-scale False --n-segments 2 \</div>
<div class="line">  --learn-embeddings True --dict-endtoken __start__ \</div>
<div class="line">  --model-file &lt;YOUR MODEL FILE&gt;</div>
</div><!-- fragment --> </div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.16
</small></address>
</body>
</html>
