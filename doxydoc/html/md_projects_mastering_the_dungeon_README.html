<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ParlAI: Mastering the Dungeon</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ParlAI
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Mastering the Dungeon </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This project contains the code we used in our paper:</p>
<p><a href="https://arxiv.org/abs/1711.07950">Mastering the Dungeon: Grounded Language Learning by Mechanical Turker Descent</a></p>
<p>Zhilin Yang, Saizheng Zhang, Jack Urbanek, Will Feng, Alexander H. Miller, Arthur Szlam, Douwe Kiela, Jason Weston</p>
<h2>Requirements</h2>
<p>Python 3.6, PyTorch 0.2, spacy</p>
<p>To install <code>spacy</code> and download related packages: </p><div class="fragment"><div class="line">python -m pip install spacy</div><div class="line">python -m spacy.en.download all</div></div><!-- fragment --><h2>Get the Data</h2>
<p>Go to the ParlAI root directory. Create a data directory if it does not exist. </p><div class="fragment"><div class="line">mkdir data</div></div><!-- fragment --><p>Then we can download the data </p><div class="fragment"><div class="line">cd data</div><div class="line">wget http://parl.ai/downloads/mastering_the_dungeon/mastering_the_dungeon.tgz</div><div class="line">tar -xvzf mastering_the_dungeon.tgz</div></div><!-- fragment --><h3>Data Organization</h3>
<p>The dataset is organized as follows. <code>data/graph_world2</code> contains the pilot study data, where each file ending with <code>.pkl</code> is a pickle file of an example. <code>data/graph_world2_v*_r*</code> contains a pickle file <code>filtered_data_list.pkl</code> storing the data collected in a specific round of a specific setting. The number after <code>r</code> indicates the round index. The number after <code>v</code> indicates the setting, where <code>v13</code> means <code>MTD limit</code>, <code>v14</code> means <code>MTD</code>, <code>v15</code> means <code>MTD limit w/o model</code>, and <code>BASELINE_2</code> means the baseline. For example, <code>data/graph_world2_v15_r2/filtered_data_list.pkl</code> contains the data collected in the second round of the setting <code>MTD limit w/o model</code>.</p>
<h3>Splitting the Data</h3>
<p>We split the dataset before doing training and evaluation. Assuming you are under the ParlAI root directory, </p><div class="fragment"><div class="line">cd projects/mastering_the_dungeon/mturk/tasks/MTD</div><div class="line">python run.py --split</div></div><!-- fragment --><p>This will split the data from different rounds under different settings into training and test sets, while ensuring the number of training examples to be the same.</p>
<h2>Training and Evaluation</h2>
<p>There are three steps: 1) start a GPU placeholder; 2) run the training and evaluation jobs; 3) terminate the GPU placeholder. We will illustrate the usage first on one single GPU, and later on a cluster with Slurm installed.</p>
<h3>Start GPU Placeholder</h3>
<p>Open a new window (e.g. a screen session), and go to the ParlAI root directory. </p><div class="fragment"><div class="line">cd projects/mastering_the_dungeon/projects/graph_world2</div><div class="line">python train.py --job_num 0</div></div><!-- fragment --><p>This will start a GPU placeholder that accepts new training jobs on the fly.</p>
<h3>Run Training Jobs</h3>
<p>Open another window (e.g. another screen session), and go to the ParlAI root directory.</p>
<p>#### Training </p><div class="fragment"><div class="line">cd projects/mastering_the_dungeon/mturk/tasks/MTD</div><div class="line">python run.py --train [--seq2seq]</div></div><!-- fragment --><p>This will train the models using AC-Seq2Seq (by default) or Seq2Seq (when seq2seq is specified).</p>
<p>#### Evaluation </p><div class="fragment"><div class="line">cd projects/mastering_the_dungeon/mturk/tasks/MTD</div><div class="line">python run.py --eval [--seq2seq] [--constrain]</div></div><!-- fragment --><p>This will evaluate the models just trained. The option <code>constrain</code> indicates using constrained decoding or not; turning it on is recommended.</p>
<p>#### Breakdown by Round </p><div class="fragment"><div class="line">cd projects/mastering_the_dungeon/mturk/tasks/MTD</div><div class="line">python run.py --rounds_breakdown [--seq2seq]</div></div><!-- fragment --><p>In this experiment, we will see the performance of agents in different rounds.</p>
<p>#### Breakdown by Dataset </p><div class="fragment"><div class="line">cd projects/mastering_the_dungeon/mturk/tasks/MTD</div><div class="line">python run.py --data_breakdown [--seq2seq]</div></div><!-- fragment --><p>In this experiment, we will see the performance of agents trained on one dataset and evaluated on another dataset.</p>
<p>#### Ablation Study </p><div class="fragment"><div class="line">cd projects/mastering_the_dungeon/mturk/tasks/MTD</div><div class="line">python run.py --ablation</div></div><!-- fragment --><p>This will do the ablation study by considering removing the counter feature and the room embeddings.</p>
<h3>Terminate GPU Placeholder</h3>
<p>After all training and evaluation jobs are finished, we can now terminate the placeholders. Go back to the placeholder window, and press <code>Ctrl+C</code> to terminate the process.</p>
<h3>Running on GPU Cluster</h3>
<p>There are a few changes to be made when running on a GPU cluster. In this section we assume <a href="https://slurm.schedmd.com/">Slurm</a> is installed on the cluster.</p>
<p>When creating the GPU placeholder, we use the following commands: </p><div class="fragment"><div class="line">cd projects/mastering_the_dungeon/projects/graph_world2</div><div class="line">python gen_sbatch_script.py --num_gpus &lt;num_gpus&gt; --slurm</div><div class="line">./batch_holder.sh</div></div><!-- fragment --><p> where <code>num_gpus</code> is the number of GPUs to use.</p>
<p>For training and evaluation, we need to add an additional option <code>--num_machines</code> to every command. For example, training now becomes: </p><div class="fragment"><div class="line">cd projects/mastering_the_dungeon/mturk/tasks/MTD</div><div class="line">python run.py --train --num_machines &lt;num_gpus&gt; [--seq2seq]</div></div><!-- fragment --><p>To terminate the GPU placeholder, simply cancel all jobs (this assumes you are not running other jobs using Slurm) </p><div class="fragment"><div class="line">scancel -u &lt;my_username_on_slurm&gt;</div></div><!-- fragment --> </div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
