<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ParlAI: Self-feeding Chatbot</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ParlAI
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Self-feeding Chatbot </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Paper information</h2>
<p>Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazaré, Jason Weston. <em><a href="https://arxiv.org/abs/1901.05415">Learning from Dialogue after Deployment: Feed Yourself, Chatbot!</a></em>. To appear in ACL 2019.</p>
<h2>Abstract</h2>
<p>The majority of conversations a dialogue agent sees over its lifetime occur after it has already been trained and deployed, leaving a vast store of potential training signal untapped. In this work, we propose the self-feeding chatbot, a dialogue agent with the ability to extract new training examples from the conversations it participates in. As our agent engages in conversation, it also estimates user satisfaction in its responses. When the conversation appears to be going well, the user’s responses become new training examples to imitate. When the agent believes it has made a mistake, it asks for feedback; learning to predict the feedback that will be given improves the chatbot’s dialogue abilities further. On the PersonaChat chit-chat dataset with over 131k training examples, we find that learning from dialogue with a self- feeding chatbot significantly improves performance, regardless of the amount of traditional supervision.</p>
<h2>Citation</h2>
<p>If you use the dataset or models in your own work, please cite with the following BibTex entry: </p><pre class="fragment">@inproceedings{hancock2019feed,
  author={Braden Hancock and Antoine Bordes and Pierre-Emmanuel Mazar\'{e} and Jason Weston},
  booktitle={Association for Computational Linguistics (ACL)},
  title={Learning from Dialogue after Deployment: Feed Yourself, Chatbot!},
  url={https://arxiv.org/abs/1901.05415},
  year={2019},
}
</pre><h1>Code Instructions</h1>
<p>Once you have <a href="https://github.com/facebookresearch/ParlAI/#installing-parlai">installed ParlAI</a>, follow the instructions below.</p>
<h2>Download the data</h2>
<p>Running the commands to train or chat with the models will automatically download the data for you. Alternatively, you can manually download the data by running <code>python <a class="el" href="download__data_8py.html">projects/self_feeding/download_data.py</a></code>. This will download the following files to <code>data/self_feeding/</code>:</p>
<ul>
<li><code>{train, valid, test}_hh.txt</code>: DIALOGUE Human-Human (HH) conversations from the PersonaChat dataset, with one context and response per line (train: 131,438; valid: 2,000; test: 5,801).</li>
<li><code>train_hb.txt</code>: DIALOGUE Human-Bot (HB) conversations collected between crowdworkers and a trained chatbot, with only human utterances as responses (train: 131,923).</li>
<li><code>train_fb_a.txt</code>: FEEDBACK Human-Bot conversations wherein all responses are the feedback given by a human in response to a request by the bot after it estimated that the human was dissatisfied with its previous response. (The turns where the bot messed up, the human expressed dissatisfaction, and the bot requested feedback are removed so that the context is primarily normal-looking conversation). (train: 40,082)</li>
<li><code>train_fb_b.txt</code>: The same as <code>train_fb_a.txt</code> but with a chatbot that was retrained using the additional feedback examples collected from the A set (train: 21,257).</li>
<li><code>{valid, test}_fb.txt</code>: FEEDBACK validation and test sets collected at the same time and with the same model as the <code>train_fb_a.txt</code> file.</li>
</ul>
<p>We also include three derivative files for convenience (as they were used in experiments and in some of the sample commands in the sections below):</p>
<ul>
<li><code>train_fb.txt</code>: The result of <code>cat train_fb_a.txt train_fb_b.txt | shuf &gt; train_fb.txt</code></li>
<li><code>train_hb60k.txt</code>: The result of <code>head -n 60000 train_hb.txt &gt; train_hb60k.txt</code></li>
<li><code>train_hh131k_hb60k.txt</code>: The result of <code>cat train_hh.txt train_hb60k.txt &gt; train_hh131k_hb60k.txt</code></li>
</ul>
<p>For more context on the scenarios in which these data were collected (including screenshots of crowdworker interfaces), refer to the paper. In this distribution, we include all data collected of each type. To recreate the exact datasets used in the paper, keep only the first X lines of each file such that the resulting sets match the sizes reported in Table 1.</p>
<h2>Train a model</h2>
<p>To train a model, use the standard <code>ParlAI</code> protocol with <code>train_model.py</code>. The following commands assume that you have set the following environment variables:</p>
<div class="fragment"><div class="line">export PARLAIHOME=/path/to/ParlAI</div><div class="line">export MODEL=/path/to/model</div></div><!-- fragment --><p>You may require a GPU to train a model to convergence in a reasonable amount of time. On a P100 GPU, these training commands take approximately 10 minutes to converge.</p>
<h3>Train on the DIALOGUE (HH) examples</h3>
<p>Here is a minimal command for training on the DIALOGUE task using Human-Human (HH) examples:</p>
<div class="fragment"><div class="line">python examples/train_model.py -t self_feeding:dialog --model projects.self_feeding.self_feeding_agent:SelfFeedingAgent --model-file /tmp/mymodel1 -bs 128</div></div><!-- fragment --><p>Or to recreate the results in the paper for training on 131k HH examples with the same hyperparameters that we used, run the following:</p>
<div class="fragment"><div class="line">python examples/train_model.py -t self_feeding:dialog --model-file /tmp/mymodel2 -ltim 5 -vtim 10 -vp 10 -m projects.self_feeding.self_feeding_agent:SelfFeedingAgent -cands batch --eval-candidates inline -histsz 2 --embedding-type fasttext_cc --embedding-size 300 --dict-maxtokens 250000 --num-epochs 100 --optimizer adamax --embeddings-scale false -bs 128 --relu-dropout 0 --attention-dropout 0 --n-heads 2 --n-layers 2 -lr 0.0025 --ffn-size 32 --lr-scheduler invsqrt --warmup-updates 500 -vmt dia_acc -vmm max</div></div><!-- fragment --><h3>Train on DIALOGUE (HH) + DIALOGUE (HB) examples</h3>
<p>To train on both HH and HB DIALOGUE examples, point the model to a train file that includes examples from both sets. For example, if you combined 131k HH DIALOGUE examples and 60k HB dialogue examples into a file called <code>train_hh131k_hb60k.txt</code>, you could add the following flag to train on that combined file for the DIALOGUE task:</p>
<div class="fragment"><div class="line">--dia-train train_hh131k_hb60k.txt</div></div><!-- fragment --><h3>Train on DIALOGUE (HH) + FEEDBACK examples</h3>
<p>To train on more than one task (such as DIALOGUE and FEEDBACK), modify the command for training on DIALOGUE (HH) alone as follows:</p>
<ul>
<li>Change <code>-t <a class="el" href="namespaceself__feeding.html">self_feeding</a>:dialog</code> to <code>-t <a class="el" href="namespaceself__feeding.html">self_feeding</a>:diafee</code>. This will result in a different "teacher" agent being used to train the chatbot, one with access to both 'dia[logue]' and 'fee[dback]`.</li>
</ul>
<p>Putting this all together, the command to recreate the 131k HH + 60k FB result from the paper is as follows (as reported in Table 9 in the paper, this setting had the same optimal hyperparameter settings as 131k HH):</p>
<div class="fragment"><div class="line">python examples/train_model.py -t self_feeding:diafee --model-file /tmp/mymodel3 -ltim 5 -vtim 10 -vp 10 -m projects.self_feeding.self_feeding_agent:SelfFeedingAgent -cands batch --eval-candidates inline -histsz 2 --embedding-type fasttext_cc --embedding-size 300 --dict-maxtokens 250000 --num-epochs 100 --optimizer adamax --embeddings-scale false -bs 128 --relu-dropout 0 --attention-dropout 0 --n-heads 2 --n-layers 2 -lr 0.0025 --ffn-size 32 --lr-scheduler invsqrt --warmup-updates 500 -vmt dia_acc -vmm max</div></div><!-- fragment --><h3>Train on DIALOGUE (HH) + DIALOGUE (HB) + FEEDBACK (FB) + SATISFACTION (ST) examples</h3>
<p>You can train on all three tasks at once with the command below. </p><div class="fragment"><div class="line">python examples/train_model.py -t self_feeding:all --model-file /tmp/mymodel4 -ltim 5 -vtim 10 -vp 50 -m projects.self_feeding.self_feeding_agent:SelfFeedingAgent -cands batch --eval-candidates inline -histsz 2 --embedding-type fasttext_cc --embedding-size 300 --dict-maxtokens 250000 --num-epochs 500 --optimizer adamax --embeddings-scale false -bs 128 --relu-dropout 0 --attention-dropout 0 --n-heads 2 --n-layers 2 -lr 0.0025 --ffn-size 32 --lr-scheduler invsqrt --warmup-updates 500 --dia-train train_hh131k_hb60k.txt -vmt dia_acc -vmm max</div></div><!-- fragment --><h3>Evaluate a trained model</h3>
<p>To evaluate a model, use the following command, which specifies which teacher to use (the one with all three tasks), which splits to test on (<code>test</code> or <code>valid</code>), and what batch size to use (larger will evaluate faster): </p><div class="fragment"><div class="line">python examples/eval_model.py -mf /tmp/mymodel1 -t self_feeding:all --datatype test -bs 20</div></div><!-- fragment --><h2>Using a pretrained Model</h2>
<p>Running any of the following commands will automatically download a pretrained model and place it in <code>data/models/self_feeding</code>. You can then evaluate this model on the corpus, or even chat with it live!</p>
<h3>Evaluating the pretrained model</h3>
<p>You can</p>
<div class="fragment"><div class="line">python examples/eval_model.py -mf zoo:self_feeding/hh131k_hb60k_fb60k_st1k/model -t self_feeding:all --datatype test -bs 20</div></div><!-- fragment --><h3>Chat with a pretrained model</h3>
<p>To chat with a model that's already been trained, use the <code>interactive.py</code> script. You can add the flag <code>--request-feedback true</code> to have the model ask for feedback based on its estimate of your satisfaction with the conversation. </p><div class="fragment"><div class="line">python projects/self_feeding/interactive.py --model-file zoo:self_feeding/hh131k_hb60k_fb60k_st1k/model --no-cuda</div></div><!-- fragment --><p>You can change the filename to any of your own models to interactive with a model you have trained. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
