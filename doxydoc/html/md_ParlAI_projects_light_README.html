<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.16"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ParlAI: &lt;img width=&quot;5%&quot; src=&quot;scribe.png&quot;/&gt; LIGHT</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ParlAI
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.16 -->
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',false,false,'search.php','Search');
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title"><img src="scribe.png" alt="" width="5%" class="inline"/> LIGHT </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2><a class="anchor" id="autotoc_md134"></a>
Learning in Interactive Games with Humans and Text</h2>
<p><img src="tavern.png" alt="" width="90%" class="inline"/></p>
<p>The LIGHT project is a large-scale fantasy text adventure game research platform for training agents that can both talk and act, interacting either with other models or with humans.</p>
<h1><a class="anchor" id="autotoc_md135"></a>
Abstract</h1>
<p>We introduce a large-scale crowdsourced text adventure game as a research platform for studying grounded dialogue. In it, agents can both perceive, emote and act whilst conducting dialogue with other agents; models and humans can both act as characters within the game. We describe the results of training state-of-the-art generative and retrieval models in this setting. We show that in addition to using past dialogue, these models are able to effectively use the state given by the underlying world. In particular, we show that ground-ing on the details of the local environment, including location descriptions and the objects (and affordances of those objects) and characters (and their previous actions) present within it allows better predictions of agent behavior and dialogue. We analyze the ingredients necessary for successful grounding in this setting, and how each of these factors relate to agents that can talk and act successfully.</p>
<p><img src="example-dialog.png" alt="" width="90%" class="inline"/></p>
<h1><a class="anchor" id="autotoc_md136"></a>
Paper</h1>
<p>A detailed description may be found in <a href="https://arxiv.org/abs/1903.03094">Urbanek et al., 2019</a>.</p>
<h1><a class="anchor" id="autotoc_md137"></a>
Datasets</h1>
<p>LIGHT currently features 663 locations, 3462 objects and 1755 character types, described entirely in natural language. Within that game world, we collect 11,000 episodes of character interactions (talking and acting).</p>
<p>You can view the data or train your own ParlAI agent on the LIGHT tasks with <code>-t light_dialog</code>. See the <a href="http://www.parl.ai/static/docs/tutorial_quick.html">ParlAI quickstart for help</a>.</p>
<h1><a class="anchor" id="autotoc_md138"></a>
Pretrained Models</h1>
<p>The BERT Bi-Ranker dialogue model is available e.g. via this command (which automatically downloads it): </p><pre class="fragment">python examples/eval_model.py -t light_dialog -mf models:light/biranker_dialogue/model
</pre><h1><a class="anchor" id="autotoc_md139"></a>
Citation</h1>
<p>If you use the dataset or models in your own work, please cite with the following BibTex entry: </p><pre class="fragment">@inproceedings{urbanek2019light,
  author={Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rockt√§schel, Douwe Kiela, Arthur Szlam, Jason Weston},
  title={Learning to Speak and Act in a Fantasy Text Adventure Game},
  journal={arXiv preprint arXiv:1903.03094},
  year={2019},
}
</pre> </div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.16
</small></address>
</body>
</html>
