<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ParlAI: parlai/agents/transformer/modules.py Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ParlAI
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><a class="el" href="dir_5fa64f6a890d29028c37756abca1c9f7.html">parlai</a></li><li class="navelem"><a class="el" href="dir_dbd60702dc487e18803a370af604a6bd.html">agents</a></li><li class="navelem"><a class="el" href="dir_ebc5fbd252b7d23af3b7c307a5a155d0.html">transformer</a></li>  </ul>
</div>
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">modules.py</div>  </div>
</div><!--header-->
<div class="contents">
<a href="parlai_2agents_2transformer_2modules_8py.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno"><a class="line" href="namespaceparlai_1_1agents_1_1transformer_1_1modules.html">    1</a></span>&#160;<span class="comment"># Copyright (c) Facebook, Inc. and its affiliates.</span></div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;<span class="comment"># This source code is licensed under the MIT license found in the</span></div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;<span class="comment"># LICENSE file in the root directory of this source tree.</span></div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;<span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;<span class="stringliteral">Implements NN code for transformers.</span></div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;<span class="stringliteral">Original paper: https://arxiv.org/abs/1706.03762. (Vaswani, 2017). The</span></div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;<span class="stringliteral">`Annotated Transformer` (Rush, 2018) is an excellent reading guide which explains</span></div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;<span class="stringliteral">much of the mechanics of the Transformer model</span></div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;<span class="stringliteral">(http://nlp.seas.harvard.edu/2018/04/03/attention.html).</span></div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;<span class="stringliteral">This module also supports special segments (ala BERT;</span></div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;<span class="stringliteral">https://arxiv.org/abs/1810.04805), and a few different variations seen in the</span></div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;<span class="stringliteral">literature (BERT and XLM; https://arxiv.org/abs/1901.07291).</span></div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;<span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;</div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;<span class="keyword">import</span> torch</div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;<span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;</div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;<span class="keyword">import</span> math</div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;<span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;</div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;<span class="keyword">from</span> <a class="code" href="namespaceparlai_1_1core_1_1torch__generator__agent.html">parlai.core.torch_generator_agent</a> <span class="keyword">import</span> TorchGeneratorModel</div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;<span class="keyword">from</span> <a class="code" href="namespaceparlai_1_1utils_1_1misc.html">parlai.utils.misc</a> <span class="keyword">import</span> warn_once</div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;<span class="keyword">from</span> <a class="code" href="namespaceparlai_1_1utils_1_1misc.html">parlai.utils.misc</a> <span class="keyword">import</span> neginf</div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;</div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;<span class="keywordflow">try</span>:</div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;    <span class="keyword">from</span> apex.normalization.fused_layer_norm <span class="keyword">import</span> FusedLayerNorm <span class="keyword">as</span> LayerNorm</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;<span class="keywordflow">except</span> ImportError:</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;    <a class="code" href="namespaceparlai_1_1utils_1_1misc.html#acf146e70ea7f6867969a7c2b545d4b4b">warn_once</a>(<span class="stringliteral">&quot;Installing APEX can give a significant speed boost.&quot;</span>)</div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;    <span class="keyword">from</span> torch.nn <span class="keyword">import</span> LayerNorm</div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;</div><div class="line"><a name="l00035"></a><span class="lineno"><a class="line" href="namespaceparlai_1_1agents_1_1transformer_1_1modules.html#afb85e0862dd867aee20a92e26aa29f58">   35</a></span>&#160;LAYER_NORM_EPS = 1e-5  <span class="comment"># Epsilon for layer norm.</span></div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;</div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;</div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;<span class="keyword">def </span>_normalize(tensor, norm_layer):</div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;Broadcast layer norm.&quot;&quot;&quot;</span></div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;    size = tensor.size()</div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;    <span class="keywordflow">return</span> norm_layer(tensor.view(-1, size[-1])).view(size)</div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;</div><div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;</div><div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;<span class="keyword">def </span>_create_embeddings(dictionary, embedding_size, padding_idx):</div><div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;Create and initialize word embeddings.&quot;&quot;&quot;</span></div><div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;    e = nn.Embedding(len(dictionary), embedding_size, padding_idx)</div><div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;    nn.init.normal_(e.weight, mean=0, std=embedding_size ** -0.5)</div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;    nn.init.constant_(e.weight[padding_idx], 0)</div><div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;    <span class="keywordflow">return</span> e</div><div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;</div><div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;</div><div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;<span class="keyword">def </span>_build_encoder(</div><div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;    opt,</div><div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;    dictionary,</div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;    embedding=None,</div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;    padding_idx=None,</div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;    reduction_type=&#39;mean&#39;,</div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;    n_positions=1024,</div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;    n_segments=0,</div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;):</div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;    <span class="keywordflow">return</span> <a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html">TransformerEncoder</a>(</div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;        n_heads=opt[<span class="stringliteral">&#39;n_heads&#39;</span>],</div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;        n_layers=opt[<span class="stringliteral">&#39;n_layers&#39;</span>],</div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;        embedding_size=opt[<span class="stringliteral">&#39;embedding_size&#39;</span>],</div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;        ffn_size=opt[<span class="stringliteral">&#39;ffn_size&#39;</span>],</div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;        vocabulary_size=len(dictionary),</div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;        embedding=embedding,</div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;        dropout=opt[<span class="stringliteral">&#39;dropout&#39;</span>],</div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;        attention_dropout=opt[<span class="stringliteral">&#39;attention_dropout&#39;</span>],</div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;        relu_dropout=opt[<span class="stringliteral">&#39;relu_dropout&#39;</span>],</div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;        padding_idx=padding_idx,</div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;        learn_positional_embeddings=opt[<span class="stringliteral">&#39;learn_positional_embeddings&#39;</span>],</div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;        embeddings_scale=opt[<span class="stringliteral">&#39;embeddings_scale&#39;</span>],</div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;        reduction_type=reduction_type,</div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;        n_positions=n_positions,</div><div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;        n_segments=n_segments,</div><div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;        activation=opt[<span class="stringliteral">&#39;activation&#39;</span>],</div><div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;        variant=opt[<span class="stringliteral">&#39;variant&#39;</span>],</div><div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;        output_scaling=opt[<span class="stringliteral">&#39;output_scaling&#39;</span>],</div><div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;    )</div><div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;</div><div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;</div><div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;<span class="keyword">def </span>_build_decoder(</div><div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;    opt, dictionary, embedding=None, padding_idx=None, n_positions=1024, n_segments=0</div><div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;):</div><div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;    <span class="keywordflow">return</span> <a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html">TransformerDecoder</a>(</div><div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;        n_heads=opt[<span class="stringliteral">&#39;n_heads&#39;</span>],</div><div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;        n_layers=opt[<span class="stringliteral">&#39;n_layers&#39;</span>],</div><div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;        embedding_size=opt[<span class="stringliteral">&#39;embedding_size&#39;</span>],</div><div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;        ffn_size=opt[<span class="stringliteral">&#39;ffn_size&#39;</span>],</div><div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;        vocabulary_size=len(dictionary),</div><div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;        embedding=embedding,</div><div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;        dropout=opt[<span class="stringliteral">&#39;dropout&#39;</span>],</div><div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;        attention_dropout=opt[<span class="stringliteral">&#39;attention_dropout&#39;</span>],</div><div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;        relu_dropout=opt[<span class="stringliteral">&#39;relu_dropout&#39;</span>],</div><div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;        padding_idx=padding_idx,</div><div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;        learn_positional_embeddings=opt[<span class="stringliteral">&#39;learn_positional_embeddings&#39;</span>],</div><div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;        embeddings_scale=opt[<span class="stringliteral">&#39;embeddings_scale&#39;</span>],</div><div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;        n_positions=n_positions,</div><div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;        activation=opt[<span class="stringliteral">&#39;activation&#39;</span>],</div><div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;        variant=opt[<span class="stringliteral">&#39;variant&#39;</span>],</div><div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;        n_segments=n_segments,</div><div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;    )</div><div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;</div><div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;</div><div class="line"><a name="l00106"></a><span class="lineno"><a class="line" href="namespaceparlai_1_1agents_1_1transformer_1_1modules.html#ad93916b8d2188c35733089e1581c44f8">  106</a></span>&#160;<span class="keyword">def </span><a class="code" href="namespaceparlai_1_1agents_1_1transformer_1_1modules.html#ad93916b8d2188c35733089e1581c44f8">gelu</a>(tensor):</div><div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;<span class="stringliteral">    Compute gelu function.</span></div><div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;<span class="stringliteral">    c.f. https://arxiv.org/abs/1606.08415</span></div><div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;<span class="stringliteral">    &quot;&quot;&quot;</span></div><div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;    <span class="keywordflow">return</span> 0.5 * tensor * (1.0 + torch.erf(tensor / math.sqrt(2.0)))</div><div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;</div><div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;</div><div class="line"><a name="l00115"></a><span class="lineno"><a class="line" href="namespaceparlai_1_1agents_1_1transformer_1_1modules.html#ab67607512c597ddd54f2b60a1a1eaf4c">  115</a></span>&#160;<span class="keyword">def </span><a class="code" href="namespaceparlai_1_1agents_1_1transformer_1_1modules.html#ab67607512c597ddd54f2b60a1a1eaf4c">get_n_positions_from_options</a>(opt):</div><div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;Determine n_positions from options dict.&quot;&quot;&quot;</span></div><div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;    <span class="keywordflow">if</span> opt.get(<span class="stringliteral">&#39;n_positions&#39;</span>):</div><div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;        <span class="comment"># if the number of positions is explicitly provided, use that</span></div><div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;        n_positions = opt[<span class="stringliteral">&#39;n_positions&#39;</span>]</div><div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;    <span class="keywordflow">else</span>:</div><div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;        <span class="comment"># else, use the worst case from truncate</span></div><div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;        n_positions = max(</div><div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;            opt.get(<span class="stringliteral">&#39;truncate&#39;</span>) <span class="keywordflow">or</span> 0,</div><div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;            opt.get(<span class="stringliteral">&#39;text_truncate&#39;</span>) <span class="keywordflow">or</span> 0,</div><div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;            opt.get(<span class="stringliteral">&#39;label_truncate&#39;</span>) <span class="keywordflow">or</span> 0,</div><div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;        )</div><div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;        <span class="keywordflow">if</span> n_positions == 0:</div><div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;            n_positions = 1024</div><div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;    <span class="keywordflow">return</span> n_positions</div><div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;</div><div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;</div><div class="line"><a name="l00132"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html">  132</a></span>&#160;<span class="keyword">class </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html">TransformerMemNetModel</a>(nn.Module):</div><div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;Model which takes context, memories, candidates and encodes them.&quot;&quot;&quot;</span></div><div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;</div><div class="line"><a name="l00135"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a0b1598bb8f1a8b899c00af25044ffe9c">  135</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a0b1598bb8f1a8b899c00af25044ffe9c">__init__</a>(self, opt, dictionary):</div><div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;        super().<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a0b1598bb8f1a8b899c00af25044ffe9c">__init__</a>()</div><div class="line"><a name="l00137"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a293a7995da2bc66087db07ba4669fb35">  137</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a293a7995da2bc66087db07ba4669fb35">opt</a> = opt</div><div class="line"><a name="l00138"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#ab70ecab55a92126804e165d745642142">  138</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#ab70ecab55a92126804e165d745642142">pad_idx</a> = dictionary[dictionary.null_token]</div><div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;</div><div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;        <span class="comment"># set up embeddings</span></div><div class="line"><a name="l00141"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#aa776378904d1ffd3148e330036230713">  141</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#aa776378904d1ffd3148e330036230713">embeddings</a> = _create_embeddings(</div><div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;            dictionary, opt[<span class="stringliteral">&#39;embedding_size&#39;</span>], self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#ab70ecab55a92126804e165d745642142">pad_idx</a></div><div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;        )</div><div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;</div><div class="line"><a name="l00145"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a92d7385aa1b2603fa03ed2f5ad7f66be">  145</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a92d7385aa1b2603fa03ed2f5ad7f66be">share_word_embedding</a> = opt.get(<span class="stringliteral">&#39;share_word_embeddings&#39;</span>, <span class="keyword">True</span>)</div><div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;        <span class="keywordflow">if</span> <span class="keywordflow">not</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a92d7385aa1b2603fa03ed2f5ad7f66be">share_word_embedding</a>:</div><div class="line"><a name="l00147"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a6c12d979c4ce2a21f9a756e7ccc6e7e5">  147</a></span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a6c12d979c4ce2a21f9a756e7ccc6e7e5">cand_embeddings</a> = _create_embeddings(</div><div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;                dictionary, opt[<span class="stringliteral">&#39;embedding_size&#39;</span>], self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#ab70ecab55a92126804e165d745642142">pad_idx</a></div><div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;            )</div><div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;</div><div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;        <span class="keywordflow">if</span> <span class="keywordflow">not</span> opt.get(<span class="stringliteral">&#39;learn_embeddings&#39;</span>):</div><div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#aa776378904d1ffd3148e330036230713">embeddings</a>.weight.requires_grad = <span class="keyword">False</span></div><div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;            <span class="keywordflow">if</span> <span class="keywordflow">not</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a92d7385aa1b2603fa03ed2f5ad7f66be">share_word_embedding</a>:</div><div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;                self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a6c12d979c4ce2a21f9a756e7ccc6e7e5">cand_embeddings</a>.weight.requires_grad = <span class="keyword">False</span></div><div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;</div><div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;        n_positions = <a class="code" href="namespaceparlai_1_1agents_1_1transformer_1_1modules.html#ab67607512c597ddd54f2b60a1a1eaf4c">get_n_positions_from_options</a>(opt)</div><div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;</div><div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;        <span class="keywordflow">if</span> n_positions &lt; 0:</div><div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;            <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&#39;n_positions must be positive&#39;</span>)</div><div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;</div><div class="line"><a name="l00161"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a3211cf5e4f390d5cf0f7598a7b789985">  161</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a3211cf5e4f390d5cf0f7598a7b789985">reduction_type</a> = opt.get(<span class="stringliteral">&#39;reduction_type&#39;</span>, <span class="stringliteral">&#39;mean&#39;</span>)</div><div class="line"><a name="l00162"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a9ac6c75642035a573f9ac123ee302403">  162</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a9ac6c75642035a573f9ac123ee302403">n_segments</a> = opt.get(<span class="stringliteral">&#39;n_segments&#39;</span>, 0)</div><div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;</div><div class="line"><a name="l00164"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a899a125583f6fbf7f4809c2744162ac9">  164</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a899a125583f6fbf7f4809c2744162ac9">context_encoder</a> = _build_encoder(</div><div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;            opt,</div><div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;            dictionary,</div><div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#aa776378904d1ffd3148e330036230713">embeddings</a>,</div><div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#ab70ecab55a92126804e165d745642142">pad_idx</a>,</div><div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;            reduction_type=self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a3211cf5e4f390d5cf0f7598a7b789985">reduction_type</a>,</div><div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;            n_positions=n_positions,</div><div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;            n_segments=self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a9ac6c75642035a573f9ac123ee302403">n_segments</a>,</div><div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;        )</div><div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;</div><div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;        <span class="keywordflow">if</span> opt.get(<span class="stringliteral">&#39;share_encoders&#39;</span>):</div><div class="line"><a name="l00175"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#adf42f25f3ae4564f531835aeec39bb5f">  175</a></span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#adf42f25f3ae4564f531835aeec39bb5f">cand_encoder</a> = <a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html">TransformerResponseWrapper</a>(</div><div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;                self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a899a125583f6fbf7f4809c2744162ac9">context_encoder</a>, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a899a125583f6fbf7f4809c2744162ac9">context_encoder</a>.out_dim</div><div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;            )</div><div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;            <span class="keywordflow">if</span> <span class="keywordflow">not</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a92d7385aa1b2603fa03ed2f5ad7f66be">share_word_embedding</a>:</div><div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;                cand_embeddings = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a6c12d979c4ce2a21f9a756e7ccc6e7e5">cand_embeddings</a></div><div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;            <span class="keywordflow">else</span>:</div><div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;                cand_embeddings = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#aa776378904d1ffd3148e330036230713">embeddings</a></div><div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#adf42f25f3ae4564f531835aeec39bb5f">cand_encoder</a> = _build_encoder(</div><div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;                opt,</div><div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;                dictionary,</div><div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;                cand_embeddings,</div><div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;                self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#ab70ecab55a92126804e165d745642142">pad_idx</a>,</div><div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;                n_positions=n_positions,</div><div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;                reduction_type=self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a3211cf5e4f390d5cf0f7598a7b789985">reduction_type</a>,</div><div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160;                n_segments=self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a9ac6c75642035a573f9ac123ee302403">n_segments</a>,</div><div class="line"><a name="l00191"></a><span class="lineno">  191</span>&#160;            )</div><div class="line"><a name="l00192"></a><span class="lineno">  192</span>&#160;</div><div class="line"><a name="l00193"></a><span class="lineno">  193</span>&#160;        <span class="comment"># build memory encoder</span></div><div class="line"><a name="l00194"></a><span class="lineno">  194</span>&#160;        <span class="keywordflow">if</span> opt.get(<span class="stringliteral">&#39;wrap_memory_encoder&#39;</span>, <span class="keyword">False</span>):</div><div class="line"><a name="l00195"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a6d1a5e20639c71de7a60e63b88238784">  195</a></span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a6d1a5e20639c71de7a60e63b88238784">memory_transformer</a> = <a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html">TransformerResponseWrapper</a>(</div><div class="line"><a name="l00196"></a><span class="lineno">  196</span>&#160;                self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a899a125583f6fbf7f4809c2744162ac9">context_encoder</a>, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a899a125583f6fbf7f4809c2744162ac9">context_encoder</a>.out_dim</div><div class="line"><a name="l00197"></a><span class="lineno">  197</span>&#160;            )</div><div class="line"><a name="l00198"></a><span class="lineno">  198</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00199"></a><span class="lineno">  199</span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a6d1a5e20639c71de7a60e63b88238784">memory_transformer</a> = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a899a125583f6fbf7f4809c2744162ac9">context_encoder</a></div><div class="line"><a name="l00200"></a><span class="lineno">  200</span>&#160;</div><div class="line"><a name="l00201"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a95c72e762694a7017243bfcf0b37a1e7">  201</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a95c72e762694a7017243bfcf0b37a1e7">attender</a> = <a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html">BasicAttention</a>(</div><div class="line"><a name="l00202"></a><span class="lineno">  202</span>&#160;            dim=2, attn=opt[<span class="stringliteral">&#39;memory_attention&#39;</span>], residual=<span class="keyword">True</span></div><div class="line"><a name="l00203"></a><span class="lineno">  203</span>&#160;        )</div><div class="line"><a name="l00204"></a><span class="lineno">  204</span>&#160;</div><div class="line"><a name="l00205"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a1e39cdf6357c9e8dd9f90e7f351e42a1">  205</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a1e39cdf6357c9e8dd9f90e7f351e42a1">encode_cand</a>(self, words):</div><div class="line"><a name="l00206"></a><span class="lineno">  206</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;Encode the candidates.&quot;&quot;&quot;</span></div><div class="line"><a name="l00207"></a><span class="lineno">  207</span>&#160;        <span class="keywordflow">if</span> words <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00208"></a><span class="lineno">  208</span>&#160;            <span class="keywordflow">return</span> <span class="keywordtype">None</span></div><div class="line"><a name="l00209"></a><span class="lineno">  209</span>&#160;</div><div class="line"><a name="l00210"></a><span class="lineno">  210</span>&#160;        <span class="comment"># flatten if there are many candidates</span></div><div class="line"><a name="l00211"></a><span class="lineno">  211</span>&#160;        <span class="keywordflow">if</span> words.dim() == 3:</div><div class="line"><a name="l00212"></a><span class="lineno">  212</span>&#160;            oldshape = words.shape</div><div class="line"><a name="l00213"></a><span class="lineno">  213</span>&#160;            words = words.reshape(oldshape[0] * oldshape[1], oldshape[2])</div><div class="line"><a name="l00214"></a><span class="lineno">  214</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00215"></a><span class="lineno">  215</span>&#160;            oldshape = <span class="keywordtype">None</span></div><div class="line"><a name="l00216"></a><span class="lineno">  216</span>&#160;</div><div class="line"><a name="l00217"></a><span class="lineno">  217</span>&#160;        encoded = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#adf42f25f3ae4564f531835aeec39bb5f">cand_encoder</a>(words)</div><div class="line"><a name="l00218"></a><span class="lineno">  218</span>&#160;</div><div class="line"><a name="l00219"></a><span class="lineno">  219</span>&#160;        <span class="keywordflow">if</span> oldshape <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;            encoded = encoded.reshape(oldshape[0], oldshape[1], -1)</div><div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;</div><div class="line"><a name="l00222"></a><span class="lineno">  222</span>&#160;        <span class="keywordflow">return</span> encoded</div><div class="line"><a name="l00223"></a><span class="lineno">  223</span>&#160;</div><div class="line"><a name="l00224"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a12c6c4a26be5401e61c43ec4aa886812">  224</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a12c6c4a26be5401e61c43ec4aa886812">encode_context_memory</a>(self, context_w, memories_w, context_segments=None):</div><div class="line"><a name="l00225"></a><span class="lineno">  225</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;Encode the context and memories.&quot;&quot;&quot;</span></div><div class="line"><a name="l00226"></a><span class="lineno">  226</span>&#160;        <span class="comment"># [batch, d]</span></div><div class="line"><a name="l00227"></a><span class="lineno">  227</span>&#160;        <span class="keywordflow">if</span> context_w <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00228"></a><span class="lineno">  228</span>&#160;            <span class="comment"># it&#39;s possible that only candidates were passed into the</span></div><div class="line"><a name="l00229"></a><span class="lineno">  229</span>&#160;            <span class="comment"># forward function, return None here for LHS representation</span></div><div class="line"><a name="l00230"></a><span class="lineno">  230</span>&#160;            <span class="keywordflow">return</span> <span class="keywordtype">None</span>, <span class="keywordtype">None</span></div><div class="line"><a name="l00231"></a><span class="lineno">  231</span>&#160;</div><div class="line"><a name="l00232"></a><span class="lineno">  232</span>&#160;        context_h = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a899a125583f6fbf7f4809c2744162ac9">context_encoder</a>(context_w, segments=context_segments)</div><div class="line"><a name="l00233"></a><span class="lineno">  233</span>&#160;</div><div class="line"><a name="l00234"></a><span class="lineno">  234</span>&#160;        <span class="keywordflow">if</span> memories_w <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00235"></a><span class="lineno">  235</span>&#160;            <span class="keywordflow">return</span> [], context_h</div><div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;</div><div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;        bsz = memories_w.size(0)</div><div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;        memories_w = memories_w.view(-1, memories_w.size(-1))</div><div class="line"><a name="l00239"></a><span class="lineno">  239</span>&#160;        memories_h = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a6d1a5e20639c71de7a60e63b88238784">memory_transformer</a>(memories_w)</div><div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;        memories_h = memories_h.view(bsz, -1, memories_h.size(-1))</div><div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;</div><div class="line"><a name="l00242"></a><span class="lineno">  242</span>&#160;        context_h = context_h.unsqueeze(1)</div><div class="line"><a name="l00243"></a><span class="lineno">  243</span>&#160;        context_h, weights = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a95c72e762694a7017243bfcf0b37a1e7">attender</a>(context_h, memories_h)</div><div class="line"><a name="l00244"></a><span class="lineno">  244</span>&#160;</div><div class="line"><a name="l00245"></a><span class="lineno">  245</span>&#160;        <span class="keywordflow">return</span> weights, context_h</div><div class="line"><a name="l00246"></a><span class="lineno">  246</span>&#160;</div><div class="line"><a name="l00247"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a8c27f924c01b5017c7968b9d98969496">  247</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a8c27f924c01b5017c7968b9d98969496">forward</a>(self, xs, mems, cands, context_segments=None):</div><div class="line"><a name="l00248"></a><span class="lineno">  248</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00249"></a><span class="lineno">  249</span>&#160;<span class="stringliteral">        Forward pass.</span></div><div class="line"><a name="l00250"></a><span class="lineno">  250</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00251"></a><span class="lineno">  251</span>&#160;<span class="stringliteral">        :param LongTensor[batch,seqlen] xs: input tokens IDs</span></div><div class="line"><a name="l00252"></a><span class="lineno">  252</span>&#160;<span class="stringliteral">        :param LongTensor[batch,num_mems,seqlen] mems: memory token IDs</span></div><div class="line"><a name="l00253"></a><span class="lineno">  253</span>&#160;<span class="stringliteral">        :param LongTensor[batch,num_cands,seqlen] cands: candidate token IDs</span></div><div class="line"><a name="l00254"></a><span class="lineno">  254</span>&#160;<span class="stringliteral">        :param LongTensor[batch,seqlen] context_segments: segment IDs for xs,</span></div><div class="line"><a name="l00255"></a><span class="lineno">  255</span>&#160;<span class="stringliteral">            used if n_segments is &gt; 0 for the context encoder</span></div><div class="line"><a name="l00256"></a><span class="lineno">  256</span>&#160;<span class="stringliteral">        &quot;&quot;&quot;</span></div><div class="line"><a name="l00257"></a><span class="lineno">  257</span>&#160;        <span class="comment"># encode the context and memories together</span></div><div class="line"><a name="l00258"></a><span class="lineno">  258</span>&#160;        weights, context_h = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a12c6c4a26be5401e61c43ec4aa886812">encode_context_memory</a>(</div><div class="line"><a name="l00259"></a><span class="lineno">  259</span>&#160;            xs, mems, context_segments=context_segments</div><div class="line"><a name="l00260"></a><span class="lineno">  260</span>&#160;        )</div><div class="line"><a name="l00261"></a><span class="lineno">  261</span>&#160;        <span class="comment"># encode the candidates</span></div><div class="line"><a name="l00262"></a><span class="lineno">  262</span>&#160;        cands_h = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a1e39cdf6357c9e8dd9f90e7f351e42a1">encode_cand</a>(cands)</div><div class="line"><a name="l00263"></a><span class="lineno">  263</span>&#160;</div><div class="line"><a name="l00264"></a><span class="lineno">  264</span>&#160;        <span class="comment"># possibly normalize the context and candidate representations</span></div><div class="line"><a name="l00265"></a><span class="lineno">  265</span>&#160;        <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a293a7995da2bc66087db07ba4669fb35">opt</a>[<span class="stringliteral">&#39;normalize_sent_emb&#39;</span>]:</div><div class="line"><a name="l00266"></a><span class="lineno">  266</span>&#160;            context_h = context_h / context_h.norm(2, dim=1, keepdim=<span class="keyword">True</span>)</div><div class="line"><a name="l00267"></a><span class="lineno">  267</span>&#160;            cands_h = cands_h / cands_h.norm(2, dim=1, keepdim=<span class="keyword">True</span>)</div><div class="line"><a name="l00268"></a><span class="lineno">  268</span>&#160;</div><div class="line"><a name="l00269"></a><span class="lineno">  269</span>&#160;        <span class="keywordflow">return</span> context_h, cands_h</div><div class="line"><a name="l00270"></a><span class="lineno">  270</span>&#160;</div><div class="line"><a name="l00271"></a><span class="lineno">  271</span>&#160;</div><div class="line"><a name="l00272"></a><span class="lineno"><a class="line" href="namespaceparlai_1_1agents_1_1transformer_1_1modules.html#a0b86437e6e9682fa3100e9cadcaae259">  272</a></span>&#160;<span class="keyword">def </span><a class="code" href="namespaceparlai_1_1agents_1_1transformer_1_1modules.html#a0b86437e6e9682fa3100e9cadcaae259">create_position_codes</a>(n_pos, dim, out):</div><div class="line"><a name="l00273"></a><span class="lineno">  273</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;Create positional codes and store them in ``out``.&quot;&quot;&quot;</span></div><div class="line"><a name="l00274"></a><span class="lineno">  274</span>&#160;    position_enc = np.array(</div><div class="line"><a name="l00275"></a><span class="lineno">  275</span>&#160;        [</div><div class="line"><a name="l00276"></a><span class="lineno">  276</span>&#160;            [pos / np.power(10000, 2 * j / dim) <span class="keywordflow">for</span> j <span class="keywordflow">in</span> range(dim // 2)]</div><div class="line"><a name="l00277"></a><span class="lineno">  277</span>&#160;            <span class="keywordflow">for</span> pos <span class="keywordflow">in</span> range(n_pos)</div><div class="line"><a name="l00278"></a><span class="lineno">  278</span>&#160;        ]</div><div class="line"><a name="l00279"></a><span class="lineno">  279</span>&#160;    )</div><div class="line"><a name="l00280"></a><span class="lineno">  280</span>&#160;</div><div class="line"><a name="l00281"></a><span class="lineno">  281</span>&#160;    out[:, 0::2] = torch.FloatTensor(np.sin(position_enc)).type_as(out)</div><div class="line"><a name="l00282"></a><span class="lineno">  282</span>&#160;    out[:, 1::2] = torch.FloatTensor(np.cos(position_enc)).type_as(out)</div><div class="line"><a name="l00283"></a><span class="lineno">  283</span>&#160;    out.detach_()</div><div class="line"><a name="l00284"></a><span class="lineno">  284</span>&#160;    out.requires_grad = <span class="keyword">False</span></div><div class="line"><a name="l00285"></a><span class="lineno">  285</span>&#160;</div><div class="line"><a name="l00286"></a><span class="lineno">  286</span>&#160;</div><div class="line"><a name="l00287"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html">  287</a></span>&#160;<span class="keyword">class </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html">TransformerResponseWrapper</a>(nn.Module):</div><div class="line"><a name="l00288"></a><span class="lineno">  288</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00289"></a><span class="lineno">  289</span>&#160;<span class="stringliteral">    Wrap transformer response.</span></div><div class="line"><a name="l00290"></a><span class="lineno">  290</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00291"></a><span class="lineno">  291</span>&#160;<span class="stringliteral">    Pushes input through transformer and MLP.</span></div><div class="line"><a name="l00292"></a><span class="lineno">  292</span>&#160;<span class="stringliteral">    &quot;&quot;&quot;</span></div><div class="line"><a name="l00293"></a><span class="lineno">  293</span>&#160;</div><div class="line"><a name="l00294"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#ab078bf40bb275ebd56b4e205923ab827">  294</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#ab078bf40bb275ebd56b4e205923ab827">__init__</a>(self, transformer, hdim):</div><div class="line"><a name="l00295"></a><span class="lineno">  295</span>&#160;        super(TransformerResponseWrapper, self).<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#ab078bf40bb275ebd56b4e205923ab827">__init__</a>()</div><div class="line"><a name="l00296"></a><span class="lineno">  296</span>&#160;        dim = transformer.out_dim</div><div class="line"><a name="l00297"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#ae757bd573993a3ca4a31e9d65b1a7e9a">  297</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#ae757bd573993a3ca4a31e9d65b1a7e9a">transformer</a> = transformer</div><div class="line"><a name="l00298"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#a11ccb5cd5d1b5236b08c904bdeb9e041">  298</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#a11ccb5cd5d1b5236b08c904bdeb9e041">mlp</a> = nn.Sequential(</div><div class="line"><a name="l00299"></a><span class="lineno">  299</span>&#160;            nn.Linear(dim, hdim),</div><div class="line"><a name="l00300"></a><span class="lineno">  300</span>&#160;            nn.ReLU(),  <span class="comment"># TODO: should this also be gelu?</span></div><div class="line"><a name="l00301"></a><span class="lineno">  301</span>&#160;            nn.Linear(hdim, dim),</div><div class="line"><a name="l00302"></a><span class="lineno">  302</span>&#160;        )</div><div class="line"><a name="l00303"></a><span class="lineno">  303</span>&#160;</div><div class="line"><a name="l00304"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#ad716cb3a5874f46b1aa4beb45ba1b944">  304</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#ad716cb3a5874f46b1aa4beb45ba1b944">forward</a>(self, *args):</div><div class="line"><a name="l00305"></a><span class="lineno">  305</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;Forward pass.&quot;&quot;&quot;</span></div><div class="line"><a name="l00306"></a><span class="lineno">  306</span>&#160;        <span class="keywordflow">return</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#a11ccb5cd5d1b5236b08c904bdeb9e041">mlp</a>(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#ae757bd573993a3ca4a31e9d65b1a7e9a">transformer</a>(*args))</div><div class="line"><a name="l00307"></a><span class="lineno">  307</span>&#160;</div><div class="line"><a name="l00308"></a><span class="lineno">  308</span>&#160;</div><div class="line"><a name="l00309"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html">  309</a></span>&#160;<span class="keyword">class </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html">TransformerLinearWrapper</a>(nn.Module):</div><div class="line"><a name="l00310"></a><span class="lineno">  310</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;Wrap a transformer in a linear layer.&quot;&quot;&quot;</span></div><div class="line"><a name="l00311"></a><span class="lineno">  311</span>&#160;</div><div class="line"><a name="l00312"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#a2676a49582292330a7829e235a7cf8f5">  312</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#a2676a49582292330a7829e235a7cf8f5">__init__</a>(self, transformer, output_dim):</div><div class="line"><a name="l00313"></a><span class="lineno">  313</span>&#160;        super().<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#a2676a49582292330a7829e235a7cf8f5">__init__</a>()</div><div class="line"><a name="l00314"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#acac2d2f099f07b21f792326f90541ffd">  314</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#acac2d2f099f07b21f792326f90541ffd">transformer</a> = transformer</div><div class="line"><a name="l00315"></a><span class="lineno">  315</span>&#160;        input_dim = transformer.out_dim</div><div class="line"><a name="l00316"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#af083195926e1f091010ecb4c6d409a2a">  316</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#af083195926e1f091010ecb4c6d409a2a">additional_linear_layer</a> = nn.Linear(input_dim, output_dim)</div><div class="line"><a name="l00317"></a><span class="lineno">  317</span>&#160;</div><div class="line"><a name="l00318"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#a82f571e85719bde36f8fcf09f27d3e4c">  318</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#a82f571e85719bde36f8fcf09f27d3e4c">forward</a>(self, *args):</div><div class="line"><a name="l00319"></a><span class="lineno">  319</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;Forward pass.</span></div><div class="line"><a name="l00320"></a><span class="lineno">  320</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00321"></a><span class="lineno">  321</span>&#160;<span class="stringliteral">        Apply transformer, then additional linear layer.</span></div><div class="line"><a name="l00322"></a><span class="lineno">  322</span>&#160;<span class="stringliteral">        &quot;&quot;&quot;</span></div><div class="line"><a name="l00323"></a><span class="lineno">  323</span>&#160;        context_h = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#acac2d2f099f07b21f792326f90541ffd">transformer</a>(*args)</div><div class="line"><a name="l00324"></a><span class="lineno">  324</span>&#160;        <span class="keywordflow">return</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#af083195926e1f091010ecb4c6d409a2a">additional_linear_layer</a>(context_h)</div><div class="line"><a name="l00325"></a><span class="lineno">  325</span>&#160;</div><div class="line"><a name="l00326"></a><span class="lineno">  326</span>&#160;</div><div class="line"><a name="l00327"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html">  327</a></span>&#160;<span class="keyword">class </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html">TransformerEncoder</a>(nn.Module):</div><div class="line"><a name="l00328"></a><span class="lineno">  328</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00329"></a><span class="lineno">  329</span>&#160;<span class="stringliteral">    Transformer encoder module.</span></div><div class="line"><a name="l00330"></a><span class="lineno">  330</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00331"></a><span class="lineno">  331</span>&#160;<span class="stringliteral">    :param int n_heads: the number of multihead attention heads.</span></div><div class="line"><a name="l00332"></a><span class="lineno">  332</span>&#160;<span class="stringliteral">    :param int n_layers: number of transformer layers.</span></div><div class="line"><a name="l00333"></a><span class="lineno">  333</span>&#160;<span class="stringliteral">    :param int embedding_size: the embedding sizes. Must be a multiple of n_heads.</span></div><div class="line"><a name="l00334"></a><span class="lineno">  334</span>&#160;<span class="stringliteral">    :param int ffn_size: the size of the hidden layer in the FFN</span></div><div class="line"><a name="l00335"></a><span class="lineno">  335</span>&#160;<span class="stringliteral">    :param embedding: an embedding matrix for the bottom layer of the transformer.</span></div><div class="line"><a name="l00336"></a><span class="lineno">  336</span>&#160;<span class="stringliteral">        If none, one is created for this encoder.</span></div><div class="line"><a name="l00337"></a><span class="lineno">  337</span>&#160;<span class="stringliteral">    :param float dropout: Dropout used around embeddings and before layer</span></div><div class="line"><a name="l00338"></a><span class="lineno">  338</span>&#160;<span class="stringliteral">        layer normalizations. This is used in Vaswani 2017 and works well on</span></div><div class="line"><a name="l00339"></a><span class="lineno">  339</span>&#160;<span class="stringliteral">        large datasets.</span></div><div class="line"><a name="l00340"></a><span class="lineno">  340</span>&#160;<span class="stringliteral">    :param float attention_dropout: Dropout performed after the multhead attention</span></div><div class="line"><a name="l00341"></a><span class="lineno">  341</span>&#160;<span class="stringliteral">        softmax. This is not used in Vaswani 2017.</span></div><div class="line"><a name="l00342"></a><span class="lineno">  342</span>&#160;<span class="stringliteral">    :param float relu_attention: Dropout used after the ReLU in the FFN. Not used</span></div><div class="line"><a name="l00343"></a><span class="lineno">  343</span>&#160;<span class="stringliteral">        in Vaswani 2017, but used in Tensor2Tensor.</span></div><div class="line"><a name="l00344"></a><span class="lineno">  344</span>&#160;<span class="stringliteral">    :param int padding_idx: Reserved padding index in the embeddings matrix.</span></div><div class="line"><a name="l00345"></a><span class="lineno">  345</span>&#160;<span class="stringliteral">    :param bool learn_positional_embeddings: If off, sinusoidal embeddings are</span></div><div class="line"><a name="l00346"></a><span class="lineno">  346</span>&#160;<span class="stringliteral">        used. If on, position embeddings are learned from scratch.</span></div><div class="line"><a name="l00347"></a><span class="lineno">  347</span>&#160;<span class="stringliteral">    :param bool embeddings_scale: Scale embeddings relative to their dimensionality.</span></div><div class="line"><a name="l00348"></a><span class="lineno">  348</span>&#160;<span class="stringliteral">        Found useful in fairseq.</span></div><div class="line"><a name="l00349"></a><span class="lineno">  349</span>&#160;<span class="stringliteral">    :param bool reduction: If true, returns the mean vector for the entire encoding</span></div><div class="line"><a name="l00350"></a><span class="lineno">  350</span>&#160;<span class="stringliteral">        sequence.</span></div><div class="line"><a name="l00351"></a><span class="lineno">  351</span>&#160;<span class="stringliteral">    :param int n_positions:</span></div><div class="line"><a name="l00352"></a><span class="lineno">  352</span>&#160;<span class="stringliteral">        Size of the position embeddings matrix.</span></div><div class="line"><a name="l00353"></a><span class="lineno">  353</span>&#160;<span class="stringliteral">    :param int n_segments:</span></div><div class="line"><a name="l00354"></a><span class="lineno">  354</span>&#160;<span class="stringliteral">        Number of segments/lang/sentence embeddings.</span></div><div class="line"><a name="l00355"></a><span class="lineno">  355</span>&#160;<span class="stringliteral">    :param activation:</span></div><div class="line"><a name="l00356"></a><span class="lineno">  356</span>&#160;<span class="stringliteral">        Type of nonlinear activation. Can be relu or gelu.</span></div><div class="line"><a name="l00357"></a><span class="lineno">  357</span>&#160;<span class="stringliteral">    :param variant:</span></div><div class="line"><a name="l00358"></a><span class="lineno">  358</span>&#160;<span class="stringliteral">        Which transformer architecture to use. Could be AIAYN or XLM.</span></div><div class="line"><a name="l00359"></a><span class="lineno">  359</span>&#160;<span class="stringliteral">        Future versions may support things like GPT-2, ...</span></div><div class="line"><a name="l00360"></a><span class="lineno">  360</span>&#160;<span class="stringliteral">    :param output_scaling:</span></div><div class="line"><a name="l00361"></a><span class="lineno">  361</span>&#160;<span class="stringliteral">        Scale the outputs by a given scalar</span></div><div class="line"><a name="l00362"></a><span class="lineno">  362</span>&#160;<span class="stringliteral">    &quot;&quot;&quot;</span></div><div class="line"><a name="l00363"></a><span class="lineno">  363</span>&#160;</div><div class="line"><a name="l00364"></a><span class="lineno">  364</span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a362f8b951877f3712ca59f0fb6a0ae9c">__init__</a>(</div><div class="line"><a name="l00365"></a><span class="lineno">  365</span>&#160;        self,</div><div class="line"><a name="l00366"></a><span class="lineno">  366</span>&#160;        n_heads,</div><div class="line"><a name="l00367"></a><span class="lineno">  367</span>&#160;        n_layers,</div><div class="line"><a name="l00368"></a><span class="lineno">  368</span>&#160;        embedding_size,</div><div class="line"><a name="l00369"></a><span class="lineno">  369</span>&#160;        ffn_size,</div><div class="line"><a name="l00370"></a><span class="lineno">  370</span>&#160;        vocabulary_size,</div><div class="line"><a name="l00371"></a><span class="lineno">  371</span>&#160;        embedding=None,</div><div class="line"><a name="l00372"></a><span class="lineno">  372</span>&#160;        dropout=0.0,</div><div class="line"><a name="l00373"></a><span class="lineno">  373</span>&#160;        attention_dropout=0.0,</div><div class="line"><a name="l00374"></a><span class="lineno">  374</span>&#160;        relu_dropout=0.0,</div><div class="line"><a name="l00375"></a><span class="lineno">  375</span>&#160;        padding_idx=0,</div><div class="line"><a name="l00376"></a><span class="lineno">  376</span>&#160;        learn_positional_embeddings=False,</div><div class="line"><a name="l00377"></a><span class="lineno">  377</span>&#160;        embeddings_scale=False,</div><div class="line"><a name="l00378"></a><span class="lineno">  378</span>&#160;        reduction_type=&#39;mean&#39;,</div><div class="line"><a name="l00379"></a><span class="lineno">  379</span>&#160;        n_positions=1024,</div><div class="line"><a name="l00380"></a><span class="lineno">  380</span>&#160;        activation=&#39;relu&#39;,</div><div class="line"><a name="l00381"></a><span class="lineno">  381</span>&#160;        variant=&#39;aiayn&#39;,</div><div class="line"><a name="l00382"></a><span class="lineno">  382</span>&#160;        n_segments=0,</div><div class="line"><a name="l00383"></a><span class="lineno">  383</span>&#160;        output_scaling=1.0,</div><div class="line"><a name="l00384"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a362f8b951877f3712ca59f0fb6a0ae9c">  384</a></span>&#160;    ):</div><div class="line"><a name="l00385"></a><span class="lineno">  385</span>&#160;        super(TransformerEncoder, self).<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a362f8b951877f3712ca59f0fb6a0ae9c">__init__</a>()</div><div class="line"><a name="l00386"></a><span class="lineno">  386</span>&#160;</div><div class="line"><a name="l00387"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a57f137ae98789edeae26de90118efdd6">  387</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a57f137ae98789edeae26de90118efdd6">embedding_size</a> = embedding_size</div><div class="line"><a name="l00388"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#afadc86cb37af464565167dc169447057">  388</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#afadc86cb37af464565167dc169447057">ffn_size</a> = ffn_size</div><div class="line"><a name="l00389"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a1fac9c2d34c4d2268f50fc90b1e74e04">  389</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a1fac9c2d34c4d2268f50fc90b1e74e04">n_layers</a> = n_layers</div><div class="line"><a name="l00390"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab6830a5aa0850e50549c159586c17515">  390</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab6830a5aa0850e50549c159586c17515">n_heads</a> = n_heads</div><div class="line"><a name="l00391"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab56fa99f887753677dc7f4d435595114">  391</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab56fa99f887753677dc7f4d435595114">dim</a> = embedding_size</div><div class="line"><a name="l00392"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a80122462ebd32da923d7b4479053ab01">  392</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a80122462ebd32da923d7b4479053ab01">embeddings_scale</a> = embeddings_scale</div><div class="line"><a name="l00393"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a2703f337fd44344ccd93743e57be554e">  393</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a2703f337fd44344ccd93743e57be554e">reduction_type</a> = reduction_type</div><div class="line"><a name="l00394"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a74702a13d7e41afc3e77c94bb60bd119">  394</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a74702a13d7e41afc3e77c94bb60bd119">padding_idx</a> = padding_idx</div><div class="line"><a name="l00395"></a><span class="lineno">  395</span>&#160;        <span class="comment"># this is --dropout, not --relu-dropout or --attention-dropout</span></div><div class="line"><a name="l00396"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#af28dbf5c6c0f70c1a032a3bbb5359661">  396</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#af28dbf5c6c0f70c1a032a3bbb5359661">dropout</a> = nn.Dropout(p=dropout)</div><div class="line"><a name="l00397"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a6ea46aeb8ca865c97ec8e99746bedd7b">  397</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a6ea46aeb8ca865c97ec8e99746bedd7b">variant</a> = variant</div><div class="line"><a name="l00398"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab5ec60adca3ce418f73e409187a82625">  398</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab5ec60adca3ce418f73e409187a82625">n_segments</a> = n_segments</div><div class="line"><a name="l00399"></a><span class="lineno">  399</span>&#160;</div><div class="line"><a name="l00400"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab885dd6b80284dbd2837ecaf75f96cb0">  400</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab885dd6b80284dbd2837ecaf75f96cb0">n_positions</a> = n_positions</div><div class="line"><a name="l00401"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#afd4ec22c3a8b8611a4a1caa85225b426">  401</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#afd4ec22c3a8b8611a4a1caa85225b426">out_dim</a> = embedding_size</div><div class="line"><a name="l00402"></a><span class="lineno">  402</span>&#160;        <span class="keyword">assert</span> (</div><div class="line"><a name="l00403"></a><span class="lineno">  403</span>&#160;            embedding_size % n_heads == 0</div><div class="line"><a name="l00404"></a><span class="lineno">  404</span>&#160;        ), <span class="stringliteral">&#39;Transformer embedding size must be a multiple of n_heads&#39;</span></div><div class="line"><a name="l00405"></a><span class="lineno">  405</span>&#160;</div><div class="line"><a name="l00406"></a><span class="lineno">  406</span>&#160;        <span class="comment"># check input formats:</span></div><div class="line"><a name="l00407"></a><span class="lineno">  407</span>&#160;        <span class="keywordflow">if</span> embedding <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00408"></a><span class="lineno">  408</span>&#160;            <span class="keyword">assert</span> (</div><div class="line"><a name="l00409"></a><span class="lineno">  409</span>&#160;                embedding_size <span class="keywordflow">is</span> <span class="keywordtype">None</span> <span class="keywordflow">or</span> embedding_size == embedding.weight.shape[1]</div><div class="line"><a name="l00410"></a><span class="lineno">  410</span>&#160;            ), <span class="stringliteral">&quot;Embedding dim must match the embedding size.&quot;</span></div><div class="line"><a name="l00411"></a><span class="lineno">  411</span>&#160;</div><div class="line"><a name="l00412"></a><span class="lineno">  412</span>&#160;        <span class="keywordflow">if</span> embedding <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00413"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a179492c3f7ebaf6ba8fb3a7f912e93ce">  413</a></span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a179492c3f7ebaf6ba8fb3a7f912e93ce">embeddings</a> = embedding</div><div class="line"><a name="l00414"></a><span class="lineno">  414</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00415"></a><span class="lineno">  415</span>&#160;            <span class="keywordflow">raise</span> AssertionError(</div><div class="line"><a name="l00416"></a><span class="lineno">  416</span>&#160;                <span class="stringliteral">&quot;This code should not execute. Left here in case we want to enable it.&quot;</span></div><div class="line"><a name="l00417"></a><span class="lineno">  417</span>&#160;            )</div><div class="line"><a name="l00418"></a><span class="lineno">  418</span>&#160;            <span class="keyword">assert</span> padding_idx <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span></div><div class="line"><a name="l00419"></a><span class="lineno">  419</span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a179492c3f7ebaf6ba8fb3a7f912e93ce">embeddings</a> = nn.Embedding(</div><div class="line"><a name="l00420"></a><span class="lineno">  420</span>&#160;                vocabulary_size, embedding_size, padding_idx=padding_idx</div><div class="line"><a name="l00421"></a><span class="lineno">  421</span>&#160;            )</div><div class="line"><a name="l00422"></a><span class="lineno">  422</span>&#160;            nn.init.normal_(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a179492c3f7ebaf6ba8fb3a7f912e93ce">embeddings</a>.weight, 0, embedding_size ** -0.5)</div><div class="line"><a name="l00423"></a><span class="lineno">  423</span>&#160;</div><div class="line"><a name="l00424"></a><span class="lineno">  424</span>&#160;        <span class="comment"># create the positional embeddings</span></div><div class="line"><a name="l00425"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ad2dd819e6d66d17f69e8bfa5026d192a">  425</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ad2dd819e6d66d17f69e8bfa5026d192a">position_embeddings</a> = nn.Embedding(n_positions, embedding_size)</div><div class="line"><a name="l00426"></a><span class="lineno">  426</span>&#160;        <span class="keywordflow">if</span> <span class="keywordflow">not</span> learn_positional_embeddings:</div><div class="line"><a name="l00427"></a><span class="lineno">  427</span>&#160;            <a class="code" href="namespaceparlai_1_1agents_1_1transformer_1_1modules.html#a0b86437e6e9682fa3100e9cadcaae259">create_position_codes</a>(</div><div class="line"><a name="l00428"></a><span class="lineno">  428</span>&#160;                n_positions, embedding_size, out=self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ad2dd819e6d66d17f69e8bfa5026d192a">position_embeddings</a>.weight</div><div class="line"><a name="l00429"></a><span class="lineno">  429</span>&#160;            )</div><div class="line"><a name="l00430"></a><span class="lineno">  430</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00431"></a><span class="lineno">  431</span>&#160;            nn.init.normal_(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ad2dd819e6d66d17f69e8bfa5026d192a">position_embeddings</a>.weight, 0, embedding_size ** -0.5)</div><div class="line"><a name="l00432"></a><span class="lineno">  432</span>&#160;</div><div class="line"><a name="l00433"></a><span class="lineno">  433</span>&#160;        <span class="comment"># embedding normalization</span></div><div class="line"><a name="l00434"></a><span class="lineno">  434</span>&#160;        <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a6ea46aeb8ca865c97ec8e99746bedd7b">variant</a> == <span class="stringliteral">&#39;xlm&#39;</span>:</div><div class="line"><a name="l00435"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ae8676d5d506949c00d35c34ac41b9fff">  435</a></span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ae8676d5d506949c00d35c34ac41b9fff">norm_embeddings</a> = LayerNorm(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab56fa99f887753677dc7f4d435595114">dim</a>, eps=LAYER_NORM_EPS)</div><div class="line"><a name="l00436"></a><span class="lineno">  436</span>&#160;        <span class="keywordflow">elif</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a6ea46aeb8ca865c97ec8e99746bedd7b">variant</a> == <span class="stringliteral">&#39;aiayn&#39;</span>:</div><div class="line"><a name="l00437"></a><span class="lineno">  437</span>&#160;            <span class="keywordflow">pass</span></div><div class="line"><a name="l00438"></a><span class="lineno">  438</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00439"></a><span class="lineno">  439</span>&#160;            <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Can&#39;t handle --variant {}&quot;</span>.<a class="code" href="namespaceparlai_1_1messenger_1_1core_1_1shared__utils.html#a11f2820425bccac8d8d490ab505bcb79">format</a>(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a6ea46aeb8ca865c97ec8e99746bedd7b">variant</a>))</div><div class="line"><a name="l00440"></a><span class="lineno">  440</span>&#160;</div><div class="line"><a name="l00441"></a><span class="lineno">  441</span>&#160;        <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab5ec60adca3ce418f73e409187a82625">n_segments</a> &gt;= 1:</div><div class="line"><a name="l00442"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#acbc50af3764335ae22ef2ba06a65c4a7">  442</a></span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#acbc50af3764335ae22ef2ba06a65c4a7">segment_embeddings</a> = nn.Embedding(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab5ec60adca3ce418f73e409187a82625">n_segments</a>, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab56fa99f887753677dc7f4d435595114">dim</a>)</div><div class="line"><a name="l00443"></a><span class="lineno">  443</span>&#160;</div><div class="line"><a name="l00444"></a><span class="lineno">  444</span>&#160;        <span class="comment"># build the model</span></div><div class="line"><a name="l00445"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#acbeff4ac94daff4e1627b6d93cd37424">  445</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#acbeff4ac94daff4e1627b6d93cd37424">layers</a> = nn.ModuleList()</div><div class="line"><a name="l00446"></a><span class="lineno">  446</span>&#160;        <span class="keywordflow">for</span> _ <span class="keywordflow">in</span> range(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a1fac9c2d34c4d2268f50fc90b1e74e04">n_layers</a>):</div><div class="line"><a name="l00447"></a><span class="lineno">  447</span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#acbeff4ac94daff4e1627b6d93cd37424">layers</a>.append(</div><div class="line"><a name="l00448"></a><span class="lineno">  448</span>&#160;                <a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html">TransformerEncoderLayer</a>(</div><div class="line"><a name="l00449"></a><span class="lineno">  449</span>&#160;                    n_heads,</div><div class="line"><a name="l00450"></a><span class="lineno">  450</span>&#160;                    embedding_size,</div><div class="line"><a name="l00451"></a><span class="lineno">  451</span>&#160;                    ffn_size,</div><div class="line"><a name="l00452"></a><span class="lineno">  452</span>&#160;                    attention_dropout=attention_dropout,</div><div class="line"><a name="l00453"></a><span class="lineno">  453</span>&#160;                    relu_dropout=relu_dropout,</div><div class="line"><a name="l00454"></a><span class="lineno">  454</span>&#160;                    dropout=dropout,</div><div class="line"><a name="l00455"></a><span class="lineno">  455</span>&#160;                    variant=variant,</div><div class="line"><a name="l00456"></a><span class="lineno">  456</span>&#160;                    activation=activation,</div><div class="line"><a name="l00457"></a><span class="lineno">  457</span>&#160;                )</div><div class="line"><a name="l00458"></a><span class="lineno">  458</span>&#160;            )</div><div class="line"><a name="l00459"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ad904c0d06a4f6851b38fbbc0cab224f9">  459</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ad904c0d06a4f6851b38fbbc0cab224f9">output_scaling</a> = output_scaling</div><div class="line"><a name="l00460"></a><span class="lineno">  460</span>&#160;</div><div class="line"><a name="l00461"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a23005871ba6683940f63511ed463266f">  461</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a23005871ba6683940f63511ed463266f">forward</a>(self, input, positions=None, segments=None):</div><div class="line"><a name="l00462"></a><span class="lineno">  462</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00463"></a><span class="lineno">  463</span>&#160;<span class="stringliteral">        Forward pass.</span></div><div class="line"><a name="l00464"></a><span class="lineno">  464</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00465"></a><span class="lineno">  465</span>&#160;<span class="stringliteral">        :param LongTensor[batch,seqlen] input:</span></div><div class="line"><a name="l00466"></a><span class="lineno">  466</span>&#160;<span class="stringliteral">            The input IDs</span></div><div class="line"><a name="l00467"></a><span class="lineno">  467</span>&#160;<span class="stringliteral">        :param BoolTensor[batch,seqlen] mask:</span></div><div class="line"><a name="l00468"></a><span class="lineno">  468</span>&#160;<span class="stringliteral">            The attention mask; 1 means attend, 0 means ignore.</span></div><div class="line"><a name="l00469"></a><span class="lineno">  469</span>&#160;<span class="stringliteral">        :param LongTensor[batch,seqlen]:</span></div><div class="line"><a name="l00470"></a><span class="lineno">  470</span>&#160;<span class="stringliteral">            If provided, additionally adds ``segments`` as extra embedding features.</span></div><div class="line"><a name="l00471"></a><span class="lineno">  471</span>&#160;<span class="stringliteral">        &quot;&quot;&quot;</span></div><div class="line"><a name="l00472"></a><span class="lineno">  472</span>&#160;        mask = input != self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a74702a13d7e41afc3e77c94bb60bd119">padding_idx</a></div><div class="line"><a name="l00473"></a><span class="lineno">  473</span>&#160;        <span class="keywordflow">if</span> positions <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00474"></a><span class="lineno">  474</span>&#160;            positions = (mask.cumsum(dim=1, dtype=torch.int64) - 1).clamp_(min=0)</div><div class="line"><a name="l00475"></a><span class="lineno">  475</span>&#160;        tensor = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a179492c3f7ebaf6ba8fb3a7f912e93ce">embeddings</a>(input)</div><div class="line"><a name="l00476"></a><span class="lineno">  476</span>&#160;        <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a80122462ebd32da923d7b4479053ab01">embeddings_scale</a>:</div><div class="line"><a name="l00477"></a><span class="lineno">  477</span>&#160;            tensor = tensor * np.sqrt(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab56fa99f887753677dc7f4d435595114">dim</a>)</div><div class="line"><a name="l00478"></a><span class="lineno">  478</span>&#160;</div><div class="line"><a name="l00479"></a><span class="lineno">  479</span>&#160;        <span class="keywordflow">if</span> positions.max().item() &gt; self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab885dd6b80284dbd2837ecaf75f96cb0">n_positions</a>:</div><div class="line"><a name="l00480"></a><span class="lineno">  480</span>&#160;            <a class="code" href="namespaceparlai_1_1utils_1_1misc.html#acf146e70ea7f6867969a7c2b545d4b4b">warn_once</a>(</div><div class="line"><a name="l00481"></a><span class="lineno">  481</span>&#160;                <span class="stringliteral">&#39;You are inputting a sequence of {x} length, but only have &#39;</span></div><div class="line"><a name="l00482"></a><span class="lineno">  482</span>&#160;                <span class="stringliteral">&#39;--n-positions {y}. Set --truncate or increase --n-positions&#39;</span>.<a class="code" href="namespaceparlai_1_1messenger_1_1core_1_1shared__utils.html#a11f2820425bccac8d8d490ab505bcb79">format</a>(</div><div class="line"><a name="l00483"></a><span class="lineno">  483</span>&#160;                    x=positions.max().item(), y=self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab885dd6b80284dbd2837ecaf75f96cb0">n_positions</a></div><div class="line"><a name="l00484"></a><span class="lineno">  484</span>&#160;                )</div><div class="line"><a name="l00485"></a><span class="lineno">  485</span>&#160;            )</div><div class="line"><a name="l00486"></a><span class="lineno">  486</span>&#160;        position_embs = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ad2dd819e6d66d17f69e8bfa5026d192a">position_embeddings</a>(positions).expand_as(tensor)</div><div class="line"><a name="l00487"></a><span class="lineno">  487</span>&#160;        tensor = tensor + position_embs</div><div class="line"><a name="l00488"></a><span class="lineno">  488</span>&#160;</div><div class="line"><a name="l00489"></a><span class="lineno">  489</span>&#160;        <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab5ec60adca3ce418f73e409187a82625">n_segments</a> &gt;= 1:</div><div class="line"><a name="l00490"></a><span class="lineno">  490</span>&#160;            <span class="keywordflow">if</span> segments <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00491"></a><span class="lineno">  491</span>&#160;                segments = torch.zeros_like(input)</div><div class="line"><a name="l00492"></a><span class="lineno">  492</span>&#160;            tensor = tensor + self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#acbc50af3764335ae22ef2ba06a65c4a7">segment_embeddings</a>(segments)</div><div class="line"><a name="l00493"></a><span class="lineno">  493</span>&#160;</div><div class="line"><a name="l00494"></a><span class="lineno">  494</span>&#160;        <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a6ea46aeb8ca865c97ec8e99746bedd7b">variant</a> == <span class="stringliteral">&#39;xlm&#39;</span>:</div><div class="line"><a name="l00495"></a><span class="lineno">  495</span>&#160;            tensor = _normalize(tensor, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ae8676d5d506949c00d35c34ac41b9fff">norm_embeddings</a>)</div><div class="line"><a name="l00496"></a><span class="lineno">  496</span>&#160;</div><div class="line"><a name="l00497"></a><span class="lineno">  497</span>&#160;        <span class="comment"># --dropout on the embeddings</span></div><div class="line"><a name="l00498"></a><span class="lineno">  498</span>&#160;        tensor = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#af28dbf5c6c0f70c1a032a3bbb5359661">dropout</a>(tensor)</div><div class="line"><a name="l00499"></a><span class="lineno">  499</span>&#160;</div><div class="line"><a name="l00500"></a><span class="lineno">  500</span>&#160;        tensor *= mask.unsqueeze(-1).type_as(tensor)</div><div class="line"><a name="l00501"></a><span class="lineno">  501</span>&#160;        <span class="keywordflow">for</span> i <span class="keywordflow">in</span> range(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a1fac9c2d34c4d2268f50fc90b1e74e04">n_layers</a>):</div><div class="line"><a name="l00502"></a><span class="lineno">  502</span>&#160;            tensor = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#acbeff4ac94daff4e1627b6d93cd37424">layers</a>[i](tensor, mask)</div><div class="line"><a name="l00503"></a><span class="lineno">  503</span>&#160;</div><div class="line"><a name="l00504"></a><span class="lineno">  504</span>&#160;        tensor *= self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ad904c0d06a4f6851b38fbbc0cab224f9">output_scaling</a></div><div class="line"><a name="l00505"></a><span class="lineno">  505</span>&#160;        <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a2703f337fd44344ccd93743e57be554e">reduction_type</a> == <span class="stringliteral">&#39;first&#39;</span>:</div><div class="line"><a name="l00506"></a><span class="lineno">  506</span>&#160;            <span class="keywordflow">return</span> tensor[:, 0, :]</div><div class="line"><a name="l00507"></a><span class="lineno">  507</span>&#160;        <span class="keywordflow">elif</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a2703f337fd44344ccd93743e57be554e">reduction_type</a> == <span class="stringliteral">&#39;max&#39;</span>:</div><div class="line"><a name="l00508"></a><span class="lineno">  508</span>&#160;            <span class="keywordflow">return</span> tensor.max(dim=1)[0]</div><div class="line"><a name="l00509"></a><span class="lineno">  509</span>&#160;        <span class="keywordflow">elif</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a2703f337fd44344ccd93743e57be554e">reduction_type</a> == <span class="stringliteral">&#39;mean&#39;</span>:</div><div class="line"><a name="l00510"></a><span class="lineno">  510</span>&#160;            divisor = mask.float().sum(dim=1).unsqueeze(-1).clamp(min=1).type_as(tensor)</div><div class="line"><a name="l00511"></a><span class="lineno">  511</span>&#160;            output = tensor.sum(dim=1) / divisor</div><div class="line"><a name="l00512"></a><span class="lineno">  512</span>&#160;            <span class="keywordflow">return</span> output</div><div class="line"><a name="l00513"></a><span class="lineno">  513</span>&#160;        <span class="keywordflow">elif</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a2703f337fd44344ccd93743e57be554e">reduction_type</a> <span class="keywordflow">is</span> <span class="keywordtype">None</span> <span class="keywordflow">or</span> <span class="stringliteral">&#39;none&#39;</span> <span class="keywordflow">in</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a2703f337fd44344ccd93743e57be554e">reduction_type</a>:</div><div class="line"><a name="l00514"></a><span class="lineno">  514</span>&#160;            output = tensor</div><div class="line"><a name="l00515"></a><span class="lineno">  515</span>&#160;            ret = (output, mask)</div><div class="line"><a name="l00516"></a><span class="lineno">  516</span>&#160;            <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a2703f337fd44344ccd93743e57be554e">reduction_type</a> == <span class="stringliteral">&#39;none_with_pos_embs&#39;</span>:</div><div class="line"><a name="l00517"></a><span class="lineno">  517</span>&#160;                ret = (output, mask, position_embs)</div><div class="line"><a name="l00518"></a><span class="lineno">  518</span>&#160;            <span class="keywordflow">return</span> ret</div><div class="line"><a name="l00519"></a><span class="lineno">  519</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00520"></a><span class="lineno">  520</span>&#160;            <span class="keywordflow">raise</span> ValueError(</div><div class="line"><a name="l00521"></a><span class="lineno">  521</span>&#160;                <span class="stringliteral">&quot;Can&#39;t handle --reduction-type {}&quot;</span>.<a class="code" href="namespaceparlai_1_1messenger_1_1core_1_1shared__utils.html#a11f2820425bccac8d8d490ab505bcb79">format</a>(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a2703f337fd44344ccd93743e57be554e">reduction_type</a>)</div><div class="line"><a name="l00522"></a><span class="lineno">  522</span>&#160;            )</div><div class="line"><a name="l00523"></a><span class="lineno">  523</span>&#160;</div><div class="line"><a name="l00524"></a><span class="lineno">  524</span>&#160;</div><div class="line"><a name="l00525"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html">  525</a></span>&#160;<span class="keyword">class </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html">TransformerEncoderLayer</a>(nn.Module):</div><div class="line"><a name="l00526"></a><span class="lineno">  526</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;Implements a single Transformer encoder layer.&quot;&quot;&quot;</span></div><div class="line"><a name="l00527"></a><span class="lineno">  527</span>&#160;</div><div class="line"><a name="l00528"></a><span class="lineno">  528</span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a68caa3f119bfb946e34b7cf3c8bd9e48">__init__</a>(</div><div class="line"><a name="l00529"></a><span class="lineno">  529</span>&#160;        self,</div><div class="line"><a name="l00530"></a><span class="lineno">  530</span>&#160;        n_heads,</div><div class="line"><a name="l00531"></a><span class="lineno">  531</span>&#160;        embedding_size,</div><div class="line"><a name="l00532"></a><span class="lineno">  532</span>&#160;        ffn_size,</div><div class="line"><a name="l00533"></a><span class="lineno">  533</span>&#160;        attention_dropout=0.0,</div><div class="line"><a name="l00534"></a><span class="lineno">  534</span>&#160;        relu_dropout=0.0,</div><div class="line"><a name="l00535"></a><span class="lineno">  535</span>&#160;        dropout=0.0,</div><div class="line"><a name="l00536"></a><span class="lineno">  536</span>&#160;        activation=&#39;relu&#39;,</div><div class="line"><a name="l00537"></a><span class="lineno">  537</span>&#160;        variant=None,</div><div class="line"><a name="l00538"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a68caa3f119bfb946e34b7cf3c8bd9e48">  538</a></span>&#160;    ):</div><div class="line"><a name="l00539"></a><span class="lineno">  539</span>&#160;        super().<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a68caa3f119bfb946e34b7cf3c8bd9e48">__init__</a>()</div><div class="line"><a name="l00540"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#ae0f31a52dac8d678fdf6ba573201e7d7">  540</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#ae0f31a52dac8d678fdf6ba573201e7d7">dim</a> = embedding_size</div><div class="line"><a name="l00541"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#abcdb8b9abb049afacdd132d2a811faff">  541</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#abcdb8b9abb049afacdd132d2a811faff">ffn_dim</a> = ffn_size</div><div class="line"><a name="l00542"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a2986c3841832cc56d76908347f22a0a3">  542</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a2986c3841832cc56d76908347f22a0a3">activation</a> = activation</div><div class="line"><a name="l00543"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a33ff5864b155499ecf8d31383b33e8d8">  543</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a33ff5864b155499ecf8d31383b33e8d8">variant</a> = variant</div><div class="line"><a name="l00544"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#ad8b0a3b903ca2595e72dd54f82f4dd9f">  544</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#ad8b0a3b903ca2595e72dd54f82f4dd9f">attention</a> = <a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html">MultiHeadAttention</a>(</div><div class="line"><a name="l00545"></a><span class="lineno">  545</span>&#160;            n_heads, embedding_size, dropout=attention_dropout  <span class="comment"># --attention-dropout</span></div><div class="line"><a name="l00546"></a><span class="lineno">  546</span>&#160;        )</div><div class="line"><a name="l00547"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a74730602aaea5afd4564d0f35c84af15">  547</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a74730602aaea5afd4564d0f35c84af15">norm1</a> = LayerNorm(embedding_size, eps=LAYER_NORM_EPS)</div><div class="line"><a name="l00548"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a2f40ac67a332293a3f51eb11647e2ec1">  548</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a2f40ac67a332293a3f51eb11647e2ec1">ffn</a> = <a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html">TransformerFFN</a>(</div><div class="line"><a name="l00549"></a><span class="lineno">  549</span>&#160;            embedding_size,</div><div class="line"><a name="l00550"></a><span class="lineno">  550</span>&#160;            ffn_size,</div><div class="line"><a name="l00551"></a><span class="lineno">  551</span>&#160;            relu_dropout=relu_dropout,</div><div class="line"><a name="l00552"></a><span class="lineno">  552</span>&#160;            activation=self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a2986c3841832cc56d76908347f22a0a3">activation</a>,</div><div class="line"><a name="l00553"></a><span class="lineno">  553</span>&#160;        )</div><div class="line"><a name="l00554"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#acc0beb3a5c95f3ab45b13dff322e4225">  554</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#acc0beb3a5c95f3ab45b13dff322e4225">norm2</a> = LayerNorm(embedding_size, eps=LAYER_NORM_EPS)</div><div class="line"><a name="l00555"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a3bcd00a8073a48b6a1626fccb87be143">  555</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a3bcd00a8073a48b6a1626fccb87be143">dropout</a> = nn.Dropout(p=dropout)</div><div class="line"><a name="l00556"></a><span class="lineno">  556</span>&#160;</div><div class="line"><a name="l00557"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#aba0474eb3a08b6c482648adc0804776e">  557</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#aba0474eb3a08b6c482648adc0804776e">forward</a>(self, tensor, mask):</div><div class="line"><a name="l00558"></a><span class="lineno">  558</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;Forward pass.&quot;&quot;&quot;</span></div><div class="line"><a name="l00559"></a><span class="lineno">  559</span>&#160;        tensor = tensor + self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a3bcd00a8073a48b6a1626fccb87be143">dropout</a>(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#ad8b0a3b903ca2595e72dd54f82f4dd9f">attention</a>(tensor, mask=mask))</div><div class="line"><a name="l00560"></a><span class="lineno">  560</span>&#160;        tensor = _normalize(tensor, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a74730602aaea5afd4564d0f35c84af15">norm1</a>)</div><div class="line"><a name="l00561"></a><span class="lineno">  561</span>&#160;        tensor = tensor + self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a3bcd00a8073a48b6a1626fccb87be143">dropout</a>(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a2f40ac67a332293a3f51eb11647e2ec1">ffn</a>(tensor))</div><div class="line"><a name="l00562"></a><span class="lineno">  562</span>&#160;        tensor = _normalize(tensor, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#acc0beb3a5c95f3ab45b13dff322e4225">norm2</a>)</div><div class="line"><a name="l00563"></a><span class="lineno">  563</span>&#160;        tensor *= mask.unsqueeze(-1).type_as(tensor)</div><div class="line"><a name="l00564"></a><span class="lineno">  564</span>&#160;        <span class="keywordflow">return</span> tensor</div><div class="line"><a name="l00565"></a><span class="lineno">  565</span>&#160;</div><div class="line"><a name="l00566"></a><span class="lineno">  566</span>&#160;</div><div class="line"><a name="l00567"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html">  567</a></span>&#160;<span class="keyword">class </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html">TransformerDecoder</a>(nn.Module):</div><div class="line"><a name="l00568"></a><span class="lineno">  568</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00569"></a><span class="lineno">  569</span>&#160;<span class="stringliteral">    Transformer Decoder layer.</span></div><div class="line"><a name="l00570"></a><span class="lineno">  570</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00571"></a><span class="lineno">  571</span>&#160;<span class="stringliteral">    :param int n_heads: the number of multihead attention heads.</span></div><div class="line"><a name="l00572"></a><span class="lineno">  572</span>&#160;<span class="stringliteral">    :param int n_layers: number of transformer layers.</span></div><div class="line"><a name="l00573"></a><span class="lineno">  573</span>&#160;<span class="stringliteral">    :param int embedding_size: the embedding sizes. Must be a multiple of n_heads.</span></div><div class="line"><a name="l00574"></a><span class="lineno">  574</span>&#160;<span class="stringliteral">    :param int ffn_size: the size of the hidden layer in the FFN</span></div><div class="line"><a name="l00575"></a><span class="lineno">  575</span>&#160;<span class="stringliteral">    :param embedding: an embedding matrix for the bottom layer of the transformer.</span></div><div class="line"><a name="l00576"></a><span class="lineno">  576</span>&#160;<span class="stringliteral">        If none, one is created for this encoder.</span></div><div class="line"><a name="l00577"></a><span class="lineno">  577</span>&#160;<span class="stringliteral">    :param float dropout: Dropout used around embeddings and before layer</span></div><div class="line"><a name="l00578"></a><span class="lineno">  578</span>&#160;<span class="stringliteral">        layer normalizations. This is used in Vaswani 2017 and works well on</span></div><div class="line"><a name="l00579"></a><span class="lineno">  579</span>&#160;<span class="stringliteral">        large datasets.</span></div><div class="line"><a name="l00580"></a><span class="lineno">  580</span>&#160;<span class="stringliteral">    :param float attention_dropout: Dropout performed after the multhead attention</span></div><div class="line"><a name="l00581"></a><span class="lineno">  581</span>&#160;<span class="stringliteral">        softmax. This is not used in Vaswani 2017.</span></div><div class="line"><a name="l00582"></a><span class="lineno">  582</span>&#160;<span class="stringliteral">    :param float relu_attention: Dropout used after the ReLU in the FFN. Not used</span></div><div class="line"><a name="l00583"></a><span class="lineno">  583</span>&#160;<span class="stringliteral">        in Vaswani 2017, but used in Tensor2Tensor.</span></div><div class="line"><a name="l00584"></a><span class="lineno">  584</span>&#160;<span class="stringliteral">    :param int padding_idx: Reserved padding index in the embeddings matrix.</span></div><div class="line"><a name="l00585"></a><span class="lineno">  585</span>&#160;<span class="stringliteral">    :param bool learn_positional_embeddings: If off, sinusoidal embeddings are</span></div><div class="line"><a name="l00586"></a><span class="lineno">  586</span>&#160;<span class="stringliteral">        used. If on, position embeddings are learned from scratch.</span></div><div class="line"><a name="l00587"></a><span class="lineno">  587</span>&#160;<span class="stringliteral">    :param bool embeddings_scale: Scale embeddings relative to their dimensionality.</span></div><div class="line"><a name="l00588"></a><span class="lineno">  588</span>&#160;<span class="stringliteral">        Found useful in fairseq.</span></div><div class="line"><a name="l00589"></a><span class="lineno">  589</span>&#160;<span class="stringliteral">    :param int n_positions: Size of the position embeddings matrix.</span></div><div class="line"><a name="l00590"></a><span class="lineno">  590</span>&#160;<span class="stringliteral">    &quot;&quot;&quot;</span></div><div class="line"><a name="l00591"></a><span class="lineno">  591</span>&#160;</div><div class="line"><a name="l00592"></a><span class="lineno">  592</span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#ac9356241c0dd44bf980a155124cbde59">__init__</a>(</div><div class="line"><a name="l00593"></a><span class="lineno">  593</span>&#160;        self,</div><div class="line"><a name="l00594"></a><span class="lineno">  594</span>&#160;        n_heads,</div><div class="line"><a name="l00595"></a><span class="lineno">  595</span>&#160;        n_layers,</div><div class="line"><a name="l00596"></a><span class="lineno">  596</span>&#160;        embedding_size,</div><div class="line"><a name="l00597"></a><span class="lineno">  597</span>&#160;        ffn_size,</div><div class="line"><a name="l00598"></a><span class="lineno">  598</span>&#160;        vocabulary_size,</div><div class="line"><a name="l00599"></a><span class="lineno">  599</span>&#160;        embedding=None,</div><div class="line"><a name="l00600"></a><span class="lineno">  600</span>&#160;        dropout=0.0,</div><div class="line"><a name="l00601"></a><span class="lineno">  601</span>&#160;        attention_dropout=0.0,</div><div class="line"><a name="l00602"></a><span class="lineno">  602</span>&#160;        relu_dropout=0.0,</div><div class="line"><a name="l00603"></a><span class="lineno">  603</span>&#160;        embeddings_scale=True,</div><div class="line"><a name="l00604"></a><span class="lineno">  604</span>&#160;        learn_positional_embeddings=False,</div><div class="line"><a name="l00605"></a><span class="lineno">  605</span>&#160;        padding_idx=None,</div><div class="line"><a name="l00606"></a><span class="lineno">  606</span>&#160;        n_positions=1024,</div><div class="line"><a name="l00607"></a><span class="lineno">  607</span>&#160;        n_segments=0,</div><div class="line"><a name="l00608"></a><span class="lineno">  608</span>&#160;        variant=&#39;aiayn&#39;,</div><div class="line"><a name="l00609"></a><span class="lineno">  609</span>&#160;        activation=&#39;relu&#39;,</div><div class="line"><a name="l00610"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#ac9356241c0dd44bf980a155124cbde59">  610</a></span>&#160;    ):</div><div class="line"><a name="l00611"></a><span class="lineno">  611</span>&#160;        super().<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#ac9356241c0dd44bf980a155124cbde59">__init__</a>()</div><div class="line"><a name="l00612"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#abb275df1d00ad62deb2424266f1563d6">  612</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#abb275df1d00ad62deb2424266f1563d6">embedding_size</a> = embedding_size</div><div class="line"><a name="l00613"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a90c40d660300194a42a18dc38f7e9fb2">  613</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a90c40d660300194a42a18dc38f7e9fb2">ffn_size</a> = ffn_size</div><div class="line"><a name="l00614"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a20983026d2ed3edd48698ed08d666287">  614</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a20983026d2ed3edd48698ed08d666287">n_layers</a> = n_layers</div><div class="line"><a name="l00615"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#af83edca868bacf80329be8606ffc7efb">  615</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#af83edca868bacf80329be8606ffc7efb">n_heads</a> = n_heads</div><div class="line"><a name="l00616"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#ab29b2b4f3a44d3069d208c61ddba6f21">  616</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#ab29b2b4f3a44d3069d208c61ddba6f21">dim</a> = embedding_size</div><div class="line"><a name="l00617"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#aced85fffc20320f29be295f94e686a63">  617</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#aced85fffc20320f29be295f94e686a63">activation</a> = activation</div><div class="line"><a name="l00618"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a84349da2592ce07a34257a3995dd2254">  618</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a84349da2592ce07a34257a3995dd2254">variant</a> = variant</div><div class="line"><a name="l00619"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a45a356a825e02ed79cdc43cf7f5dc8af">  619</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a45a356a825e02ed79cdc43cf7f5dc8af">embeddings_scale</a> = embeddings_scale</div><div class="line"><a name="l00620"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a0d41303200949e8406daf8426ce7cbb3">  620</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a0d41303200949e8406daf8426ce7cbb3">dropout</a> = nn.Dropout(p=dropout)  <span class="comment"># --dropout</span></div><div class="line"><a name="l00621"></a><span class="lineno">  621</span>&#160;</div><div class="line"><a name="l00622"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a91210271cf67a77c0ab81f03f29bf70d">  622</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a91210271cf67a77c0ab81f03f29bf70d">n_positions</a> = n_positions</div><div class="line"><a name="l00623"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a18e16e542264d07a8c70f0c5b0d2e5ce">  623</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a18e16e542264d07a8c70f0c5b0d2e5ce">out_dim</a> = embedding_size</div><div class="line"><a name="l00624"></a><span class="lineno">  624</span>&#160;        <span class="keyword">assert</span> (</div><div class="line"><a name="l00625"></a><span class="lineno">  625</span>&#160;            embedding_size % n_heads == 0</div><div class="line"><a name="l00626"></a><span class="lineno">  626</span>&#160;        ), <span class="stringliteral">&#39;Transformer embedding size must be a multiple of n_heads&#39;</span></div><div class="line"><a name="l00627"></a><span class="lineno">  627</span>&#160;</div><div class="line"><a name="l00628"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#aa6d97e0eeed7b3cc11e4b60fb4a86167">  628</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#aa6d97e0eeed7b3cc11e4b60fb4a86167">embeddings</a> = embedding</div><div class="line"><a name="l00629"></a><span class="lineno">  629</span>&#160;</div><div class="line"><a name="l00630"></a><span class="lineno">  630</span>&#160;        <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a84349da2592ce07a34257a3995dd2254">variant</a> == <span class="stringliteral">&#39;xlm&#39;</span>:</div><div class="line"><a name="l00631"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a4a930028ffdff0380897228bc156fe74">  631</a></span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a4a930028ffdff0380897228bc156fe74">norm_embeddings</a> = LayerNorm(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#ab29b2b4f3a44d3069d208c61ddba6f21">dim</a>, eps=LAYER_NORM_EPS)</div><div class="line"><a name="l00632"></a><span class="lineno">  632</span>&#160;        <span class="keywordflow">elif</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a84349da2592ce07a34257a3995dd2254">variant</a> == <span class="stringliteral">&#39;aiayn&#39;</span>:</div><div class="line"><a name="l00633"></a><span class="lineno">  633</span>&#160;            <span class="keywordflow">pass</span></div><div class="line"><a name="l00634"></a><span class="lineno">  634</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00635"></a><span class="lineno">  635</span>&#160;            <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&quot;Can&#39;t handle --variant {}&quot;</span>.<a class="code" href="namespaceparlai_1_1messenger_1_1core_1_1shared__utils.html#a11f2820425bccac8d8d490ab505bcb79">format</a>(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a84349da2592ce07a34257a3995dd2254">variant</a>))</div><div class="line"><a name="l00636"></a><span class="lineno">  636</span>&#160;</div><div class="line"><a name="l00637"></a><span class="lineno">  637</span>&#160;        <span class="comment"># create the positional embeddings</span></div><div class="line"><a name="l00638"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a253da75d3014b859206169b2d692ab07">  638</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a253da75d3014b859206169b2d692ab07">position_embeddings</a> = nn.Embedding(n_positions, embedding_size)</div><div class="line"><a name="l00639"></a><span class="lineno">  639</span>&#160;        <span class="keywordflow">if</span> <span class="keywordflow">not</span> learn_positional_embeddings:</div><div class="line"><a name="l00640"></a><span class="lineno">  640</span>&#160;            <a class="code" href="namespaceparlai_1_1agents_1_1transformer_1_1modules.html#a0b86437e6e9682fa3100e9cadcaae259">create_position_codes</a>(</div><div class="line"><a name="l00641"></a><span class="lineno">  641</span>&#160;                n_positions, embedding_size, out=self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a253da75d3014b859206169b2d692ab07">position_embeddings</a>.weight</div><div class="line"><a name="l00642"></a><span class="lineno">  642</span>&#160;            )</div><div class="line"><a name="l00643"></a><span class="lineno">  643</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00644"></a><span class="lineno">  644</span>&#160;            nn.init.normal_(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a253da75d3014b859206169b2d692ab07">position_embeddings</a>.weight, 0, embedding_size ** -0.5)</div><div class="line"><a name="l00645"></a><span class="lineno">  645</span>&#160;</div><div class="line"><a name="l00646"></a><span class="lineno">  646</span>&#160;        <span class="comment"># build the model</span></div><div class="line"><a name="l00647"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a5007810b22cdfddf768bdd94c233f87e">  647</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a5007810b22cdfddf768bdd94c233f87e">layers</a> = nn.ModuleList()</div><div class="line"><a name="l00648"></a><span class="lineno">  648</span>&#160;        <span class="keywordflow">for</span> _ <span class="keywordflow">in</span> range(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a20983026d2ed3edd48698ed08d666287">n_layers</a>):</div><div class="line"><a name="l00649"></a><span class="lineno">  649</span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a5007810b22cdfddf768bdd94c233f87e">layers</a>.append(</div><div class="line"><a name="l00650"></a><span class="lineno">  650</span>&#160;                <a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html">TransformerDecoderLayer</a>(</div><div class="line"><a name="l00651"></a><span class="lineno">  651</span>&#160;                    n_heads,</div><div class="line"><a name="l00652"></a><span class="lineno">  652</span>&#160;                    embedding_size,</div><div class="line"><a name="l00653"></a><span class="lineno">  653</span>&#160;                    ffn_size,</div><div class="line"><a name="l00654"></a><span class="lineno">  654</span>&#160;                    attention_dropout=attention_dropout,</div><div class="line"><a name="l00655"></a><span class="lineno">  655</span>&#160;                    relu_dropout=relu_dropout,</div><div class="line"><a name="l00656"></a><span class="lineno">  656</span>&#160;                    dropout=dropout,</div><div class="line"><a name="l00657"></a><span class="lineno">  657</span>&#160;                    activation=activation,</div><div class="line"><a name="l00658"></a><span class="lineno">  658</span>&#160;                    variant=variant,</div><div class="line"><a name="l00659"></a><span class="lineno">  659</span>&#160;                )</div><div class="line"><a name="l00660"></a><span class="lineno">  660</span>&#160;            )</div><div class="line"><a name="l00661"></a><span class="lineno">  661</span>&#160;</div><div class="line"><a name="l00662"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a19b2e9eecef1384bcfa9f772cac24a20">  662</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a19b2e9eecef1384bcfa9f772cac24a20">forward</a>(self, input, encoder_state, incr_state=None):</div><div class="line"><a name="l00663"></a><span class="lineno">  663</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00664"></a><span class="lineno">  664</span>&#160;<span class="stringliteral">        Forward pass.</span></div><div class="line"><a name="l00665"></a><span class="lineno">  665</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00666"></a><span class="lineno">  666</span>&#160;<span class="stringliteral">        :param LongTensor[batch,seqlen] input:</span></div><div class="line"><a name="l00667"></a><span class="lineno">  667</span>&#160;<span class="stringliteral">            The decoder inputs (partial or full decoded token IDs).</span></div><div class="line"><a name="l00668"></a><span class="lineno">  668</span>&#160;<span class="stringliteral">        :param encoder_state:</span></div><div class="line"><a name="l00669"></a><span class="lineno">  669</span>&#160;<span class="stringliteral">            Output from the encoder module forward pass.</span></div><div class="line"><a name="l00670"></a><span class="lineno">  670</span>&#160;<span class="stringliteral">        :param incr_state:</span></div><div class="line"><a name="l00671"></a><span class="lineno">  671</span>&#160;<span class="stringliteral">            Ignored. Should always be ``None`` in this version.</span></div><div class="line"><a name="l00672"></a><span class="lineno">  672</span>&#160;<span class="stringliteral">        &quot;&quot;&quot;</span></div><div class="line"><a name="l00673"></a><span class="lineno">  673</span>&#160;        encoder_output, encoder_mask = encoder_state</div><div class="line"><a name="l00674"></a><span class="lineno">  674</span>&#160;</div><div class="line"><a name="l00675"></a><span class="lineno">  675</span>&#160;        seq_len = input.size(1)</div><div class="line"><a name="l00676"></a><span class="lineno">  676</span>&#160;        positions = input.new(seq_len).long()</div><div class="line"><a name="l00677"></a><span class="lineno">  677</span>&#160;        positions = torch.arange(seq_len, out=positions).unsqueeze(0)</div><div class="line"><a name="l00678"></a><span class="lineno">  678</span>&#160;        tensor = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#aa6d97e0eeed7b3cc11e4b60fb4a86167">embeddings</a>(input)</div><div class="line"><a name="l00679"></a><span class="lineno">  679</span>&#160;        <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a45a356a825e02ed79cdc43cf7f5dc8af">embeddings_scale</a>:</div><div class="line"><a name="l00680"></a><span class="lineno">  680</span>&#160;            tensor = tensor * np.sqrt(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#ab29b2b4f3a44d3069d208c61ddba6f21">dim</a>)</div><div class="line"><a name="l00681"></a><span class="lineno">  681</span>&#160;        <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a84349da2592ce07a34257a3995dd2254">variant</a> == <span class="stringliteral">&#39;xlm&#39;</span>:</div><div class="line"><a name="l00682"></a><span class="lineno">  682</span>&#160;            tensor = _normalize(tensor, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a4a930028ffdff0380897228bc156fe74">norm_embeddings</a>)</div><div class="line"><a name="l00683"></a><span class="lineno">  683</span>&#160;        <span class="keywordflow">if</span> positions.max().item() &gt; self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a91210271cf67a77c0ab81f03f29bf70d">n_positions</a>:</div><div class="line"><a name="l00684"></a><span class="lineno">  684</span>&#160;            <a class="code" href="namespaceparlai_1_1utils_1_1misc.html#acf146e70ea7f6867969a7c2b545d4b4b">warn_once</a>(</div><div class="line"><a name="l00685"></a><span class="lineno">  685</span>&#160;                <span class="stringliteral">&#39;You are inputting a sequence of {x} length, but only have &#39;</span></div><div class="line"><a name="l00686"></a><span class="lineno">  686</span>&#160;                <span class="stringliteral">&#39;--n-positions {y}. Set --truncate or increase --n-positions&#39;</span>.<a class="code" href="namespaceparlai_1_1messenger_1_1core_1_1shared__utils.html#a11f2820425bccac8d8d490ab505bcb79">format</a>(</div><div class="line"><a name="l00687"></a><span class="lineno">  687</span>&#160;                    x=positions.max().item(), y=self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a91210271cf67a77c0ab81f03f29bf70d">n_positions</a></div><div class="line"><a name="l00688"></a><span class="lineno">  688</span>&#160;                )</div><div class="line"><a name="l00689"></a><span class="lineno">  689</span>&#160;            )</div><div class="line"><a name="l00690"></a><span class="lineno">  690</span>&#160;        tensor = tensor + self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a253da75d3014b859206169b2d692ab07">position_embeddings</a>(positions).expand_as(tensor)</div><div class="line"><a name="l00691"></a><span class="lineno">  691</span>&#160;        tensor = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a0d41303200949e8406daf8426ce7cbb3">dropout</a>(tensor)  <span class="comment"># --dropout</span></div><div class="line"><a name="l00692"></a><span class="lineno">  692</span>&#160;</div><div class="line"><a name="l00693"></a><span class="lineno">  693</span>&#160;        <span class="keywordflow">for</span> layer <span class="keywordflow">in</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a5007810b22cdfddf768bdd94c233f87e">layers</a>:</div><div class="line"><a name="l00694"></a><span class="lineno">  694</span>&#160;            tensor = layer(tensor, encoder_output, encoder_mask)</div><div class="line"><a name="l00695"></a><span class="lineno">  695</span>&#160;</div><div class="line"><a name="l00696"></a><span class="lineno">  696</span>&#160;        <span class="keywordflow">return</span> tensor, <span class="keywordtype">None</span></div><div class="line"><a name="l00697"></a><span class="lineno">  697</span>&#160;</div><div class="line"><a name="l00698"></a><span class="lineno">  698</span>&#160;</div><div class="line"><a name="l00699"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html">  699</a></span>&#160;<span class="keyword">class </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html">TransformerDecoderLayer</a>(nn.Module):</div><div class="line"><a name="l00700"></a><span class="lineno">  700</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00701"></a><span class="lineno">  701</span>&#160;<span class="stringliteral">    Implements a single Transformer decoder layer.</span></div><div class="line"><a name="l00702"></a><span class="lineno">  702</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00703"></a><span class="lineno">  703</span>&#160;<span class="stringliteral">    Decoder layers are similar to encoder layers but:</span></div><div class="line"><a name="l00704"></a><span class="lineno">  704</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00705"></a><span class="lineno">  705</span>&#160;<span class="stringliteral">    1. Self-attention is limited in a casaul (auto-regressive) manner.</span></div><div class="line"><a name="l00706"></a><span class="lineno">  706</span>&#160;<span class="stringliteral">    2. Attend over all of the encoder states.</span></div><div class="line"><a name="l00707"></a><span class="lineno">  707</span>&#160;<span class="stringliteral">    &quot;&quot;&quot;</span></div><div class="line"><a name="l00708"></a><span class="lineno">  708</span>&#160;</div><div class="line"><a name="l00709"></a><span class="lineno">  709</span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ad893ac51805fd662173f777c91b2cca4">__init__</a>(</div><div class="line"><a name="l00710"></a><span class="lineno">  710</span>&#160;        self,</div><div class="line"><a name="l00711"></a><span class="lineno">  711</span>&#160;        n_heads,</div><div class="line"><a name="l00712"></a><span class="lineno">  712</span>&#160;        embedding_size,</div><div class="line"><a name="l00713"></a><span class="lineno">  713</span>&#160;        ffn_size,</div><div class="line"><a name="l00714"></a><span class="lineno">  714</span>&#160;        attention_dropout=0.0,</div><div class="line"><a name="l00715"></a><span class="lineno">  715</span>&#160;        relu_dropout=0.0,</div><div class="line"><a name="l00716"></a><span class="lineno">  716</span>&#160;        dropout=0.0,</div><div class="line"><a name="l00717"></a><span class="lineno">  717</span>&#160;        activation=&#39;relu&#39;,</div><div class="line"><a name="l00718"></a><span class="lineno">  718</span>&#160;        variant=&#39;aiayn&#39;,</div><div class="line"><a name="l00719"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ad893ac51805fd662173f777c91b2cca4">  719</a></span>&#160;    ):</div><div class="line"><a name="l00720"></a><span class="lineno">  720</span>&#160;        super().<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ad893ac51805fd662173f777c91b2cca4">__init__</a>()</div><div class="line"><a name="l00721"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#afc3159ec4e88329e2967edf28923ea20">  721</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#afc3159ec4e88329e2967edf28923ea20">dim</a> = embedding_size</div><div class="line"><a name="l00722"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a296aa88e8a31be628bb31f55de0a0360">  722</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a296aa88e8a31be628bb31f55de0a0360">ffn_dim</a> = ffn_size</div><div class="line"><a name="l00723"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a94a9a714a2585fd6a675a08643e3ce6b">  723</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a94a9a714a2585fd6a675a08643e3ce6b">variant</a> = variant</div><div class="line"><a name="l00724"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a33276633e128020853db98128f1d93c3">  724</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a33276633e128020853db98128f1d93c3">activation</a> = activation</div><div class="line"><a name="l00725"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#aba87c66e402b0cfdf196872c3f038d43">  725</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#aba87c66e402b0cfdf196872c3f038d43">dropout</a> = nn.Dropout(p=dropout)</div><div class="line"><a name="l00726"></a><span class="lineno">  726</span>&#160;</div><div class="line"><a name="l00727"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ad656c203b5b2ab890ea419febace286f">  727</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ad656c203b5b2ab890ea419febace286f">self_attention</a> = <a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html">MultiHeadAttention</a>(</div><div class="line"><a name="l00728"></a><span class="lineno">  728</span>&#160;            n_heads, embedding_size, dropout=attention_dropout</div><div class="line"><a name="l00729"></a><span class="lineno">  729</span>&#160;        )</div><div class="line"><a name="l00730"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#aa82d4bae763a0867633b6d5bc8484129">  730</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#aa82d4bae763a0867633b6d5bc8484129">norm1</a> = LayerNorm(embedding_size, eps=LAYER_NORM_EPS)</div><div class="line"><a name="l00731"></a><span class="lineno">  731</span>&#160;</div><div class="line"><a name="l00732"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a50be54030dc6526fd5cae1f48a448f04">  732</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a50be54030dc6526fd5cae1f48a448f04">encoder_attention</a> = <a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html">MultiHeadAttention</a>(</div><div class="line"><a name="l00733"></a><span class="lineno">  733</span>&#160;            n_heads, embedding_size, dropout=attention_dropout</div><div class="line"><a name="l00734"></a><span class="lineno">  734</span>&#160;        )</div><div class="line"><a name="l00735"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ac581c47d531c120ca010ce5fa3836b24">  735</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ac581c47d531c120ca010ce5fa3836b24">norm2</a> = LayerNorm(embedding_size, eps=LAYER_NORM_EPS)</div><div class="line"><a name="l00736"></a><span class="lineno">  736</span>&#160;</div><div class="line"><a name="l00737"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a394248deb2cfa738a51f890d4c232224">  737</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a394248deb2cfa738a51f890d4c232224">ffn</a> = <a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html">TransformerFFN</a>(</div><div class="line"><a name="l00738"></a><span class="lineno">  738</span>&#160;            embedding_size, ffn_size, relu_dropout=relu_dropout, activation=activation</div><div class="line"><a name="l00739"></a><span class="lineno">  739</span>&#160;        )</div><div class="line"><a name="l00740"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a24e685c1a13c11ce7bd4a839bc93ab2a">  740</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a24e685c1a13c11ce7bd4a839bc93ab2a">norm3</a> = LayerNorm(embedding_size, eps=LAYER_NORM_EPS)</div><div class="line"><a name="l00741"></a><span class="lineno">  741</span>&#160;</div><div class="line"><a name="l00742"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ad0a63d90190682d0f9ff325fcf60f426">  742</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ad0a63d90190682d0f9ff325fcf60f426">forward</a>(self, x, encoder_output, encoder_mask):</div><div class="line"><a name="l00743"></a><span class="lineno">  743</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;Forward pass.&quot;&quot;&quot;</span></div><div class="line"><a name="l00744"></a><span class="lineno">  744</span>&#160;        decoder_mask = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ac20ce9888aacdabd2766583f28c9d2f5">_create_selfattn_mask</a>(x)</div><div class="line"><a name="l00745"></a><span class="lineno">  745</span>&#160;        <span class="comment"># first self attn</span></div><div class="line"><a name="l00746"></a><span class="lineno">  746</span>&#160;        residual = x</div><div class="line"><a name="l00747"></a><span class="lineno">  747</span>&#160;        <span class="comment"># don&#39;t peak into the future!</span></div><div class="line"><a name="l00748"></a><span class="lineno">  748</span>&#160;        x = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ad656c203b5b2ab890ea419febace286f">self_attention</a>(query=x, mask=decoder_mask)</div><div class="line"><a name="l00749"></a><span class="lineno">  749</span>&#160;        x = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#aba87c66e402b0cfdf196872c3f038d43">dropout</a>(x)  <span class="comment"># --dropout</span></div><div class="line"><a name="l00750"></a><span class="lineno">  750</span>&#160;        x = x + residual</div><div class="line"><a name="l00751"></a><span class="lineno">  751</span>&#160;        x = _normalize(x, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#aa82d4bae763a0867633b6d5bc8484129">norm1</a>)</div><div class="line"><a name="l00752"></a><span class="lineno">  752</span>&#160;</div><div class="line"><a name="l00753"></a><span class="lineno">  753</span>&#160;        residual = x</div><div class="line"><a name="l00754"></a><span class="lineno">  754</span>&#160;        x = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a50be54030dc6526fd5cae1f48a448f04">encoder_attention</a>(</div><div class="line"><a name="l00755"></a><span class="lineno">  755</span>&#160;            query=x, key=encoder_output, value=encoder_output, mask=encoder_mask</div><div class="line"><a name="l00756"></a><span class="lineno">  756</span>&#160;        )</div><div class="line"><a name="l00757"></a><span class="lineno">  757</span>&#160;        x = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#aba87c66e402b0cfdf196872c3f038d43">dropout</a>(x)  <span class="comment"># --dropout</span></div><div class="line"><a name="l00758"></a><span class="lineno">  758</span>&#160;        x = residual + x</div><div class="line"><a name="l00759"></a><span class="lineno">  759</span>&#160;        x = _normalize(x, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ac581c47d531c120ca010ce5fa3836b24">norm2</a>)</div><div class="line"><a name="l00760"></a><span class="lineno">  760</span>&#160;</div><div class="line"><a name="l00761"></a><span class="lineno">  761</span>&#160;        <span class="comment"># finally the ffn</span></div><div class="line"><a name="l00762"></a><span class="lineno">  762</span>&#160;        residual = x</div><div class="line"><a name="l00763"></a><span class="lineno">  763</span>&#160;        x = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a394248deb2cfa738a51f890d4c232224">ffn</a>(x)</div><div class="line"><a name="l00764"></a><span class="lineno">  764</span>&#160;        x = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#aba87c66e402b0cfdf196872c3f038d43">dropout</a>(x)  <span class="comment"># --dropout</span></div><div class="line"><a name="l00765"></a><span class="lineno">  765</span>&#160;        x = residual + x</div><div class="line"><a name="l00766"></a><span class="lineno">  766</span>&#160;        x = _normalize(x, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a24e685c1a13c11ce7bd4a839bc93ab2a">norm3</a>)</div><div class="line"><a name="l00767"></a><span class="lineno">  767</span>&#160;</div><div class="line"><a name="l00768"></a><span class="lineno">  768</span>&#160;        <span class="keywordflow">return</span> x</div><div class="line"><a name="l00769"></a><span class="lineno">  769</span>&#160;</div><div class="line"><a name="l00770"></a><span class="lineno">  770</span>&#160;    <span class="keyword">def </span>_create_selfattn_mask(self, x):</div><div class="line"><a name="l00771"></a><span class="lineno">  771</span>&#160;        <span class="comment"># figure out how many timestamps we need</span></div><div class="line"><a name="l00772"></a><span class="lineno">  772</span>&#160;        bsz = x.size(0)</div><div class="line"><a name="l00773"></a><span class="lineno">  773</span>&#160;        time = x.size(1)</div><div class="line"><a name="l00774"></a><span class="lineno">  774</span>&#160;        <span class="comment"># make sure that we don&#39;t look into the future</span></div><div class="line"><a name="l00775"></a><span class="lineno">  775</span>&#160;        mask = torch.tril(x.new(time, time).fill_(1))</div><div class="line"><a name="l00776"></a><span class="lineno">  776</span>&#160;        <span class="comment"># broadcast across batch</span></div><div class="line"><a name="l00777"></a><span class="lineno">  777</span>&#160;        mask = mask.unsqueeze(0).expand(bsz, -1, -1)</div><div class="line"><a name="l00778"></a><span class="lineno">  778</span>&#160;        <span class="keywordflow">return</span> mask</div><div class="line"><a name="l00779"></a><span class="lineno">  779</span>&#160;</div><div class="line"><a name="l00780"></a><span class="lineno">  780</span>&#160;</div><div class="line"><a name="l00781"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html">  781</a></span>&#160;<span class="keyword">class </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html">TransformerGeneratorModel</a>(<a class="code" href="classparlai_1_1core_1_1torch__generator__agent_1_1TorchGeneratorModel.html">TorchGeneratorModel</a>):</div><div class="line"><a name="l00782"></a><span class="lineno">  782</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;Implements a full generator model, with one encoder and one decoder.&quot;&quot;&quot;</span></div><div class="line"><a name="l00783"></a><span class="lineno">  783</span>&#160;</div><div class="line"><a name="l00784"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a17276e3090a132914c0adae133e60357">  784</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a17276e3090a132914c0adae133e60357">__init__</a>(self, opt, dictionary):</div><div class="line"><a name="l00785"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#aefac02b3630ef361bb008b4f7973c55b">  785</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#aefac02b3630ef361bb008b4f7973c55b">pad_idx</a> = dictionary[dictionary.null_token]</div><div class="line"><a name="l00786"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a381b6d63e5aba194730c36d9fb93c6d6">  786</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a381b6d63e5aba194730c36d9fb93c6d6">start_idx</a> = dictionary[dictionary.start_token]</div><div class="line"><a name="l00787"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#ab2b8403205fcd407e47546dc56022d2f">  787</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#ab2b8403205fcd407e47546dc56022d2f">end_idx</a> = dictionary[dictionary.end_token]</div><div class="line"><a name="l00788"></a><span class="lineno">  788</span>&#160;        super().<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a17276e3090a132914c0adae133e60357">__init__</a>(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#aefac02b3630ef361bb008b4f7973c55b">pad_idx</a>, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a381b6d63e5aba194730c36d9fb93c6d6">start_idx</a>, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#ab2b8403205fcd407e47546dc56022d2f">end_idx</a>)</div><div class="line"><a name="l00789"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a18411754103e8c8f4cd2e9338351b98d">  789</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a18411754103e8c8f4cd2e9338351b98d">embeddings</a> = _create_embeddings(</div><div class="line"><a name="l00790"></a><span class="lineno">  790</span>&#160;            dictionary, opt[<span class="stringliteral">&#39;embedding_size&#39;</span>], self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#aefac02b3630ef361bb008b4f7973c55b">pad_idx</a></div><div class="line"><a name="l00791"></a><span class="lineno">  791</span>&#160;        )</div><div class="line"><a name="l00792"></a><span class="lineno">  792</span>&#160;</div><div class="line"><a name="l00793"></a><span class="lineno">  793</span>&#160;        <span class="keywordflow">if</span> opt.get(<span class="stringliteral">&#39;n_positions&#39;</span>):</div><div class="line"><a name="l00794"></a><span class="lineno">  794</span>&#160;            <span class="comment"># if the number of positions is explicitly provided, use that</span></div><div class="line"><a name="l00795"></a><span class="lineno">  795</span>&#160;            n_positions = opt[<span class="stringliteral">&#39;n_positions&#39;</span>]</div><div class="line"><a name="l00796"></a><span class="lineno">  796</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00797"></a><span class="lineno">  797</span>&#160;            <span class="comment"># else, use the worst case from truncate</span></div><div class="line"><a name="l00798"></a><span class="lineno">  798</span>&#160;            n_positions = max(</div><div class="line"><a name="l00799"></a><span class="lineno">  799</span>&#160;                opt.get(<span class="stringliteral">&#39;truncate&#39;</span>) <span class="keywordflow">or</span> 0,</div><div class="line"><a name="l00800"></a><span class="lineno">  800</span>&#160;                opt.get(<span class="stringliteral">&#39;text_truncate&#39;</span>) <span class="keywordflow">or</span> 0,</div><div class="line"><a name="l00801"></a><span class="lineno">  801</span>&#160;                opt.get(<span class="stringliteral">&#39;label_truncate&#39;</span>) <span class="keywordflow">or</span> 0,</div><div class="line"><a name="l00802"></a><span class="lineno">  802</span>&#160;            )</div><div class="line"><a name="l00803"></a><span class="lineno">  803</span>&#160;            <span class="keywordflow">if</span> n_positions == 0:</div><div class="line"><a name="l00804"></a><span class="lineno">  804</span>&#160;                <span class="comment"># default to 1024</span></div><div class="line"><a name="l00805"></a><span class="lineno">  805</span>&#160;                n_positions = 1024</div><div class="line"><a name="l00806"></a><span class="lineno">  806</span>&#160;        n_segments = opt.get(<span class="stringliteral">&#39;n_segments&#39;</span>, 0)</div><div class="line"><a name="l00807"></a><span class="lineno">  807</span>&#160;</div><div class="line"><a name="l00808"></a><span class="lineno">  808</span>&#160;        <span class="keywordflow">if</span> n_positions &lt; 0:</div><div class="line"><a name="l00809"></a><span class="lineno">  809</span>&#160;            <span class="keywordflow">raise</span> ValueError(<span class="stringliteral">&#39;n_positions must be positive&#39;</span>)</div><div class="line"><a name="l00810"></a><span class="lineno">  810</span>&#160;</div><div class="line"><a name="l00811"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#af165b8ec1505f768c530aea02d68ae09">  811</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#af165b8ec1505f768c530aea02d68ae09">encoder</a> = _build_encoder(</div><div class="line"><a name="l00812"></a><span class="lineno">  812</span>&#160;            opt,</div><div class="line"><a name="l00813"></a><span class="lineno">  813</span>&#160;            dictionary,</div><div class="line"><a name="l00814"></a><span class="lineno">  814</span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a18411754103e8c8f4cd2e9338351b98d">embeddings</a>,</div><div class="line"><a name="l00815"></a><span class="lineno">  815</span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#aefac02b3630ef361bb008b4f7973c55b">pad_idx</a>,</div><div class="line"><a name="l00816"></a><span class="lineno">  816</span>&#160;            reduction_type=<span class="keywordtype">None</span>,</div><div class="line"><a name="l00817"></a><span class="lineno">  817</span>&#160;            n_positions=n_positions,</div><div class="line"><a name="l00818"></a><span class="lineno">  818</span>&#160;            n_segments=n_segments,</div><div class="line"><a name="l00819"></a><span class="lineno">  819</span>&#160;        )</div><div class="line"><a name="l00820"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a6467f0fdec226329cb8efe77245996a5">  820</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a6467f0fdec226329cb8efe77245996a5">decoder</a> = _build_decoder(</div><div class="line"><a name="l00821"></a><span class="lineno">  821</span>&#160;            opt, dictionary, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a18411754103e8c8f4cd2e9338351b98d">embeddings</a>, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#aefac02b3630ef361bb008b4f7973c55b">pad_idx</a>, n_positions=n_positions</div><div class="line"><a name="l00822"></a><span class="lineno">  822</span>&#160;        )</div><div class="line"><a name="l00823"></a><span class="lineno">  823</span>&#160;</div><div class="line"><a name="l00824"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a9b9261bca59f10692c15cbaaa3ffce65">  824</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a9b9261bca59f10692c15cbaaa3ffce65">reorder_encoder_states</a>(self, encoder_states, indices):</div><div class="line"><a name="l00825"></a><span class="lineno">  825</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00826"></a><span class="lineno">  826</span>&#160;<span class="stringliteral">        Reorder the encoder states.</span></div><div class="line"><a name="l00827"></a><span class="lineno">  827</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00828"></a><span class="lineno">  828</span>&#160;<span class="stringliteral">        See ``TorchGeneratorModel.reorder_encoder_states`` for a description.</span></div><div class="line"><a name="l00829"></a><span class="lineno">  829</span>&#160;<span class="stringliteral">        &quot;&quot;&quot;</span></div><div class="line"><a name="l00830"></a><span class="lineno">  830</span>&#160;        enc, mask = encoder_states</div><div class="line"><a name="l00831"></a><span class="lineno">  831</span>&#160;        <span class="keywordflow">if</span> <span class="keywordflow">not</span> torch.is_tensor(indices):</div><div class="line"><a name="l00832"></a><span class="lineno">  832</span>&#160;            indices = torch.LongTensor(indices).to(enc.device)</div><div class="line"><a name="l00833"></a><span class="lineno">  833</span>&#160;        enc = torch.index_select(enc, 0, indices)</div><div class="line"><a name="l00834"></a><span class="lineno">  834</span>&#160;        mask = torch.index_select(mask, 0, indices)</div><div class="line"><a name="l00835"></a><span class="lineno">  835</span>&#160;        <span class="keywordflow">return</span> enc, mask</div><div class="line"><a name="l00836"></a><span class="lineno">  836</span>&#160;</div><div class="line"><a name="l00837"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a7ee3a87dd94d18353bca09990e8d5821">  837</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a7ee3a87dd94d18353bca09990e8d5821">reorder_decoder_incremental_state</a>(self, incremental_state, inds):</div><div class="line"><a name="l00838"></a><span class="lineno">  838</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00839"></a><span class="lineno">  839</span>&#160;<span class="stringliteral">        Reorder the decoder incremental state.</span></div><div class="line"><a name="l00840"></a><span class="lineno">  840</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00841"></a><span class="lineno">  841</span>&#160;<span class="stringliteral">        Not implemented in Transformers, since ``incremental_state`` is always None.</span></div><div class="line"><a name="l00842"></a><span class="lineno">  842</span>&#160;<span class="stringliteral">        &quot;&quot;&quot;</span></div><div class="line"><a name="l00843"></a><span class="lineno">  843</span>&#160;        <span class="comment"># no support for incremental decoding at this time</span></div><div class="line"><a name="l00844"></a><span class="lineno">  844</span>&#160;        <span class="keywordflow">return</span> <span class="keywordtype">None</span></div><div class="line"><a name="l00845"></a><span class="lineno">  845</span>&#160;</div><div class="line"><a name="l00846"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#ae0f0d3d038cc12075501aae3a059c997">  846</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#ae0f0d3d038cc12075501aae3a059c997">output</a>(self, tensor):</div><div class="line"><a name="l00847"></a><span class="lineno">  847</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;Compute output logits.&quot;&quot;&quot;</span></div><div class="line"><a name="l00848"></a><span class="lineno">  848</span>&#160;        <span class="comment"># project back to vocabulary</span></div><div class="line"><a name="l00849"></a><span class="lineno">  849</span>&#160;        output = F.linear(tensor, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a18411754103e8c8f4cd2e9338351b98d">embeddings</a>.weight)</div><div class="line"><a name="l00850"></a><span class="lineno">  850</span>&#160;        <span class="keywordflow">return</span> output</div><div class="line"><a name="l00851"></a><span class="lineno">  851</span>&#160;</div><div class="line"><a name="l00852"></a><span class="lineno">  852</span>&#160;</div><div class="line"><a name="l00853"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html">  853</a></span>&#160;<span class="keyword">class </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html">BasicAttention</a>(nn.Module):</div><div class="line"><a name="l00854"></a><span class="lineno">  854</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;Implements simple/classical attention.&quot;&quot;&quot;</span></div><div class="line"><a name="l00855"></a><span class="lineno">  855</span>&#160;</div><div class="line"><a name="l00856"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#afa25129ca6ce9851053205f44f090fd6">  856</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#afa25129ca6ce9851053205f44f090fd6">__init__</a>(self, dim=1, attn=&#39;cosine&#39;, residual=False, get_weights=True):</div><div class="line"><a name="l00857"></a><span class="lineno">  857</span>&#160;        super().<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#afa25129ca6ce9851053205f44f090fd6">__init__</a>()</div><div class="line"><a name="l00858"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a669fa3f4cd988703371fc127a4943f07">  858</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a669fa3f4cd988703371fc127a4943f07">softmax</a> = nn.Softmax(dim=dim)</div><div class="line"><a name="l00859"></a><span class="lineno">  859</span>&#160;        <span class="keywordflow">if</span> attn == <span class="stringliteral">&#39;cosine&#39;</span>:</div><div class="line"><a name="l00860"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#abed26e42fa2293f14cbbcf51c090794c">  860</a></span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#abed26e42fa2293f14cbbcf51c090794c">cosine</a> = nn.CosineSimilarity(dim=dim)</div><div class="line"><a name="l00861"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a04d74d5efbfdf47d36aff30331775368">  861</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a04d74d5efbfdf47d36aff30331775368">attn</a> = attn</div><div class="line"><a name="l00862"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#ad0beb8f8b3e4514139b08555cf739d55">  862</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#ad0beb8f8b3e4514139b08555cf739d55">dim</a> = dim</div><div class="line"><a name="l00863"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a98d54b38fac0a88d46eba7adf6ac542c">  863</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a98d54b38fac0a88d46eba7adf6ac542c">get_weights</a> = get_weights</div><div class="line"><a name="l00864"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a98c0f263db25cf305c333e04f8a0843d">  864</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a98c0f263db25cf305c333e04f8a0843d">residual</a> = residual</div><div class="line"><a name="l00865"></a><span class="lineno">  865</span>&#160;</div><div class="line"><a name="l00866"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#adb18b6c2564672c1820b3c72583dc0c0">  866</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#adb18b6c2564672c1820b3c72583dc0c0">forward</a>(self, xs, ys, mask_ys=None, values=None):</div><div class="line"><a name="l00867"></a><span class="lineno">  867</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;Compute attention.</span></div><div class="line"><a name="l00868"></a><span class="lineno">  868</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00869"></a><span class="lineno">  869</span>&#160;<span class="stringliteral">        Attend over ys with query xs to obtain weights, then apply weights to</span></div><div class="line"><a name="l00870"></a><span class="lineno">  870</span>&#160;<span class="stringliteral">        values (ys if yalues is None)</span></div><div class="line"><a name="l00871"></a><span class="lineno">  871</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00872"></a><span class="lineno">  872</span>&#160;<span class="stringliteral">        Args:</span></div><div class="line"><a name="l00873"></a><span class="lineno">  873</span>&#160;<span class="stringliteral">            xs: B x query_len x dim (queries)</span></div><div class="line"><a name="l00874"></a><span class="lineno">  874</span>&#160;<span class="stringliteral">            ys: B x key_len x dim (keys)</span></div><div class="line"><a name="l00875"></a><span class="lineno">  875</span>&#160;<span class="stringliteral">            mask_ys: B x key_len (mask)</span></div><div class="line"><a name="l00876"></a><span class="lineno">  876</span>&#160;<span class="stringliteral">            values: B x value_len x dim (values); if None, default to ys</span></div><div class="line"><a name="l00877"></a><span class="lineno">  877</span>&#160;<span class="stringliteral">        &quot;&quot;&quot;</span></div><div class="line"><a name="l00878"></a><span class="lineno">  878</span>&#160;        bsz = xs.size(0)</div><div class="line"><a name="l00879"></a><span class="lineno">  879</span>&#160;        y_len = ys.size(1)</div><div class="line"><a name="l00880"></a><span class="lineno">  880</span>&#160;        x_len = xs.size(1)</div><div class="line"><a name="l00881"></a><span class="lineno">  881</span>&#160;        <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a04d74d5efbfdf47d36aff30331775368">attn</a> == <span class="stringliteral">&#39;cosine&#39;</span>:</div><div class="line"><a name="l00882"></a><span class="lineno">  882</span>&#160;            l1 = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#abed26e42fa2293f14cbbcf51c090794c">cosine</a>(xs, ys).unsqueeze(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#ad0beb8f8b3e4514139b08555cf739d55">dim</a> - 1)</div><div class="line"><a name="l00883"></a><span class="lineno">  883</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00884"></a><span class="lineno">  884</span>&#160;            l1 = torch.bmm(xs, ys.transpose(1, 2))</div><div class="line"><a name="l00885"></a><span class="lineno">  885</span>&#160;            <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a04d74d5efbfdf47d36aff30331775368">attn</a> == <span class="stringliteral">&#39;sqrt&#39;</span>:</div><div class="line"><a name="l00886"></a><span class="lineno">  886</span>&#160;                d_k = ys.size(-1)</div><div class="line"><a name="l00887"></a><span class="lineno">  887</span>&#160;                l1 = l1 / math.sqrt(d_k)</div><div class="line"><a name="l00888"></a><span class="lineno">  888</span>&#160;        <span class="keywordflow">if</span> mask_ys <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00889"></a><span class="lineno">  889</span>&#160;            attn_mask = (mask_ys == 0).view(bsz, 1, y_len)</div><div class="line"><a name="l00890"></a><span class="lineno">  890</span>&#160;            attn_mask = attn_mask.repeat(1, x_len, 1)</div><div class="line"><a name="l00891"></a><span class="lineno">  891</span>&#160;            l1.masked_fill_(attn_mask, -<a class="code" href="namespacemake__control__dataset.html#aab4734ca3fc74fda1c53ad21ec9f47b0">float</a>(<span class="stringliteral">&#39;inf&#39;</span>))</div><div class="line"><a name="l00892"></a><span class="lineno">  892</span>&#160;        l2 = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a669fa3f4cd988703371fc127a4943f07">softmax</a>(l1)</div><div class="line"><a name="l00893"></a><span class="lineno">  893</span>&#160;        <span class="keywordflow">if</span> values <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00894"></a><span class="lineno">  894</span>&#160;            values = ys</div><div class="line"><a name="l00895"></a><span class="lineno">  895</span>&#160;        lhs_emb = torch.bmm(l2, values)</div><div class="line"><a name="l00896"></a><span class="lineno">  896</span>&#160;</div><div class="line"><a name="l00897"></a><span class="lineno">  897</span>&#160;        <span class="comment"># # add back the query</span></div><div class="line"><a name="l00898"></a><span class="lineno">  898</span>&#160;        <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a98c0f263db25cf305c333e04f8a0843d">residual</a>:</div><div class="line"><a name="l00899"></a><span class="lineno">  899</span>&#160;            lhs_emb = lhs_emb.add(xs)</div><div class="line"><a name="l00900"></a><span class="lineno">  900</span>&#160;</div><div class="line"><a name="l00901"></a><span class="lineno">  901</span>&#160;        <span class="keywordflow">if</span> self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a98d54b38fac0a88d46eba7adf6ac542c">get_weights</a>:</div><div class="line"><a name="l00902"></a><span class="lineno">  902</span>&#160;            <span class="keywordflow">return</span> lhs_emb.squeeze(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#ad0beb8f8b3e4514139b08555cf739d55">dim</a> - 1), l2</div><div class="line"><a name="l00903"></a><span class="lineno">  903</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l00904"></a><span class="lineno">  904</span>&#160;            <span class="keywordflow">return</span> lhs_emb.squeeze(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#ad0beb8f8b3e4514139b08555cf739d55">dim</a> - 1)</div><div class="line"><a name="l00905"></a><span class="lineno">  905</span>&#160;</div><div class="line"><a name="l00906"></a><span class="lineno">  906</span>&#160;</div><div class="line"><a name="l00907"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html">  907</a></span>&#160;<span class="keyword">class </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html">MultiHeadAttention</a>(nn.Module):</div><div class="line"><a name="l00908"></a><span class="lineno">  908</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;</span></div><div class="line"><a name="l00909"></a><span class="lineno">  909</span>&#160;<span class="stringliteral">    Implements MultiHeadAttention; this is the core workhorse of the Transformer.</span></div><div class="line"><a name="l00910"></a><span class="lineno">  910</span>&#160;<span class="stringliteral"></span></div><div class="line"><a name="l00911"></a><span class="lineno">  911</span>&#160;<span class="stringliteral">    See Vaswani (2017) for an extensive description.</span></div><div class="line"><a name="l00912"></a><span class="lineno">  912</span>&#160;<span class="stringliteral">    &quot;&quot;&quot;</span></div><div class="line"><a name="l00913"></a><span class="lineno">  913</span>&#160;</div><div class="line"><a name="l00914"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a427b8cfaac6d37141555616d9cee8b74">  914</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a427b8cfaac6d37141555616d9cee8b74">__init__</a>(self, n_heads, dim, dropout=0):</div><div class="line"><a name="l00915"></a><span class="lineno">  915</span>&#160;        super(MultiHeadAttention, self).<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a427b8cfaac6d37141555616d9cee8b74">__init__</a>()</div><div class="line"><a name="l00916"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#aa28ff3887a718bb96a9bd27c938a0baf">  916</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#aa28ff3887a718bb96a9bd27c938a0baf">n_heads</a> = n_heads</div><div class="line"><a name="l00917"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a0ffd2e0d89db2f682561af438aaa1eb5">  917</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a0ffd2e0d89db2f682561af438aaa1eb5">dim</a> = dim</div><div class="line"><a name="l00918"></a><span class="lineno">  918</span>&#160;</div><div class="line"><a name="l00919"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#ac1c0169222b929c580d1a786479d7ed2">  919</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#ac1c0169222b929c580d1a786479d7ed2">attn_dropout</a> = nn.Dropout(p=dropout)  <span class="comment"># --attention-dropout</span></div><div class="line"><a name="l00920"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a84ab720bc232bb48e8f61e37e7c52323">  920</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a84ab720bc232bb48e8f61e37e7c52323">q_lin</a> = nn.Linear(dim, dim)</div><div class="line"><a name="l00921"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a1e86cacd8fe8129222fe04c41c48655d">  921</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a1e86cacd8fe8129222fe04c41c48655d">k_lin</a> = nn.Linear(dim, dim)</div><div class="line"><a name="l00922"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a1b1c33915c80f894221dfd4f0a0a2896">  922</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a1b1c33915c80f894221dfd4f0a0a2896">v_lin</a> = nn.Linear(dim, dim)</div><div class="line"><a name="l00923"></a><span class="lineno">  923</span>&#160;        <span class="comment"># TODO: merge for the initialization step</span></div><div class="line"><a name="l00924"></a><span class="lineno">  924</span>&#160;        nn.init.xavier_normal_(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a84ab720bc232bb48e8f61e37e7c52323">q_lin</a>.weight)</div><div class="line"><a name="l00925"></a><span class="lineno">  925</span>&#160;        nn.init.xavier_normal_(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a1e86cacd8fe8129222fe04c41c48655d">k_lin</a>.weight)</div><div class="line"><a name="l00926"></a><span class="lineno">  926</span>&#160;        nn.init.xavier_normal_(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a1b1c33915c80f894221dfd4f0a0a2896">v_lin</a>.weight)</div><div class="line"><a name="l00927"></a><span class="lineno">  927</span>&#160;        <span class="comment"># and set biases to 0</span></div><div class="line"><a name="l00928"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a495812bb583389a5f5dae989a22bda19">  928</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a495812bb583389a5f5dae989a22bda19">out_lin</a> = nn.Linear(dim, dim)</div><div class="line"><a name="l00929"></a><span class="lineno">  929</span>&#160;</div><div class="line"><a name="l00930"></a><span class="lineno">  930</span>&#160;        nn.init.xavier_normal_(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a495812bb583389a5f5dae989a22bda19">out_lin</a>.weight)</div><div class="line"><a name="l00931"></a><span class="lineno">  931</span>&#160;</div><div class="line"><a name="l00932"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a0685acda3d791bccfb9a6f4ff2c3680f">  932</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a0685acda3d791bccfb9a6f4ff2c3680f">forward</a>(self, query, key=None, value=None, mask=None):</div><div class="line"><a name="l00933"></a><span class="lineno">  933</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;Forward pass.&quot;&quot;&quot;</span></div><div class="line"><a name="l00934"></a><span class="lineno">  934</span>&#160;        <span class="comment"># TODO: there are a lot of parameters to document here.</span></div><div class="line"><a name="l00935"></a><span class="lineno">  935</span>&#160;</div><div class="line"><a name="l00936"></a><span class="lineno">  936</span>&#160;        <span class="comment"># Input is [B, query_len, dim]</span></div><div class="line"><a name="l00937"></a><span class="lineno">  937</span>&#160;        <span class="comment"># Mask is [B, key_len] (selfattn) or [B, key_len, key_len] (enc attn)</span></div><div class="line"><a name="l00938"></a><span class="lineno">  938</span>&#160;        batch_size, query_len, dim = query.size()</div><div class="line"><a name="l00939"></a><span class="lineno">  939</span>&#160;        <span class="keyword">assert</span> (</div><div class="line"><a name="l00940"></a><span class="lineno">  940</span>&#160;            dim == self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a0ffd2e0d89db2f682561af438aaa1eb5">dim</a></div><div class="line"><a name="l00941"></a><span class="lineno">  941</span>&#160;        ), <span class="stringliteral">&#39;Dimensions do not match: {} query vs {} configured&#39;</span>.<a class="code" href="namespaceparlai_1_1messenger_1_1core_1_1shared__utils.html#a11f2820425bccac8d8d490ab505bcb79">format</a>(dim, self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a0ffd2e0d89db2f682561af438aaa1eb5">dim</a>)</div><div class="line"><a name="l00942"></a><span class="lineno">  942</span>&#160;        <span class="keyword">assert</span> mask <span class="keywordflow">is</span> <span class="keywordflow">not</span> <span class="keywordtype">None</span>, <span class="stringliteral">&#39;Mask is None, please specify a mask&#39;</span></div><div class="line"><a name="l00943"></a><span class="lineno">  943</span>&#160;        n_heads = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#aa28ff3887a718bb96a9bd27c938a0baf">n_heads</a></div><div class="line"><a name="l00944"></a><span class="lineno">  944</span>&#160;        dim_per_head = dim // n_heads</div><div class="line"><a name="l00945"></a><span class="lineno">  945</span>&#160;        scale = math.sqrt(dim_per_head)</div><div class="line"><a name="l00946"></a><span class="lineno">  946</span>&#160;</div><div class="line"><a name="l00947"></a><span class="lineno">  947</span>&#160;        <span class="keyword">def </span>prepare_head(tensor):</div><div class="line"><a name="l00948"></a><span class="lineno">  948</span>&#160;            <span class="comment"># input is [batch_size, seq_len, n_heads * dim_per_head]</span></div><div class="line"><a name="l00949"></a><span class="lineno">  949</span>&#160;            <span class="comment"># output is [batch_size * n_heads, seq_len, dim_per_head]</span></div><div class="line"><a name="l00950"></a><span class="lineno">  950</span>&#160;            bsz, seq_len, _ = tensor.size()</div><div class="line"><a name="l00951"></a><span class="lineno">  951</span>&#160;            tensor = tensor.view(batch_size, tensor.size(1), n_heads, dim_per_head)</div><div class="line"><a name="l00952"></a><span class="lineno">  952</span>&#160;            tensor = (</div><div class="line"><a name="l00953"></a><span class="lineno">  953</span>&#160;                tensor.transpose(1, 2)</div><div class="line"><a name="l00954"></a><span class="lineno">  954</span>&#160;                .contiguous()</div><div class="line"><a name="l00955"></a><span class="lineno">  955</span>&#160;                .view(batch_size * n_heads, seq_len, dim_per_head)</div><div class="line"><a name="l00956"></a><span class="lineno">  956</span>&#160;            )</div><div class="line"><a name="l00957"></a><span class="lineno">  957</span>&#160;            <span class="keywordflow">return</span> tensor</div><div class="line"><a name="l00958"></a><span class="lineno">  958</span>&#160;</div><div class="line"><a name="l00959"></a><span class="lineno">  959</span>&#160;        <span class="comment"># q, k, v are the transformed values</span></div><div class="line"><a name="l00960"></a><span class="lineno">  960</span>&#160;        <span class="keywordflow">if</span> key <span class="keywordflow">is</span> <span class="keywordtype">None</span> <span class="keywordflow">and</span> value <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00961"></a><span class="lineno">  961</span>&#160;            <span class="comment"># self attention</span></div><div class="line"><a name="l00962"></a><span class="lineno">  962</span>&#160;            key = value = query</div><div class="line"><a name="l00963"></a><span class="lineno">  963</span>&#160;        <span class="keywordflow">elif</span> value <span class="keywordflow">is</span> <span class="keywordtype">None</span>:</div><div class="line"><a name="l00964"></a><span class="lineno">  964</span>&#160;            <span class="comment"># key and value are the same, but query differs</span></div><div class="line"><a name="l00965"></a><span class="lineno">  965</span>&#160;            <span class="comment"># self attention</span></div><div class="line"><a name="l00966"></a><span class="lineno">  966</span>&#160;            value = key</div><div class="line"><a name="l00967"></a><span class="lineno">  967</span>&#160;        _, key_len, dim = key.size()</div><div class="line"><a name="l00968"></a><span class="lineno">  968</span>&#160;</div><div class="line"><a name="l00969"></a><span class="lineno">  969</span>&#160;        q = prepare_head(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a84ab720bc232bb48e8f61e37e7c52323">q_lin</a>(query))</div><div class="line"><a name="l00970"></a><span class="lineno">  970</span>&#160;        k = prepare_head(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a1e86cacd8fe8129222fe04c41c48655d">k_lin</a>(key))</div><div class="line"><a name="l00971"></a><span class="lineno">  971</span>&#160;        v = prepare_head(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a1b1c33915c80f894221dfd4f0a0a2896">v_lin</a>(value))</div><div class="line"><a name="l00972"></a><span class="lineno">  972</span>&#160;</div><div class="line"><a name="l00973"></a><span class="lineno">  973</span>&#160;        dot_prod = q.div_(scale).bmm(k.transpose(1, 2))</div><div class="line"><a name="l00974"></a><span class="lineno">  974</span>&#160;        <span class="comment"># [B * n_heads, query_len, key_len]</span></div><div class="line"><a name="l00975"></a><span class="lineno">  975</span>&#160;        attn_mask = (</div><div class="line"><a name="l00976"></a><span class="lineno">  976</span>&#160;            (mask == 0)</div><div class="line"><a name="l00977"></a><span class="lineno">  977</span>&#160;            .view(batch_size, 1, -1, key_len)</div><div class="line"><a name="l00978"></a><span class="lineno">  978</span>&#160;            .<a class="code" href="namespacerepeat.html">repeat</a>(1, n_heads, 1, 1)</div><div class="line"><a name="l00979"></a><span class="lineno">  979</span>&#160;            .expand(batch_size, n_heads, query_len, key_len)</div><div class="line"><a name="l00980"></a><span class="lineno">  980</span>&#160;            .view(batch_size * n_heads, query_len, key_len)</div><div class="line"><a name="l00981"></a><span class="lineno">  981</span>&#160;        )</div><div class="line"><a name="l00982"></a><span class="lineno">  982</span>&#160;        <span class="keyword">assert</span> attn_mask.shape == dot_prod.shape</div><div class="line"><a name="l00983"></a><span class="lineno">  983</span>&#160;        dot_prod.masked_fill_(attn_mask, <a class="code" href="namespaceparlai_1_1utils_1_1misc.html#a68c44ca571de7149b683539db659c330">neginf</a>(dot_prod.dtype))</div><div class="line"><a name="l00984"></a><span class="lineno">  984</span>&#160;</div><div class="line"><a name="l00985"></a><span class="lineno">  985</span>&#160;        attn_weights = F.softmax(dot_prod, dim=-1).type_as(query)</div><div class="line"><a name="l00986"></a><span class="lineno">  986</span>&#160;        attn_weights = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#ac1c0169222b929c580d1a786479d7ed2">attn_dropout</a>(attn_weights)  <span class="comment"># --attention-dropout</span></div><div class="line"><a name="l00987"></a><span class="lineno">  987</span>&#160;</div><div class="line"><a name="l00988"></a><span class="lineno">  988</span>&#160;        attentioned = attn_weights.bmm(v)</div><div class="line"><a name="l00989"></a><span class="lineno">  989</span>&#160;        attentioned = (</div><div class="line"><a name="l00990"></a><span class="lineno">  990</span>&#160;            attentioned.type_as(query)</div><div class="line"><a name="l00991"></a><span class="lineno">  991</span>&#160;            .view(batch_size, n_heads, query_len, dim_per_head)</div><div class="line"><a name="l00992"></a><span class="lineno">  992</span>&#160;            .transpose(1, 2)</div><div class="line"><a name="l00993"></a><span class="lineno">  993</span>&#160;            .contiguous()</div><div class="line"><a name="l00994"></a><span class="lineno">  994</span>&#160;            .view(batch_size, query_len, dim)</div><div class="line"><a name="l00995"></a><span class="lineno">  995</span>&#160;        )</div><div class="line"><a name="l00996"></a><span class="lineno">  996</span>&#160;</div><div class="line"><a name="l00997"></a><span class="lineno">  997</span>&#160;        out = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a495812bb583389a5f5dae989a22bda19">out_lin</a>(attentioned)</div><div class="line"><a name="l00998"></a><span class="lineno">  998</span>&#160;</div><div class="line"><a name="l00999"></a><span class="lineno">  999</span>&#160;        <span class="keywordflow">return</span> out</div><div class="line"><a name="l01000"></a><span class="lineno"> 1000</span>&#160;</div><div class="line"><a name="l01001"></a><span class="lineno"> 1001</span>&#160;</div><div class="line"><a name="l01002"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html"> 1002</a></span>&#160;<span class="keyword">class </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html">TransformerFFN</a>(nn.Module):</div><div class="line"><a name="l01003"></a><span class="lineno"> 1003</span>&#160;    <span class="stringliteral">&quot;&quot;&quot;Implements the FFN part of the transformer.&quot;&quot;&quot;</span></div><div class="line"><a name="l01004"></a><span class="lineno"> 1004</span>&#160;</div><div class="line"><a name="l01005"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a89cffa1831968953cdd7dfdfa7867944"> 1005</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a89cffa1831968953cdd7dfdfa7867944">__init__</a>(self, dim, dim_hidden, relu_dropout=0, activation=&#39;relu&#39;):</div><div class="line"><a name="l01006"></a><span class="lineno"> 1006</span>&#160;        super(TransformerFFN, self).<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a89cffa1831968953cdd7dfdfa7867944">__init__</a>()</div><div class="line"><a name="l01007"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a22f7891e6b991a8ee6b58f08c12cf4ae"> 1007</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a22f7891e6b991a8ee6b58f08c12cf4ae">relu_dropout</a> = nn.Dropout(p=relu_dropout)</div><div class="line"><a name="l01008"></a><span class="lineno"> 1008</span>&#160;        <span class="keywordflow">if</span> activation == <span class="stringliteral">&#39;relu&#39;</span>:</div><div class="line"><a name="l01009"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a234566d2ffaffe67f04c4592cfc84265"> 1009</a></span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a234566d2ffaffe67f04c4592cfc84265">nonlinear</a> = F.relu</div><div class="line"><a name="l01010"></a><span class="lineno"> 1010</span>&#160;        <span class="keywordflow">elif</span> activation == <span class="stringliteral">&#39;gelu&#39;</span>:</div><div class="line"><a name="l01011"></a><span class="lineno"> 1011</span>&#160;            self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a234566d2ffaffe67f04c4592cfc84265">nonlinear</a> = gelu</div><div class="line"><a name="l01012"></a><span class="lineno"> 1012</span>&#160;        <span class="keywordflow">else</span>:</div><div class="line"><a name="l01013"></a><span class="lineno"> 1013</span>&#160;            <span class="keywordflow">raise</span> ValueError(</div><div class="line"><a name="l01014"></a><span class="lineno"> 1014</span>&#160;                <span class="stringliteral">&quot;Don&#39;t know how to handle --activation {}&quot;</span>.<a class="code" href="namespaceparlai_1_1messenger_1_1core_1_1shared__utils.html#a11f2820425bccac8d8d490ab505bcb79">format</a>(activation)</div><div class="line"><a name="l01015"></a><span class="lineno"> 1015</span>&#160;            )</div><div class="line"><a name="l01016"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#addc959c81b7c0ba87a47415af7f07e16"> 1016</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#addc959c81b7c0ba87a47415af7f07e16">lin1</a> = nn.Linear(dim, dim_hidden)</div><div class="line"><a name="l01017"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#ae0ee11939e782034e2d9809f18ed4708"> 1017</a></span>&#160;        self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#ae0ee11939e782034e2d9809f18ed4708">lin2</a> = nn.Linear(dim_hidden, dim)</div><div class="line"><a name="l01018"></a><span class="lineno"> 1018</span>&#160;        nn.init.xavier_uniform_(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#addc959c81b7c0ba87a47415af7f07e16">lin1</a>.weight)</div><div class="line"><a name="l01019"></a><span class="lineno"> 1019</span>&#160;        nn.init.xavier_uniform_(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#ae0ee11939e782034e2d9809f18ed4708">lin2</a>.weight)</div><div class="line"><a name="l01020"></a><span class="lineno"> 1020</span>&#160;        <span class="comment"># TODO: initialize biases to 0</span></div><div class="line"><a name="l01021"></a><span class="lineno"> 1021</span>&#160;</div><div class="line"><a name="l01022"></a><span class="lineno"><a class="line" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a54f8d1ec7c0c4228f78a3be843ab3003"> 1022</a></span>&#160;    <span class="keyword">def </span><a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a54f8d1ec7c0c4228f78a3be843ab3003">forward</a>(self, x):</div><div class="line"><a name="l01023"></a><span class="lineno"> 1023</span>&#160;        <span class="stringliteral">&quot;&quot;&quot;Forward pass.&quot;&quot;&quot;</span></div><div class="line"><a name="l01024"></a><span class="lineno"> 1024</span>&#160;        x = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a234566d2ffaffe67f04c4592cfc84265">nonlinear</a>(self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#addc959c81b7c0ba87a47415af7f07e16">lin1</a>(x))</div><div class="line"><a name="l01025"></a><span class="lineno"> 1025</span>&#160;        x = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a22f7891e6b991a8ee6b58f08c12cf4ae">relu_dropout</a>(x)  <span class="comment"># --relu-dropout</span></div><div class="line"><a name="l01026"></a><span class="lineno"> 1026</span>&#160;        x = self.<a class="code" href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#ae0ee11939e782034e2d9809f18ed4708">lin2</a>(x)</div><div class="line"><a name="l01027"></a><span class="lineno"> 1027</span>&#160;        <span class="keywordflow">return</span> x</div><div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention_html_a0685acda3d791bccfb9a6f4ff2c3680f"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a0685acda3d791bccfb9a6f4ff2c3680f">parlai.agents.transformer.modules.MultiHeadAttention.forward</a></div><div class="ttdeci">def forward(self, query, key=None, value=None, mask=None)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00932">modules.py:932</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_a12c6c4a26be5401e61c43ec4aa886812"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a12c6c4a26be5401e61c43ec4aa886812">parlai.agents.transformer.modules.TransformerMemNetModel.encode_context_memory</a></div><div class="ttdeci">def encode_context_memory(self, context_w, memories_w, context_segments=None)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00224">modules.py:224</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_html"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html">parlai.agents.transformer.modules.BasicAttention</a></div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00853">modules.py:853</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_html_a669fa3f4cd988703371fc127a4943f07"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a669fa3f4cd988703371fc127a4943f07">parlai.agents.transformer.modules.BasicAttention.softmax</a></div><div class="ttdeci">softmax</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00858">modules.py:858</a></div></div>
<div class="ttc" id="namespacerepeat_html"><div class="ttname"><a href="namespacerepeat.html">repeat</a></div><div class="ttdef"><b>Definition:</b> <a href="repeat_8py_source.html#l00001">repeat.py:1</a></div></div>
<div class="ttc" id="namespacemake__control__dataset_html_aab4734ca3fc74fda1c53ad21ec9f47b0"><div class="ttname"><a href="namespacemake__control__dataset.html#aab4734ca3fc74fda1c53ad21ec9f47b0">make_control_dataset.float</a></div><div class="ttdeci">float</div><div class="ttdef"><b>Definition:</b> <a href="make__control__dataset_8py_source.html#l00117">make_control_dataset.py:117</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN_html_a54f8d1ec7c0c4228f78a3be843ab3003"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a54f8d1ec7c0c4228f78a3be843ab3003">parlai.agents.transformer.modules.TransformerFFN.forward</a></div><div class="ttdeci">def forward(self, x)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l01022">modules.py:1022</a></div></div>
<div class="ttc" id="namespaceparlai_1_1agents_1_1transformer_1_1modules_html_ab67607512c597ddd54f2b60a1a1eaf4c"><div class="ttname"><a href="namespaceparlai_1_1agents_1_1transformer_1_1modules.html#ab67607512c597ddd54f2b60a1a1eaf4c">parlai.agents.transformer.modules.get_n_positions_from_options</a></div><div class="ttdeci">def get_n_positions_from_options(opt)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00115">modules.py:115</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer_html_a33ff5864b155499ecf8d31383b33e8d8"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a33ff5864b155499ecf8d31383b33e8d8">parlai.agents.transformer.modules.TransformerEncoderLayer.variant</a></div><div class="ttdeci">variant</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00543">modules.py:543</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_a92d7385aa1b2603fa03ed2f5ad7f66be"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a92d7385aa1b2603fa03ed2f5ad7f66be">parlai.agents.transformer.modules.TransformerMemNetModel.share_word_embedding</a></div><div class="ttdeci">share_word_embedding</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00145">modules.py:145</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_html_ad0beb8f8b3e4514139b08555cf739d55"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#ad0beb8f8b3e4514139b08555cf739d55">parlai.agents.transformer.modules.BasicAttention.dim</a></div><div class="ttdeci">dim</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00862">modules.py:862</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_ab885dd6b80284dbd2837ecaf75f96cb0"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab885dd6b80284dbd2837ecaf75f96cb0">parlai.agents.transformer.modules.TransformerEncoder.n_positions</a></div><div class="ttdeci">n_positions</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00400">modules.py:400</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel_html_a381b6d63e5aba194730c36d9fb93c6d6"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a381b6d63e5aba194730c36d9fb93c6d6">parlai.agents.transformer.modules.TransformerGeneratorModel.start_idx</a></div><div class="ttdeci">start_idx</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00786">modules.py:786</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_a293a7995da2bc66087db07ba4669fb35"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a293a7995da2bc66087db07ba4669fb35">parlai.agents.transformer.modules.TransformerMemNetModel.opt</a></div><div class="ttdeci">opt</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00137">modules.py:137</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention_html_a0ffd2e0d89db2f682561af438aaa1eb5"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a0ffd2e0d89db2f682561af438aaa1eb5">parlai.agents.transformer.modules.MultiHeadAttention.dim</a></div><div class="ttdeci">dim</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00917">modules.py:917</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer_html_ae0f31a52dac8d678fdf6ba573201e7d7"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#ae0f31a52dac8d678fdf6ba573201e7d7">parlai.agents.transformer.modules.TransformerEncoderLayer.dim</a></div><div class="ttdeci">dim</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00540">modules.py:540</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention_html_ac1c0169222b929c580d1a786479d7ed2"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#ac1c0169222b929c580d1a786479d7ed2">parlai.agents.transformer.modules.MultiHeadAttention.attn_dropout</a></div><div class="ttdeci">attn_dropout</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00919">modules.py:919</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN_html_a22f7891e6b991a8ee6b58f08c12cf4ae"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a22f7891e6b991a8ee6b58f08c12cf4ae">parlai.agents.transformer.modules.TransformerFFN.relu_dropout</a></div><div class="ttdeci">relu_dropout</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l01007">modules.py:1007</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_a1fac9c2d34c4d2268f50fc90b1e74e04"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a1fac9c2d34c4d2268f50fc90b1e74e04">parlai.agents.transformer.modules.TransformerEncoder.n_layers</a></div><div class="ttdeci">n_layers</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00389">modules.py:389</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html_aba87c66e402b0cfdf196872c3f038d43"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#aba87c66e402b0cfdf196872c3f038d43">parlai.agents.transformer.modules.TransformerDecoderLayer.dropout</a></div><div class="ttdeci">dropout</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00725">modules.py:725</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel_html_a9b9261bca59f10692c15cbaaa3ffce65"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a9b9261bca59f10692c15cbaaa3ffce65">parlai.agents.transformer.modules.TransformerGeneratorModel.reorder_encoder_states</a></div><div class="ttdeci">def reorder_encoder_states(self, encoder_states, indices)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00824">modules.py:824</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_ad904c0d06a4f6851b38fbbc0cab224f9"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ad904c0d06a4f6851b38fbbc0cab224f9">parlai.agents.transformer.modules.TransformerEncoder.output_scaling</a></div><div class="ttdeci">output_scaling</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00459">modules.py:459</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN_html_ae0ee11939e782034e2d9809f18ed4708"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#ae0ee11939e782034e2d9809f18ed4708">parlai.agents.transformer.modules.TransformerFFN.lin2</a></div><div class="ttdeci">lin2</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l01017">modules.py:1017</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention_html_a495812bb583389a5f5dae989a22bda19"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a495812bb583389a5f5dae989a22bda19">parlai.agents.transformer.modules.MultiHeadAttention.out_lin</a></div><div class="ttdeci">out_lin</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00928">modules.py:928</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_a9ac6c75642035a573f9ac123ee302403"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a9ac6c75642035a573f9ac123ee302403">parlai.agents.transformer.modules.TransformerMemNetModel.n_segments</a></div><div class="ttdeci">n_segments</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00162">modules.py:162</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer_html"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html">parlai.agents.transformer.modules.TransformerEncoderLayer</a></div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00525">modules.py:525</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper_html_ab078bf40bb275ebd56b4e205923ab827"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#ab078bf40bb275ebd56b4e205923ab827">parlai.agents.transformer.modules.TransformerResponseWrapper.__init__</a></div><div class="ttdeci">def __init__(self, transformer, hdim)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00294">modules.py:294</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html_aa82d4bae763a0867633b6d5bc8484129"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#aa82d4bae763a0867633b6d5bc8484129">parlai.agents.transformer.modules.TransformerDecoderLayer.norm1</a></div><div class="ttdeci">norm1</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00730">modules.py:730</a></div></div>
<div class="ttc" id="namespaceparlai_1_1agents_1_1transformer_1_1modules_html_ad93916b8d2188c35733089e1581c44f8"><div class="ttname"><a href="namespaceparlai_1_1agents_1_1transformer_1_1modules.html#ad93916b8d2188c35733089e1581c44f8">parlai.agents.transformer.modules.gelu</a></div><div class="ttdeci">def gelu(tensor)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00106">modules.py:106</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html_a24e685c1a13c11ce7bd4a839bc93ab2a"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a24e685c1a13c11ce7bd4a839bc93ab2a">parlai.agents.transformer.modules.TransformerDecoderLayer.norm3</a></div><div class="ttdeci">norm3</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00740">modules.py:740</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html_ac581c47d531c120ca010ce5fa3836b24"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ac581c47d531c120ca010ce5fa3836b24">parlai.agents.transformer.modules.TransformerDecoderLayer.norm2</a></div><div class="ttdeci">norm2</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00735">modules.py:735</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN_html_a89cffa1831968953cdd7dfdfa7867944"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a89cffa1831968953cdd7dfdfa7867944">parlai.agents.transformer.modules.TransformerFFN.__init__</a></div><div class="ttdeci">def __init__(self, dim, dim_hidden, relu_dropout=0, activation='relu')</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l01005">modules.py:1005</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention_html_a427b8cfaac6d37141555616d9cee8b74"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a427b8cfaac6d37141555616d9cee8b74">parlai.agents.transformer.modules.MultiHeadAttention.__init__</a></div><div class="ttdeci">def __init__(self, n_heads, dim, dropout=0)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00914">modules.py:914</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_html_a98d54b38fac0a88d46eba7adf6ac542c"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a98d54b38fac0a88d46eba7adf6ac542c">parlai.agents.transformer.modules.BasicAttention.get_weights</a></div><div class="ttdeci">get_weights</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00863">modules.py:863</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_html_a04d74d5efbfdf47d36aff30331775368"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a04d74d5efbfdf47d36aff30331775368">parlai.agents.transformer.modules.BasicAttention.attn</a></div><div class="ttdeci">attn</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00861">modules.py:861</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention_html_aa28ff3887a718bb96a9bd27c938a0baf"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#aa28ff3887a718bb96a9bd27c938a0baf">parlai.agents.transformer.modules.MultiHeadAttention.n_heads</a></div><div class="ttdeci">n_heads</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00916">modules.py:916</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html_a94a9a714a2585fd6a675a08643e3ce6b"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a94a9a714a2585fd6a675a08643e3ce6b">parlai.agents.transformer.modules.TransformerDecoderLayer.variant</a></div><div class="ttdeci">variant</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00723">modules.py:723</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper_html_a2676a49582292330a7829e235a7cf8f5"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#a2676a49582292330a7829e235a7cf8f5">parlai.agents.transformer.modules.TransformerLinearWrapper.__init__</a></div><div class="ttdeci">def __init__(self, transformer, output_dim)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00312">modules.py:312</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN_html"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html">parlai.agents.transformer.modules.TransformerFFN</a></div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l01002">modules.py:1002</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html_ad0a63d90190682d0f9ff325fcf60f426"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ad0a63d90190682d0f9ff325fcf60f426">parlai.agents.transformer.modules.TransformerDecoderLayer.forward</a></div><div class="ttdeci">def forward(self, x, encoder_output, encoder_mask)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00742">modules.py:742</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel_html_af165b8ec1505f768c530aea02d68ae09"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#af165b8ec1505f768c530aea02d68ae09">parlai.agents.transformer.modules.TransformerGeneratorModel.encoder</a></div><div class="ttdeci">encoder</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00811">modules.py:811</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer_html_a3bcd00a8073a48b6a1626fccb87be143"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a3bcd00a8073a48b6a1626fccb87be143">parlai.agents.transformer.modules.TransformerEncoderLayer.dropout</a></div><div class="ttdeci">dropout</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00555">modules.py:555</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper_html"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html">parlai.agents.transformer.modules.TransformerResponseWrapper</a></div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00287">modules.py:287</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_a6ea46aeb8ca865c97ec8e99746bedd7b"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a6ea46aeb8ca865c97ec8e99746bedd7b">parlai.agents.transformer.modules.TransformerEncoder.variant</a></div><div class="ttdeci">variant</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00397">modules.py:397</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_ab56fa99f887753677dc7f4d435595114"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab56fa99f887753677dc7f4d435595114">parlai.agents.transformer.modules.TransformerEncoder.dim</a></div><div class="ttdeci">dim</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00391">modules.py:391</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_html_afa25129ca6ce9851053205f44f090fd6"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#afa25129ca6ce9851053205f44f090fd6">parlai.agents.transformer.modules.BasicAttention.__init__</a></div><div class="ttdeci">def __init__(self, dim=1, attn='cosine', residual=False, get_weights=True)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00856">modules.py:856</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper_html_a11ccb5cd5d1b5236b08c904bdeb9e041"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#a11ccb5cd5d1b5236b08c904bdeb9e041">parlai.agents.transformer.modules.TransformerResponseWrapper.mlp</a></div><div class="ttdeci">mlp</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00298">modules.py:298</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN_html_a234566d2ffaffe67f04c4592cfc84265"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#a234566d2ffaffe67f04c4592cfc84265">parlai.agents.transformer.modules.TransformerFFN.nonlinear</a></div><div class="ttdeci">nonlinear</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l01009">modules.py:1009</a></div></div>
<div class="ttc" id="namespaceparlai_1_1utils_1_1misc_html_acf146e70ea7f6867969a7c2b545d4b4b"><div class="ttname"><a href="namespaceparlai_1_1utils_1_1misc.html#acf146e70ea7f6867969a7c2b545d4b4b">parlai.utils.misc.warn_once</a></div><div class="ttdeci">def warn_once(msg, warningtype=None)</div><div class="ttdef"><b>Definition:</b> <a href="misc_8py_source.html#l00992">misc.py:992</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_html_adb18b6c2564672c1820b3c72583dc0c0"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#adb18b6c2564672c1820b3c72583dc0c0">parlai.agents.transformer.modules.BasicAttention.forward</a></div><div class="ttdeci">def forward(self, xs, ys, mask_ys=None, values=None)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00866">modules.py:866</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_a3211cf5e4f390d5cf0f7598a7b789985"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a3211cf5e4f390d5cf0f7598a7b789985">parlai.agents.transformer.modules.TransformerMemNetModel.reduction_type</a></div><div class="ttdeci">reduction_type</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00161">modules.py:161</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_ab29b2b4f3a44d3069d208c61ddba6f21"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#ab29b2b4f3a44d3069d208c61ddba6f21">parlai.agents.transformer.modules.TransformerDecoder.dim</a></div><div class="ttdeci">dim</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00616">modules.py:616</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel_html_a6467f0fdec226329cb8efe77245996a5"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a6467f0fdec226329cb8efe77245996a5">parlai.agents.transformer.modules.TransformerGeneratorModel.decoder</a></div><div class="ttdeci">decoder</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00820">modules.py:820</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_a23005871ba6683940f63511ed463266f"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a23005871ba6683940f63511ed463266f">parlai.agents.transformer.modules.TransformerEncoder.forward</a></div><div class="ttdeci">def forward(self, input, positions=None, segments=None)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00461">modules.py:461</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_a80122462ebd32da923d7b4479053ab01"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a80122462ebd32da923d7b4479053ab01">parlai.agents.transformer.modules.TransformerEncoder.embeddings_scale</a></div><div class="ttdeci">embeddings_scale</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00392">modules.py:392</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html_ac20ce9888aacdabd2766583f28c9d2f5"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ac20ce9888aacdabd2766583f28c9d2f5">parlai.agents.transformer.modules.TransformerDecoderLayer._create_selfattn_mask</a></div><div class="ttdeci">def _create_selfattn_mask(self, x)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00770">modules.py:770</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_a2703f337fd44344ccd93743e57be554e"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a2703f337fd44344ccd93743e57be554e">parlai.agents.transformer.modules.TransformerEncoder.reduction_type</a></div><div class="ttdeci">reduction_type</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00393">modules.py:393</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper_html_af083195926e1f091010ecb4c6d409a2a"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#af083195926e1f091010ecb4c6d409a2a">parlai.agents.transformer.modules.TransformerLinearWrapper.additional_linear_layer</a></div><div class="ttdeci">additional_linear_layer</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00316">modules.py:316</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_a0d41303200949e8406daf8426ce7cbb3"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a0d41303200949e8406daf8426ce7cbb3">parlai.agents.transformer.modules.TransformerDecoder.dropout</a></div><div class="ttdeci">dropout</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00620">modules.py:620</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN_html_addc959c81b7c0ba87a47415af7f07e16"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerFFN.html#addc959c81b7c0ba87a47415af7f07e16">parlai.agents.transformer.modules.TransformerFFN.lin1</a></div><div class="ttdeci">lin1</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l01016">modules.py:1016</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_a253da75d3014b859206169b2d692ab07"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a253da75d3014b859206169b2d692ab07">parlai.agents.transformer.modules.TransformerDecoder.position_embeddings</a></div><div class="ttdeci">position_embeddings</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00638">modules.py:638</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_af28dbf5c6c0f70c1a032a3bbb5359661"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#af28dbf5c6c0f70c1a032a3bbb5359661">parlai.agents.transformer.modules.TransformerEncoder.dropout</a></div><div class="ttdeci">dropout</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00396">modules.py:396</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper_html_ad716cb3a5874f46b1aa4beb45ba1b944"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#ad716cb3a5874f46b1aa4beb45ba1b944">parlai.agents.transformer.modules.TransformerResponseWrapper.forward</a></div><div class="ttdeci">def forward(self, args)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00304">modules.py:304</a></div></div>
<div class="ttc" id="namespaceparlai_1_1messenger_1_1core_1_1shared__utils_html_a11f2820425bccac8d8d490ab505bcb79"><div class="ttname"><a href="namespaceparlai_1_1messenger_1_1core_1_1shared__utils.html#a11f2820425bccac8d8d490ab505bcb79">parlai.messenger.core.shared_utils.format</a></div><div class="ttdeci">format</div><div class="ttdef"><b>Definition:</b> <a href="messenger_2core_2shared__utils_8py_source.html#l00030">shared_utils.py:30</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention_html_a1e86cacd8fe8129222fe04c41c48655d"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a1e86cacd8fe8129222fe04c41c48655d">parlai.agents.transformer.modules.MultiHeadAttention.k_lin</a></div><div class="ttdeci">k_lin</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00921">modules.py:921</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer_html_a68caa3f119bfb946e34b7cf3c8bd9e48"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a68caa3f119bfb946e34b7cf3c8bd9e48">parlai.agents.transformer.modules.TransformerEncoderLayer.__init__</a></div><div class="ttdeci">def __init__(self, n_heads, embedding_size, ffn_size, attention_dropout=0.0, relu_dropout=0.0, dropout=0.0, activation='relu', variant=None)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00538">modules.py:538</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel_html_a18411754103e8c8f4cd2e9338351b98d"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a18411754103e8c8f4cd2e9338351b98d">parlai.agents.transformer.modules.TransformerGeneratorModel.embeddings</a></div><div class="ttdeci">embeddings</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00789">modules.py:789</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer_html_acc0beb3a5c95f3ab45b13dff322e4225"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#acc0beb3a5c95f3ab45b13dff322e4225">parlai.agents.transformer.modules.TransformerEncoderLayer.norm2</a></div><div class="ttdeci">norm2</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00554">modules.py:554</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html">parlai.agents.transformer.modules.TransformerDecoderLayer</a></div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00699">modules.py:699</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper_html_a82f571e85719bde36f8fcf09f27d3e4c"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#a82f571e85719bde36f8fcf09f27d3e4c">parlai.agents.transformer.modules.TransformerLinearWrapper.forward</a></div><div class="ttdeci">def forward(self, args)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00318">modules.py:318</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_a6c12d979c4ce2a21f9a756e7ccc6e7e5"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a6c12d979c4ce2a21f9a756e7ccc6e7e5">parlai.agents.transformer.modules.TransformerMemNetModel.cand_embeddings</a></div><div class="ttdeci">cand_embeddings</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00147">modules.py:147</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer_html_abcdb8b9abb049afacdd132d2a811faff"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#abcdb8b9abb049afacdd132d2a811faff">parlai.agents.transformer.modules.TransformerEncoderLayer.ffn_dim</a></div><div class="ttdeci">ffn_dim</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00541">modules.py:541</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_afd4ec22c3a8b8611a4a1caa85225b426"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#afd4ec22c3a8b8611a4a1caa85225b426">parlai.agents.transformer.modules.TransformerEncoder.out_dim</a></div><div class="ttdeci">out_dim</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00401">modules.py:401</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_aced85fffc20320f29be295f94e686a63"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#aced85fffc20320f29be295f94e686a63">parlai.agents.transformer.modules.TransformerDecoder.activation</a></div><div class="ttdeci">activation</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00617">modules.py:617</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer_html_a2986c3841832cc56d76908347f22a0a3"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a2986c3841832cc56d76908347f22a0a3">parlai.agents.transformer.modules.TransformerEncoderLayer.activation</a></div><div class="ttdeci">activation</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00542">modules.py:542</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_a362f8b951877f3712ca59f0fb6a0ae9c"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a362f8b951877f3712ca59f0fb6a0ae9c">parlai.agents.transformer.modules.TransformerEncoder.__init__</a></div><div class="ttdeci">def __init__(self, n_heads, n_layers, embedding_size, ffn_size, vocabulary_size, embedding=None, dropout=0.0, attention_dropout=0.0, relu_dropout=0.0, padding_idx=0, learn_positional_embeddings=False, embeddings_scale=False, reduction_type='mean', n_positions=1024, activation='relu', variant='aiayn', n_segments=0, output_scaling=1.0)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00384">modules.py:384</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper_html_ae757bd573993a3ca4a31e9d65b1a7e9a"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerResponseWrapper.html#ae757bd573993a3ca4a31e9d65b1a7e9a">parlai.agents.transformer.modules.TransformerResponseWrapper.transformer</a></div><div class="ttdeci">transformer</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00297">modules.py:297</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_a45a356a825e02ed79cdc43cf7f5dc8af"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a45a356a825e02ed79cdc43cf7f5dc8af">parlai.agents.transformer.modules.TransformerDecoder.embeddings_scale</a></div><div class="ttdeci">embeddings_scale</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00619">modules.py:619</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_ab5ec60adca3ce418f73e409187a82625"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab5ec60adca3ce418f73e409187a82625">parlai.agents.transformer.modules.TransformerEncoder.n_segments</a></div><div class="ttdeci">n_segments</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00398">modules.py:398</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_a179492c3f7ebaf6ba8fb3a7f912e93ce"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a179492c3f7ebaf6ba8fb3a7f912e93ce">parlai.agents.transformer.modules.TransformerEncoder.embeddings</a></div><div class="ttdeci">embeddings</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00413">modules.py:413</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_a18e16e542264d07a8c70f0c5b0d2e5ce"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a18e16e542264d07a8c70f0c5b0d2e5ce">parlai.agents.transformer.modules.TransformerDecoder.out_dim</a></div><div class="ttdeci">out_dim</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00623">modules.py:623</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_aa776378904d1ffd3148e330036230713"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#aa776378904d1ffd3148e330036230713">parlai.agents.transformer.modules.TransformerMemNetModel.embeddings</a></div><div class="ttdeci">embeddings</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00141">modules.py:141</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_af83edca868bacf80329be8606ffc7efb"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#af83edca868bacf80329be8606ffc7efb">parlai.agents.transformer.modules.TransformerDecoder.n_heads</a></div><div class="ttdeci">n_heads</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00615">modules.py:615</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html">parlai.agents.transformer.modules.TransformerMemNetModel</a></div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00132">modules.py:132</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html_a33276633e128020853db98128f1d93c3"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a33276633e128020853db98128f1d93c3">parlai.agents.transformer.modules.TransformerDecoderLayer.activation</a></div><div class="ttdeci">activation</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00724">modules.py:724</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html">parlai.agents.transformer.modules.TransformerDecoder</a></div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00567">modules.py:567</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention_html_a84ab720bc232bb48e8f61e37e7c52323"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a84ab720bc232bb48e8f61e37e7c52323">parlai.agents.transformer.modules.MultiHeadAttention.q_lin</a></div><div class="ttdeci">q_lin</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00920">modules.py:920</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention_html"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html">parlai.agents.transformer.modules.MultiHeadAttention</a></div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00907">modules.py:907</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_a95c72e762694a7017243bfcf0b37a1e7"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a95c72e762694a7017243bfcf0b37a1e7">parlai.agents.transformer.modules.TransformerMemNetModel.attender</a></div><div class="ttdeci">attender</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00201">modules.py:201</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_a84349da2592ce07a34257a3995dd2254"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a84349da2592ce07a34257a3995dd2254">parlai.agents.transformer.modules.TransformerDecoder.variant</a></div><div class="ttdeci">variant</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00618">modules.py:618</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer_html_aba0474eb3a08b6c482648adc0804776e"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#aba0474eb3a08b6c482648adc0804776e">parlai.agents.transformer.modules.TransformerEncoderLayer.forward</a></div><div class="ttdeci">def forward(self, tensor, mask)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00557">modules.py:557</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel_html_a17276e3090a132914c0adae133e60357"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a17276e3090a132914c0adae133e60357">parlai.agents.transformer.modules.TransformerGeneratorModel.__init__</a></div><div class="ttdeci">def __init__(self, opt, dictionary)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00784">modules.py:784</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer_html_a2f40ac67a332293a3f51eb11647e2ec1"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a2f40ac67a332293a3f51eb11647e2ec1">parlai.agents.transformer.modules.TransformerEncoderLayer.ffn</a></div><div class="ttdeci">ffn</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00548">modules.py:548</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_a4a930028ffdff0380897228bc156fe74"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a4a930028ffdff0380897228bc156fe74">parlai.agents.transformer.modules.TransformerDecoder.norm_embeddings</a></div><div class="ttdeci">norm_embeddings</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00631">modules.py:631</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_a74702a13d7e41afc3e77c94bb60bd119"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a74702a13d7e41afc3e77c94bb60bd119">parlai.agents.transformer.modules.TransformerEncoder.padding_idx</a></div><div class="ttdeci">padding_idx</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00394">modules.py:394</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention_html_a1b1c33915c80f894221dfd4f0a0a2896"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1MultiHeadAttention.html#a1b1c33915c80f894221dfd4f0a0a2896">parlai.agents.transformer.modules.MultiHeadAttention.v_lin</a></div><div class="ttdeci">v_lin</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00922">modules.py:922</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel_html_a7ee3a87dd94d18353bca09990e8d5821"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#a7ee3a87dd94d18353bca09990e8d5821">parlai.agents.transformer.modules.TransformerGeneratorModel.reorder_decoder_incremental_state</a></div><div class="ttdeci">def reorder_decoder_incremental_state(self, incremental_state, inds)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00837">modules.py:837</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_acbc50af3764335ae22ef2ba06a65c4a7"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#acbc50af3764335ae22ef2ba06a65c4a7">parlai.agents.transformer.modules.TransformerEncoder.segment_embeddings</a></div><div class="ttdeci">segment_embeddings</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00442">modules.py:442</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html_ad656c203b5b2ab890ea419febace286f"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ad656c203b5b2ab890ea419febace286f">parlai.agents.transformer.modules.TransformerDecoderLayer.self_attention</a></div><div class="ttdeci">self_attention</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00727">modules.py:727</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_a19b2e9eecef1384bcfa9f772cac24a20"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a19b2e9eecef1384bcfa9f772cac24a20">parlai.agents.transformer.modules.TransformerDecoder.forward</a></div><div class="ttdeci">def forward(self, input, encoder_state, incr_state=None)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00662">modules.py:662</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html">parlai.agents.transformer.modules.TransformerEncoder</a></div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00327">modules.py:327</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_a899a125583f6fbf7f4809c2744162ac9"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a899a125583f6fbf7f4809c2744162ac9">parlai.agents.transformer.modules.TransformerMemNetModel.context_encoder</a></div><div class="ttdeci">context_encoder</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00164">modules.py:164</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel_html_ab2b8403205fcd407e47546dc56022d2f"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#ab2b8403205fcd407e47546dc56022d2f">parlai.agents.transformer.modules.TransformerGeneratorModel.end_idx</a></div><div class="ttdeci">end_idx</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00787">modules.py:787</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_adf42f25f3ae4564f531835aeec39bb5f"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#adf42f25f3ae4564f531835aeec39bb5f">parlai.agents.transformer.modules.TransformerMemNetModel.cand_encoder</a></div><div class="ttdeci">cand_encoder</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00175">modules.py:175</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_ad2dd819e6d66d17f69e8bfa5026d192a"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ad2dd819e6d66d17f69e8bfa5026d192a">parlai.agents.transformer.modules.TransformerEncoder.position_embeddings</a></div><div class="ttdeci">position_embeddings</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00425">modules.py:425</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_a1e39cdf6357c9e8dd9f90e7f351e42a1"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a1e39cdf6357c9e8dd9f90e7f351e42a1">parlai.agents.transformer.modules.TransformerMemNetModel.encode_cand</a></div><div class="ttdeci">def encode_cand(self, words)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00205">modules.py:205</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_acbeff4ac94daff4e1627b6d93cd37424"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#acbeff4ac94daff4e1627b6d93cd37424">parlai.agents.transformer.modules.TransformerEncoder.layers</a></div><div class="ttdeci">layers</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00445">modules.py:445</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_a8c27f924c01b5017c7968b9d98969496"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a8c27f924c01b5017c7968b9d98969496">parlai.agents.transformer.modules.TransformerMemNetModel.forward</a></div><div class="ttdeci">def forward(self, xs, mems, cands, context_segments=None)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00247">modules.py:247</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_afadc86cb37af464565167dc169447057"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#afadc86cb37af464565167dc169447057">parlai.agents.transformer.modules.TransformerEncoder.ffn_size</a></div><div class="ttdeci">ffn_size</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00388">modules.py:388</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html_ad893ac51805fd662173f777c91b2cca4"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#ad893ac51805fd662173f777c91b2cca4">parlai.agents.transformer.modules.TransformerDecoderLayer.__init__</a></div><div class="ttdeci">def __init__(self, n_heads, embedding_size, ffn_size, attention_dropout=0.0, relu_dropout=0.0, dropout=0.0, activation='relu', variant='aiayn')</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00719">modules.py:719</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html_a394248deb2cfa738a51f890d4c232224"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a394248deb2cfa738a51f890d4c232224">parlai.agents.transformer.modules.TransformerDecoderLayer.ffn</a></div><div class="ttdeci">ffn</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00737">modules.py:737</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_ab6830a5aa0850e50549c159586c17515"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ab6830a5aa0850e50549c159586c17515">parlai.agents.transformer.modules.TransformerEncoder.n_heads</a></div><div class="ttdeci">n_heads</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00390">modules.py:390</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel_html"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html">parlai.agents.transformer.modules.TransformerGeneratorModel</a></div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00781">modules.py:781</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_ac9356241c0dd44bf980a155124cbde59"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#ac9356241c0dd44bf980a155124cbde59">parlai.agents.transformer.modules.TransformerDecoder.__init__</a></div><div class="ttdeci">def __init__(self, n_heads, n_layers, embedding_size, ffn_size, vocabulary_size, embedding=None, dropout=0.0, attention_dropout=0.0, relu_dropout=0.0, embeddings_scale=True, learn_positional_embeddings=False, padding_idx=None, n_positions=1024, n_segments=0, variant='aiayn', activation='relu')</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00610">modules.py:610</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel_html_aefac02b3630ef361bb008b4f7973c55b"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#aefac02b3630ef361bb008b4f7973c55b">parlai.agents.transformer.modules.TransformerGeneratorModel.pad_idx</a></div><div class="ttdeci">pad_idx</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00785">modules.py:785</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_ae8676d5d506949c00d35c34ac41b9fff"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#ae8676d5d506949c00d35c34ac41b9fff">parlai.agents.transformer.modules.TransformerEncoder.norm_embeddings</a></div><div class="ttdeci">norm_embeddings</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00435">modules.py:435</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_abb275df1d00ad62deb2424266f1563d6"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#abb275df1d00ad62deb2424266f1563d6">parlai.agents.transformer.modules.TransformerDecoder.embedding_size</a></div><div class="ttdeci">embedding_size</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00612">modules.py:612</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_a0b1598bb8f1a8b899c00af25044ffe9c"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a0b1598bb8f1a8b899c00af25044ffe9c">parlai.agents.transformer.modules.TransformerMemNetModel.__init__</a></div><div class="ttdeci">def __init__(self, opt, dictionary)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00135">modules.py:135</a></div></div>
<div class="ttc" id="namespaceparlai_1_1utils_1_1misc_html_a68c44ca571de7149b683539db659c330"><div class="ttname"><a href="namespaceparlai_1_1utils_1_1misc.html#a68c44ca571de7149b683539db659c330">parlai.utils.misc.neginf</a></div><div class="ttdeci">def neginf(dtype)</div><div class="ttdef"><b>Definition:</b> <a href="misc_8py_source.html#l00054">misc.py:54</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_a90c40d660300194a42a18dc38f7e9fb2"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a90c40d660300194a42a18dc38f7e9fb2">parlai.agents.transformer.modules.TransformerDecoder.ffn_size</a></div><div class="ttdeci">ffn_size</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00613">modules.py:613</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_html_a98c0f263db25cf305c333e04f8a0843d"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#a98c0f263db25cf305c333e04f8a0843d">parlai.agents.transformer.modules.BasicAttention.residual</a></div><div class="ttdeci">residual</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00864">modules.py:864</a></div></div>
<div class="ttc" id="classparlai_1_1core_1_1torch__generator__agent_1_1TorchGeneratorModel_html"><div class="ttname"><a href="classparlai_1_1core_1_1torch__generator__agent_1_1TorchGeneratorModel.html">parlai.core.torch_generator_agent.TorchGeneratorModel</a></div><div class="ttdef"><b>Definition:</b> <a href="torch__generator__agent_8py_source.html#l00033">torch_generator_agent.py:33</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_a6d1a5e20639c71de7a60e63b88238784"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#a6d1a5e20639c71de7a60e63b88238784">parlai.agents.transformer.modules.TransformerMemNetModel.memory_transformer</a></div><div class="ttdeci">memory_transformer</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00195">modules.py:195</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention_html_abed26e42fa2293f14cbbcf51c090794c"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1BasicAttention.html#abed26e42fa2293f14cbbcf51c090794c">parlai.agents.transformer.modules.BasicAttention.cosine</a></div><div class="ttdeci">cosine</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00860">modules.py:860</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html_a296aa88e8a31be628bb31f55de0a0360"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a296aa88e8a31be628bb31f55de0a0360">parlai.agents.transformer.modules.TransformerDecoderLayer.ffn_dim</a></div><div class="ttdeci">ffn_dim</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00722">modules.py:722</a></div></div>
<div class="ttc" id="namespaceparlai_1_1agents_1_1transformer_1_1modules_html_a0b86437e6e9682fa3100e9cadcaae259"><div class="ttname"><a href="namespaceparlai_1_1agents_1_1transformer_1_1modules.html#a0b86437e6e9682fa3100e9cadcaae259">parlai.agents.transformer.modules.create_position_codes</a></div><div class="ttdeci">def create_position_codes(n_pos, dim, out)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00272">modules.py:272</a></div></div>
<div class="ttc" id="namespaceparlai_1_1core_1_1torch__generator__agent_html"><div class="ttname"><a href="namespaceparlai_1_1core_1_1torch__generator__agent.html">parlai.core.torch_generator_agent</a></div><div class="ttdef"><b>Definition:</b> <a href="torch__generator__agent_8py_source.html#l00001">torch_generator_agent.py:1</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder_html_a57f137ae98789edeae26de90118efdd6"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoder.html#a57f137ae98789edeae26de90118efdd6">parlai.agents.transformer.modules.TransformerEncoder.embedding_size</a></div><div class="ttdeci">embedding_size</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00387">modules.py:387</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_a20983026d2ed3edd48698ed08d666287"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a20983026d2ed3edd48698ed08d666287">parlai.agents.transformer.modules.TransformerDecoder.n_layers</a></div><div class="ttdeci">n_layers</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00614">modules.py:614</a></div></div>
<div class="ttc" id="namespaceparlai_1_1utils_1_1misc_html"><div class="ttname"><a href="namespaceparlai_1_1utils_1_1misc.html">parlai.utils.misc</a></div><div class="ttdef"><b>Definition:</b> <a href="misc_8py_source.html#l00001">misc.py:1</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel_html_ab70ecab55a92126804e165d745642142"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerMemNetModel.html#ab70ecab55a92126804e165d745642142">parlai.agents.transformer.modules.TransformerMemNetModel.pad_idx</a></div><div class="ttdeci">pad_idx</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00138">modules.py:138</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_a91210271cf67a77c0ab81f03f29bf70d"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a91210271cf67a77c0ab81f03f29bf70d">parlai.agents.transformer.modules.TransformerDecoder.n_positions</a></div><div class="ttdeci">n_positions</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00622">modules.py:622</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer_html_ad8b0a3b903ca2595e72dd54f82f4dd9f"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#ad8b0a3b903ca2595e72dd54f82f4dd9f">parlai.agents.transformer.modules.TransformerEncoderLayer.attention</a></div><div class="ttdeci">attention</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00544">modules.py:544</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_aa6d97e0eeed7b3cc11e4b60fb4a86167"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#aa6d97e0eeed7b3cc11e4b60fb4a86167">parlai.agents.transformer.modules.TransformerDecoder.embeddings</a></div><div class="ttdeci">embeddings</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00628">modules.py:628</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel_html_ae0f0d3d038cc12075501aae3a059c997"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerGeneratorModel.html#ae0f0d3d038cc12075501aae3a059c997">parlai.agents.transformer.modules.TransformerGeneratorModel.output</a></div><div class="ttdeci">def output(self, tensor)</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00846">modules.py:846</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper_html_acac2d2f099f07b21f792326f90541ffd"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html#acac2d2f099f07b21f792326f90541ffd">parlai.agents.transformer.modules.TransformerLinearWrapper.transformer</a></div><div class="ttdeci">transformer</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00314">modules.py:314</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer_html_a74730602aaea5afd4564d0f35c84af15"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerEncoderLayer.html#a74730602aaea5afd4564d0f35c84af15">parlai.agents.transformer.modules.TransformerEncoderLayer.norm1</a></div><div class="ttdeci">norm1</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00547">modules.py:547</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html_afc3159ec4e88329e2967edf28923ea20"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#afc3159ec4e88329e2967edf28923ea20">parlai.agents.transformer.modules.TransformerDecoderLayer.dim</a></div><div class="ttdeci">dim</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00721">modules.py:721</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper_html"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerLinearWrapper.html">parlai.agents.transformer.modules.TransformerLinearWrapper</a></div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00309">modules.py:309</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer_html_a50be54030dc6526fd5cae1f48a448f04"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoderLayer.html#a50be54030dc6526fd5cae1f48a448f04">parlai.agents.transformer.modules.TransformerDecoderLayer.encoder_attention</a></div><div class="ttdeci">encoder_attention</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00732">modules.py:732</a></div></div>
<div class="ttc" id="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder_html_a5007810b22cdfddf768bdd94c233f87e"><div class="ttname"><a href="classparlai_1_1agents_1_1transformer_1_1modules_1_1TransformerDecoder.html#a5007810b22cdfddf768bdd94c233f87e">parlai.agents.transformer.modules.TransformerDecoder.layers</a></div><div class="ttdeci">layers</div><div class="ttdef"><b>Definition:</b> <a href="parlai_2agents_2transformer_2modules_8py_source.html#l00647">modules.py:647</a></div></div>
</div><!-- fragment --></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
