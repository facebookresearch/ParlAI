<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ParlAI: projects/personality_captions/README.md Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">ParlAI
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">projects/personality_captions/README.md</div>  </div>
</div><!--header-->
<div class="contents">
<a href="projects_2personality__captions_2README_8md.html">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;# Engaging Image Captioning via Personality</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;Kurt Shuster, Samuel Humeau, Hexiang Hu, Antoine Bordes, Jason Weston</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;Please see [Shuster et al. (CVPR 2019)](https://arxiv.org/abs/1810.10665) for more details.</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;## Abstract</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;Standard image captioning tasks such as COCO and Flickr30k are factual, neutral in tone and (to a human) state the obvious (e.g., &quot;a man playing a guitar&quot;). While such tasks are useful to verify that a machine understands the content of an image, they are not engaging to humans as captions. With this in mind we define a new task, Personality-Captions, where the goal is to be as engaging to humans as possible by incorporating controllable style and personality traits. We collect and release a large dataset of 201,858 of such captions conditioned over 215 possible traits. We build models that combine existing work from (i) sentence representations (Mazare et al., 2018) with Transformers trained on 1.7 billion dialogue examples; and (ii) image representations (Mahajan et al., 2018) with ResNets trained on 3.5 billion social media images. We obtain state-of-the-art performance on Flickr30k and COCO, and strong performance on our new task. Finally, online evaluations validate that our task and models are engaging to humans, with our best model close to human performance.</div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;</div><div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;## Dataset</div><div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;</div><div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;The Personality-Captions dataset can be accessed via ParlAI, with `-t personality_captions`. </div><div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;See the [ParlAI quickstart for help](http://www.parl.ai/static/docs/tutorial_quick.html).</div><div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;</div><div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;Additionally, the ParlAI MTurk tasks for data collection and human evaluation</div><div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;are [made available](https://github.com/facebookresearch/ParlAI/tree/master/parlai/mturk/tasks/personality_captions) in ParlAI.</div><div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;</div><div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;## Leaderboards for Personality-Captions Task</div><div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;</div><div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;### Retrieval Models</div><div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;</div><div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;Model                                | Paper          | Test R@1</div><div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;------------------------------------ | -------------- | --------</div><div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;TransResNet, ResNeXt-IG-3.5B         | [Shuster et al. (2019)](https://arxiv.org/abs/1810.10665) | 77.5</div><div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;TransResNet, ResNet152               | [Shuster et al. (2019)](https://arxiv.org/abs/1810.10665) | 51.7</div><div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;TransResNet, No Images               | [Shuster et al. (2019)](https://arxiv.org/abs/1810.10665) | 25.8</div><div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;</div><div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;### Generative Models</div><div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;</div><div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;Model                                | Paper          | BLEU1 | BLEU4 | ROUGE-L | CIDEr | SPICE</div><div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;------------------------------------ | -------------- | ----- | ----- | ------- | ----- | -----</div><div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;UpDown, ResNeXt-IG-3.5B              | [Shuster et al. (2019)](https://arxiv.org/abs/1810.10665) | 44.0 | 8.0 | 27.4 | 16.5 | 5.2</div><div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;ShowAttTell, ResNeXt-IG-3.5B         | [Shuster et al. (2019)](https://arxiv.org/abs/1810.10665) | 43.3 | 7.1 | 27.0 | 12.6 | 3.6</div><div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;ShotTell, ResNeXt-IG-3.5B            | [Shuster et al. (2019)](https://arxiv.org/abs/1810.10665) | 38.4 | 7.3 | 24.3 | 9.6 | 1.6</div><div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;</div><div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;</div><div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;## Pretrained Models</div><div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;</div><div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;We provide our best model trained with ResNet152 image features. To evaluate the model, specify the following command:</div><div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;</div><div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;      python examples/eval_model.py \</div><div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;          -bs 128 -t personality_captions</div><div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;          -mf models:personality_captions/transresnet/model</div><div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;          --num-test-labels 5 -dt test</div><div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;</div><div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;Which yields the following results:</div><div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;</div><div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;      {&#39;exs&#39;: 10000, &#39;accuracy&#39;: 0.5113, &#39;f1&#39;: 0.5951, &#39;hits@1&#39;: 0.511, &#39;hits@5&#39;: 0.816,</div><div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;      &#39;hits@10&#39;: 0.903, &#39;hits@100&#39;: 0.998, &#39;bleu&#39;: 0.4999, &#39;hits@1/100&#39;: 1.0,</div><div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;      &#39;loss&#39;: -0.002, &#39;med_rank&#39;: 1.0}</div><div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;</div><div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;Additionally, we provide an interactive script that you can use to view outputs of our pretrained model.</div><div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;Simply run the following command:</div><div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;</div><div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;      python projects/personality_captions/interactive.py \</div><div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;      -mf models:personality_captions/transresnet/model</div><div class="line"><a name="l00058"></a><span class="lineno">   58</span>&#160;</div><div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;Which will allow you to upload an image and choose a personality for the model to use.</div><div class="line"><a name="l00060"></a><span class="lineno">   60</span>&#160;</div><div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;## Model Examples</div><div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;</div><div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;85%&quot; src=&quot;Examples.png&quot; /&gt;&lt;/p&gt;</div><div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;</div><div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;## Citation</div><div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;</div><div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;If you use the dataset or models in your own work, please cite with the following BibText entry:</div><div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;</div><div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;        @InProceedings{Shuster_2019_CVPR,</div><div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;        author = {Shuster, Kurt and Humeau, Samuel and Hu, Hexiang and Bordes, Antoine and Weston, Jason},</div><div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;        title = {Engaging Image Captioning via Personality},</div><div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;        booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},</div><div class="line"><a name="l00073"></a><span class="lineno">   73</span>&#160;        month = {June},</div><div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;        year = {2019}</div><div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;        }</div></div><!-- fragment --></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
