This directory contains code and examples for the Conv\+A\+I2 competition. See the \href{http://convai.io/}{\tt website} for more details.

The Conv\+A\+I2 dataset is in Parl\+AI in parlai/agents/tasks/convai2 and is accessible by using the command line parameter {\ttfamily -\/-\/task \hyperlink{namespaceconvai2}{convai2}}.

For example, to see the training data, try\+: 
\begin{DoxyCode}
python ~/ParlAI/examples/display\_data.py -t convai2 -dt train
\end{DoxyCode}


\subsection*{Examples}

You can run examples of training on this task in the {\ttfamily baselines} folder in this directory.

For example, you can download and interact with a pre-\/trained seq2seq model (based on Parl\+AI\textquotesingle{}s implementation in parlai/agents/seq2seq) using {\ttfamily baselines/seq2seq/interact.\+py}, or train your own with the same parameters using the {\ttfamily train.\+py} file in that directory.

\subsection*{Submitting models for evaluation}

To submit an entry, create a private repo with your model that works with our evaluation code, and share it with the following github accounts\+: \href{mailto:edinan@fb.com}{\tt edinan@fb.\+com}, \href{mailto:jaseweston@gmail.com}{\tt jaseweston@gmail.\+com}, \href{mailto:jju@fb.com}{\tt jju@fb.\+com}, \href{mailto:kshuster@fb.com}{\tt kshuster@fb.\+com}.

More specifically, create a repo with a directory similar to the baseline models directory, with a {\ttfamily eval\+\_\+\+X\+X\+X.\+py} file with each metric you would like to be evaluated against. These files should import the appropriate eval function from this directory. For example, the {\ttfamily eval\+\_\+f1.\+py} file in {\ttfamily baselines/seq2seq/} sets up the right parameters for the model and then imports the evaluation function from the base {\ttfamily eval\+\_\+f1.\+py} file in this directory and runs it.

We will then run the automatic evaluations against the hidden test set (which is in the same format as the validation set you can access) and update the leaderboard. You can submit a maximum of once per month. We will use the same submitted code for the top performing models for computing human evaluations when the submission system is locked on September 30th.

\subsection*{Talking to a model interactively on the commandline\+:}

This is possible with this script\+:


\begin{DoxyCode}
python projects/convai2/interactive.py -mf models:convai2/kvmemnn/model
\end{DoxyCode}


You can change the model you talk to with the \textquotesingle{}--model-\/file\textquotesingle{} (-\/mf) argument, e.\+g. you can use seq2seq too\+:


\begin{DoxyCode}
python projects/convai2/interactive.py -mf models:convai2/seq2seq/convai2\_self\_seq2seq\_model -m
       legacy:seq2seq:0
\end{DoxyCode}


\subsection*{Remarks on dataset versions}

The several versions of the dataset can be accessed with {\ttfamily \hyperlink{namespaceconvai2}{convai2}\+:self}, {\ttfamily \hyperlink{namespaceconvai2}{convai2}\+:self\+\_\+revised} and {\ttfamily \hyperlink{namespaceconvai2}{convai2}\+:none}.

These correspond to \char`\"{}original self persona\char`\"{}, \char`\"{}revised self persona\char`\"{} and \char`\"{}no persona\char`\"{} in the original \href{https://arxiv.org/pdf/1801.07243.pdf}{\tt Persona\+Chat} paper. However, in contrast to that dataset, we have modified the preprocessing and are generating a new hidden test set. We have also added training examples from the perspective of both speakers and additional candidates to support training / evaluating using a ranking loss.

You can use the {\ttfamily none} mode to contrast your model\textquotesingle{}s ability to take advantage of its persona with {\ttfamily self}, but {\ttfamily self} is the default setting for the task, and you will be evaluated on \textquotesingle{}self\+\_\+orignal\textquotesingle{} for the leaderboard.

You can see examples of models prepared for the original (non-\/competition) Persona\+Chat task \href{https://github.com/facebookresearch/ParlAI/tree/master/projects/personachat}{\tt here}, which should generally transfer pretty easily to this task. However, we will continue to add baseline models to this project folder as we run experiments. 