Paper\+: \href{https://arxiv.org/abs/1905.01969}{\tt https\+://arxiv.\+org/abs/1905.\+01969}

\subsection*{Abstract}

The use of deep pre-\/trained bidirectional transformers has led to remarkable progress in anumber of applications (Devlin et al., 2018). For tasks that make pairwise comparisons between sequences, matching a given input with a corresponding label, two approaches are common\+: Cross-\/encoders performing full self-\/attention over the pair and Bi-\/encoders encoding the pair separately. The former often performs better, but is too slow for practical use. In this work, we develop a new transformer architecture, the Poly-\/encoder, that learns global rather than token level self-\/attention features. We perform a detailed comparison of all three approaches, including what pre-\/training and fine-\/tuning strategies work best. We show our models achieve state-\/of-\/the-\/art results on three existing tasks; that Poly-\/encoders are faster than Cross-\/encoders and more accurate than Bi-\/encoders; and that the best results are obtained by pre-\/training on large datasets similar to the downstream tasks.

\subsection*{Code and models}

Below we give details about available code and models\+:
\begin{DoxyItemize}
\item pretrained transformers on Reddit and Wikipedia + Toronto Books that can be used as a base for pretraining
\begin{DoxyItemize}
\item pretrained models fine tuned on Conv\+A\+I2 for the bi-\/encoder and poly-\/encoder The polyencoder scores 89+ on \hyperlink{namespaceconvai2}{convai2} valid set and is fast enough to interact in real time with 100k candidates (which are provided.)
\end{DoxyItemize}
\end{DoxyItemize}

\subsection*{Interacting with a pretrained model on Convai2}

Run this command\+: (assumes your model zoo is in the default ./data/models) 
\begin{DoxyCode}
python examples/interactive.py -m transformer/polyencoder \(\backslash\)
    -mf zoo:pretrained\_transformers/model\_poly/model \(\backslash\)
    --encode-candidate-vecs true \(\backslash\)
    --eval-candidates fixed  \(\backslash\)
    --fixed-candidates-path data/models/pretrained\_transformers/convai\_trainset\_cands.txt
\end{DoxyCode}


Example output\+: 
\begin{DoxyCode}
Enter Your Message: your persona: i love to drink fancy tea.\(\backslash\)nyour persona: i have a big library at
       home.\(\backslash\)nyour persona: i'm a museum tour guide.\(\backslash\)nhi how are you doing ?
[Polyencoder]: i am alright . i am back from the library .
Enter Your Message: oh, what do you do for a living?
[Polyencoder]: i work at the museum downtown . i love it there .
Enter Your Message: what is your favorite drink?
[Polyencoder]: i am more of a tea guy . i get my tea from china .
\end{DoxyCode}


Note the polyencoder gives 89+ hits@1/20 on \hyperlink{namespaceconvai2}{convai2}, however, it expects data that is close to the dataset. If you do not include the multiple \textquotesingle{}your persona\+: ...~\newline
\textquotesingle{} at the beginning it will answer nonsense.

\subsection*{Fine tuning on your own tasks}

\subsubsection*{bi-\/encoder}

Execute this to train a biencoder scoring 86+ on Convai2 valid set (requires 8 x G\+PU 32\+GB., If you don\textquotesingle{}t have this, reduce the batch size )


\begin{DoxyCode}
python -u examples/train\_model.py \(\backslash\)
    --init-model zoo:pretrained\_transformers/bi\_model\_huge\_reddit/model \(\backslash\)
    --batchsize 512 -pyt convai2 \(\backslash\)
    --shuffle true --model transformer/biencoder --eval-batchsize 6 \(\backslash\)
    --warmup\_updates 100 --lr-scheduler-patience 0 \(\backslash\)
    --lr-scheduler-decay 0.4 -lr 5e-05 --data-parallel True \(\backslash\)
    --history-size 20 --label-truncate 72 --text-truncate 360 \(\backslash\)
    --num-epochs 10.0 --max\_train\_time 200000 -veps 0.5 -vme 8000 \(\backslash\)
    --validation-metric accuracy --validation-metric-mode max \(\backslash\)
    --save-after-valid True --log\_every\_n\_secs 20 --candidates batch \(\backslash\)
    --dict-tokenizer bpe --dict-lower True --optimizer adamax \(\backslash\)
    --output-scaling 0.06 \(\backslash\)
     --variant xlm --reduction-type mean --share-encoders False \(\backslash\)
     --learn-positional-embeddings True --n-layers 12 --n-heads 12 \(\backslash\)
     --ffn-size 3072 --attention-dropout 0.1 --relu-dropout 0.0 --dropout 0.1 \(\backslash\)
     --n-positions 1024 --embedding-size 768 --activation gelu \(\backslash\)
     --embeddings-scale False --n-segments 2 --learn-embeddings True \(\backslash\)
     --share-word-embeddings False --dict-endtoken \_\_start\_\_ --fp16 True \(\backslash\)
     --model-file <YOUR MODEL FILE>
\end{DoxyCode}


\subsubsection*{poly-\/encoder}

Execute this to train a poly-\/encoder scoring 89+ on Convai2 valid set (requires 8 x G\+PU 32\+GB., If you don\textquotesingle{}t have this, reduce the batch size )


\begin{DoxyCode}
python -u examples/train\_model.py \(\backslash\)
  --init-model zoo:pretrained\_transformers/poly\_model\_huge\_reddit/model \(\backslash\)
  -pyt convai2 --shuffle true \(\backslash\)
  --model transformer/polyencoder --batchsize 256 --eval-batchsize 10 \(\backslash\)
  --warmup\_updates 100 --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 \(\backslash\)
  -lr 5e-05 --data-parallel True --history-size 20 --label-truncate 72 \(\backslash\)
  --text-truncate 360 --num-epochs 8.0 --max\_train\_time 200000 -veps 0.5 \(\backslash\)
  -vme 8000 --validation-metric accuracy --validation-metric-mode max \(\backslash\)
  --save-after-valid True --log\_every\_n\_secs 20 --candidates batch --fp16 True \(\backslash\)
  --dict-tokenizer bpe --dict-lower True --optimizer adamax --output-scaling 0.06 \(\backslash\)
  --variant xlm --reduction-type mean --share-encoders False \(\backslash\)
  --learn-positional-embeddings True --n-layers 12 --n-heads 12 --ffn-size 3072 \(\backslash\)
  --attention-dropout 0.1 --relu-dropout 0.0 --dropout 0.1 --n-positions 1024 \(\backslash\)
  --embedding-size 768 --activation gelu --embeddings-scale False --n-segments 2 \(\backslash\)
  --learn-embeddings True --polyencoder-type codes --poly-n-codes 64 \(\backslash\)
  --poly-attention-type basic --dict-endtoken \_\_start\_\_ \(\backslash\)
  --model-file <YOUR MODEL FILE>
\end{DoxyCode}


\subsubsection*{cross-\/encoder}

Execute this to train a cross-\/encoder scoring 90+ on Convai2 valid set (requires 8 x G\+PU 32\+GB., If you don\textquotesingle{}t have this, reduce the batch size )


\begin{DoxyCode}
python -u examples/train\_model.py \(\backslash\)
  --init-model zoo:pretrained\_transformers/cross\_model\_huge\_reddit/model \(\backslash\)
  -pyt convai2 --shuffle true \(\backslash\)
  --model transformer/crossencoder --batchsize 16 --eval-batchsize 10 \(\backslash\)
  --warmup\_updates 1000 --lr-scheduler-patience 0 --lr-scheduler-decay 0.4 \(\backslash\)
  -lr 5e-05 --data-parallel True --history-size 20 --label-truncate 72 \(\backslash\)
  --text-truncate 360 --num-epochs 12.0 --max\_train\_time 200000 -veps 0.5 \(\backslash\)
  -vme 2500 --validation-metric accuracy --validation-metric-mode max \(\backslash\)
  --save-after-valid True --log\_every\_n\_secs 20 --candidates inline --fp16 True \(\backslash\)
  --dict-tokenizer bpe --dict-lower True --optimizer adamax --output-scaling 0.06 \(\backslash\)
  --variant xlm --reduction-type first --share-encoders False \(\backslash\)
  --learn-positional-embeddings True --n-layers 12 --n-heads 12 --ffn-size 3072 \(\backslash\)
  --attention-dropout 0.1 --relu-dropout 0.0 --dropout 0.1 --n-positions 1024 \(\backslash\)
  --embedding-size 768 --activation gelu --embeddings-scale False --n-segments 2 \(\backslash\)
  --learn-embeddings True --dict-endtoken \_\_start\_\_ \(\backslash\)
  --model-file <YOUR MODEL FILE>
\end{DoxyCode}
 