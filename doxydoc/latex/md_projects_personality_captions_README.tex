Kurt Shuster, Samuel Humeau, Hexiang Hu, Antoine Bordes, Jason Weston

Please see \href{https://arxiv.org/abs/1810.10665}{\tt Shuster et al. (C\+V\+PR 2019)} for more details.

\subsection*{Abstract}

Standard image captioning tasks such as C\+O\+CO and Flickr30k are factual, neutral in tone and (to a human) state the obvious (e.\+g., \char`\"{}a man playing a guitar\char`\"{}). While such tasks are useful to verify that a machine understands the content of an image, they are not engaging to humans as captions. With this in mind we define a new task, Personality-\/\+Captions, where the goal is to be as engaging to humans as possible by incorporating controllable style and personality traits. We collect and release a large dataset of 201,858 of such captions conditioned over 215 possible traits. We build models that combine existing work from (i) sentence representations (Mazare et al., 2018) with Transformers trained on 1.\+7 billion dialogue examples; and (ii) image representations (Mahajan et al., 2018) with Res\+Nets trained on 3.\+5 billion social media images. We obtain state-\/of-\/the-\/art performance on Flickr30k and C\+O\+CO, and strong performance on our new task. Finally, online evaluations validate that our task and models are engaging to humans, with our best model close to human performance.

\subsection*{Dataset}

The Personality-\/\+Captions dataset can be accessed via Parl\+AI, with {\ttfamily -\/t \hyperlink{namespacepersonality__captions}{personality\+\_\+captions}}. See the \href{http://www.parl.ai/static/docs/tutorial_quick.html}{\tt Parl\+AI quickstart for help}.

Additionally, the Parl\+AI M\+Turk tasks for data collection and human evaluation are \href{https://github.com/facebookresearch/ParlAI/tree/master/parlai/mturk/tasks/personality_captions}{\tt made available} in Parl\+AI.

\subsection*{Leaderboards for Personality-\/\+Captions Task}

\subsubsection*{Retrieval Models}

\tabulinesep=1mm
\begin{longtabu} spread 0pt [c]{*{3}{|X[-1]}|}
\hline
\rowcolor{\tableheadbgcolor}\textbf{ Model }&\textbf{ Paper }&\textbf{ Test R@1  }\\\cline{1-3}
\endfirsthead
\hline
\endfoot
\hline
\rowcolor{\tableheadbgcolor}\textbf{ Model }&\textbf{ Paper }&\textbf{ Test R@1  }\\\cline{1-3}
\endhead
Trans\+Res\+Net, Res\+Ne\+Xt-\/\+I\+G-\/3.\+5B &\href{https://arxiv.org/abs/1810.10665}{\tt Shuster et al. (2019)} &77.\+5 \\\cline{1-3}
Trans\+Res\+Net, Res\+Net152 &\href{https://arxiv.org/abs/1810.10665}{\tt Shuster et al. (2019)} &51.\+7 \\\cline{1-3}
Trans\+Res\+Net, No Images &\href{https://arxiv.org/abs/1810.10665}{\tt Shuster et al. (2019)} &25.\+8 \\\cline{1-3}
\end{longtabu}
\subsubsection*{Generative Models}

\tabulinesep=1mm
\begin{longtabu} spread 0pt [c]{*{7}{|X[-1]}|}
\hline
\rowcolor{\tableheadbgcolor}\textbf{ Model }&\textbf{ Paper }&\textbf{ B\+L\+E\+U1 }&\textbf{ B\+L\+E\+U4 }&\textbf{ R\+O\+U\+G\+E-\/L }&\textbf{ C\+I\+D\+Er }&\textbf{ S\+P\+I\+CE  }\\\cline{1-7}
\endfirsthead
\hline
\endfoot
\hline
\rowcolor{\tableheadbgcolor}\textbf{ Model }&\textbf{ Paper }&\textbf{ B\+L\+E\+U1 }&\textbf{ B\+L\+E\+U4 }&\textbf{ R\+O\+U\+G\+E-\/L }&\textbf{ C\+I\+D\+Er }&\textbf{ S\+P\+I\+CE  }\\\cline{1-7}
\endhead
Up\+Down, Res\+Ne\+Xt-\/\+I\+G-\/3.\+5B &\href{https://arxiv.org/abs/1810.10665}{\tt Shuster et al. (2019)} &44.\+0 &8.\+0 &27.\+4 &16.\+5 &5.\+2 \\\cline{1-7}
Show\+Att\+Tell, Res\+Ne\+Xt-\/\+I\+G-\/3.\+5B &\href{https://arxiv.org/abs/1810.10665}{\tt Shuster et al. (2019)} &43.\+3 &7.\+1 &27.\+0 &12.\+6 &3.\+6 \\\cline{1-7}
Shot\+Tell, Res\+Ne\+Xt-\/\+I\+G-\/3.\+5B &\href{https://arxiv.org/abs/1810.10665}{\tt Shuster et al. (2019)} &38.\+4 &7.\+3 &24.\+3 &9.\+6 &1.\+6 \\\cline{1-7}
\end{longtabu}


\subsection*{Pretrained Models}

We provide our best model trained with Res\+Net152 image features. To evaluate the model, specify the following command\+: \begin{DoxyVerb}  python examples/eval_model.py \
      -bs 128 -t personality_captions
      -mf models:personality_captions/transresnet/model
      --num-test-labels 5 -dt test
\end{DoxyVerb}


Which yields the following results\+: \begin{DoxyVerb}  {'exs': 10000, 'accuracy': 0.5113, 'f1': 0.5951, 'hits@1': 0.511, 'hits@5': 0.816,
  'hits@10': 0.903, 'hits@100': 0.998, 'bleu': 0.4999, 'hits@1/100': 1.0,
  'loss': -0.002, 'med_rank': 1.0}
\end{DoxyVerb}


Additionally, we provide an interactive script that you can use to view outputs of our pretrained model. Simply run the following command\+: \begin{DoxyVerb}  python projects/personality_captions/interactive.py \
  -mf models:personality_captions/transresnet/model
\end{DoxyVerb}


Which will allow you to upload an image and choose a personality for the model to use.

\subsection*{Model Examples}



\subsection*{Citation}

If you use the dataset or models in your own work, please cite with the following Bib\+Text entry\+: \begin{DoxyVerb}    @InProceedings{Shuster_2019_CVPR,
    author = {Shuster, Kurt and Humeau, Samuel and Hu, Hexiang and Bordes, Antoine and Weston, Jason},
    title = {Engaging Image Captioning via Personality},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2019}
    }\end{DoxyVerb}
 