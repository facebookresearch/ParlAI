\subsection*{Paper information}

Abigail See, Stephen Roller, Douwe Kiela, Jason Weston. {\itshape \href{https://arxiv.org/abs/1902.08654}{\tt What makes a good conversation? How controllable attributes affect human judgments}}. To appear in N\+A\+A\+CL 2019.

\subsection*{Abstract}

A good conversation requires balance -- between simplicity and detail; staying on topic and changing it; asking questions and answering them. Although dialogue agents are commonly evaluated via human judgments of overall quality, the relationship between quality and these individual factors is less well-\/studied. In this work, we examine two controllable neural text generation methods, conditional training and weighted decoding, in order to control four important attributes for chitchat dialogue\+: repetition, specificity, response-\/relatedness and question-\/asking. We conduct a large-\/scale human evaluation to measure the effect of these control parameters on multi-\/turn interactive conversations on the Persona\+Chat task. We provide a detailed analysis of their relationship to high-\/level aspects of conversation, and show that by controlling combinations of these variables our models obtain clear improvements in human quality judgments.

\subsection*{Citation}

If you use the dataset or models in your own work, please cite with the following Bib\+Tex entry\+: \begin{DoxyVerb}@inproceedings{see2019what,
  author={Abigail See and Stephen Roller and Douwe Kiela and Jason Weston},
  booktitle={North American Chapter of the Association for Computational Linguistics (NAACL)},
  title={What makes a good conversation? How controllable attributes affect human judgments},
  url={https://arxiv.org/abs/1902.08654},
  year={2019},
}
\end{DoxyVerb}


\section*{Code Instructions}

Once you have \href{https://github.com/facebookresearch/ParlAI/#installing-parlai}{\tt installed Parl\+AI}, follow the instructions below.

\subsection*{Download the data}

Running the commands to train or chat with the models will automatically download the data for you. Alternatively, you can manually download the data by running {\ttfamily python \hyperlink{projects_2controllable__dialogue_2tasks_2build_8py}{projects/controllable\+\_\+dialogue/tasks/build.\+py}}. This will download the following files to {\ttfamily data/controllable\+\_\+dialogue}\+:


\begin{DoxyItemize}
\item {\ttfamily train.\+txt}\+: This is Convai2 training data, with extra annotations for three CT controllable attributes ({\ttfamily question}, {\ttfamily lastuttsim}, {\ttfamily avg\+\_\+nidf}). It is in parlai format.
\item {\ttfamily valid.\+txt}\+: Similarly to train.\+txt.
\item {\ttfamily arora.\+pkl}\+: This is a pickle file containing information necessary to compute Arora-\/style sentence embeddings, needed for the response-\/relatedness control methods.
\item {\ttfamily word2count.\+pkl}\+: This is a pickle file containing information necessary to compute N\+I\+DF measures, needed for the specificity control methods.
\item {\ttfamily personas\+\_\+validation.\+txt}\+: This file contains all the Conv\+A\+I2 validation set personas, provided for convenience (useful for talking to the model interactively).
\item {\ttfamily Conv\+A\+I2\+\_\+parlaiformat/}\+:
\begin{DoxyItemize}
\item {\ttfamily train.\+txt}\+: This is the Conv\+A\+I2 training set ({\ttfamily data/\+Conv\+A\+I2/train\+\_\+self\+\_\+original\+\_\+no\+\_\+cands.\+txt}) converted to parlai format.
\item {\ttfamily valid.\+txt}\+: This is the Conv\+A\+I2 validation set ({\ttfamily data/\+Conv\+A\+I2/valid\+\_\+self\+\_\+original\+\_\+no\+\_\+cands.\+txt}) converted to parlai format.
\end{DoxyItemize}
\item {\ttfamily wordstat\+\_\+files/}\+: This directory contains json files with generated output and automatic metrics computed for the various pretrained models.
\item {\ttfamily evaluation\+\_\+logs/}\+: This directory contains logs and evaluations from the human evaluations.
\end{DoxyItemize}

\subsubsection*{(Alternatively) Making the data yourself}

For reproducibility, in this section we provide the commands to create the data yourself.

{\itshape Note\+: Due to changes in Parl\+AI, there might be some small differences between the generated files obtained in this section, and the downloaded files in the previous section.}

First, convert the Conv\+A\+I2 data to Parl\+AI format\+: \begin{DoxyVerb}mkdir -p data/controllable_dialogue/ConvAI2_parlaiformat

python parlai/scripts/convert_data_to_parlai_format.py \
--task convai2:SelfOriginal:no_cands \
--datatype train:ordered \
--outfile data/controllable_dialogue/ConvAI2_parlaiformat/train.txt

python parlai/scripts/convert_data_to_parlai_format.py \
--task convai2:SelfOriginal:no_cands \
--datatype valid \
--outfile data/controllable_dialogue/ConvAI2_parlaiformat/valid.txt
\end{DoxyVerb}


Next, create {\ttfamily word2count.\+pkl}\+: \begin{DoxyVerb}python projects/controllable_dialogue/controllable_seq2seq/nidf.py
\end{DoxyVerb}


This will create a file called {\ttfamily word2count.\+pkl} in your {\ttfamily data/controllable\+\_\+dialogue} directory. It might take a while, especially the part when it goes through the Twitter dataset counting words.

Next, create {\ttfamily arora.\+pkl}\+: \begin{DoxyVerb}python projects/controllable_dialogue/controllable_seq2seq/arora.py
\end{DoxyVerb}


This will create a file called {\ttfamily arora.\+pkl} in your {\ttfamily data/controllable\+\_\+dialogue} directory. It might take a while -\/ in particular, if necessary it will download Glo\+Ve vectors and store them in {\ttfamily Parl\+A\+I/data/models/glove\+\_\+vectors}.

Next, create {\ttfamily data/controllable\+\_\+dialogue/train.\+txt} and {\ttfamily valid.\+txt}\+: \begin{DoxyVerb}python projects/controllable_dialogue/make_control_dataset.py \
--fromfile_datapath data/controllable_dialogue/ConvAI2_parlaiformat/train.txt \
--outfile data/controllable_dialogue/train.txt \
--controls question,lastuttsim,avg_nidf

python projects/controllable_dialogue/make_control_dataset.py \
--fromfile_datapath data/controllable_dialogue/ConvAI2_parlaiformat/valid.txt \
--outfile data/controllable_dialogue/valid.txt \
--controls question,lastuttsim,avg_nidf
\end{DoxyVerb}


This will create files called {\ttfamily train.\+txt} and {\ttfamily valid.\+txt} in your {\ttfamily data/controllable\+\_\+dialogue} directory.

\subsection*{The pretrained models}

Running the commands in the next section to chat with the pretrained models will automatically download them for you. In {\ttfamily data/models/controllable\+\_\+dialogue} you will find the following models, along with their {\ttfamily .opt} files\+:


\begin{DoxyItemize}
\item {\ttfamily twitter\+\_\+pretrained\+\_\+baseline}\+: A seq2seq model trained on the Twitter dataset.
\item {\ttfamily convai2\+\_\+finetuned\+\_\+baseline}\+: The {\ttfamily twitter\+\_\+pretrained\+\_\+baseline} model, after fine-\/tuning on the Conv\+A\+I2 dataset.
\item {\ttfamily control\+\_\+avgnidf10b10e}\+: The {\ttfamily convai2\+\_\+finetuned\+\_\+baseline} model, after adding parameters for CT specificity control (10 buckets, embedding size 10), and fine-\/tuned on the Conv\+A\+I2 dataset with loss\+\_\+\+CT as described in Section 5.\+1 of the paper.
\item {\ttfamily control\+\_\+questionb11e10}\+: Similarly to {\ttfamily control\+\_\+avgnidf10b10e}, except this is CT question-\/asking control (11 buckets, embedding size 10).
\end{DoxyItemize}

The directory also contains a dictionary file\+:


\begin{DoxyItemize}
\item {\ttfamily dict\+\_\+twit30k\+\_\+train\+\_\+split}\+: This is the dictionary used for all models.
\end{DoxyItemize}

\subsection*{Chat with the pretrained models}

This section provides the commands to talk to the model configurations described in the paper. You can refer to Table 5 in the paper to see how these commands correspond to the configurations described there.

Running any of these commands will also download the pretrained models, if necessary.

{\bfseries Talk to the greedy search baseline model\+:} \begin{DoxyVerb}python projects/controllable_dialogue/interactive.py \
-mf models:controllable_dialogue/convai2_finetuned_baseline \
--beam-size 1
\end{DoxyVerb}


{\bfseries Talk to the beam search baseline model\+:} \begin{DoxyVerb}python projects/controllable_dialogue/interactive.py \
-mf models:controllable_dialogue/convai2_finetuned_baseline
\end{DoxyVerb}


This setting uses beam size 20 by default.

{\bfseries Talk to the repetition-\/controlled (WD) baseline\+:} \begin{DoxyVerb}python projects/controllable_dialogue/interactive.py \
-mf models:controllable_dialogue/convai2_finetuned_baseline \
-wd extrep_2gram:-3.5,extrep_nonstopword:-1e20,intrep_nonstopword:-1e20
\end{DoxyVerb}


You can change the weights for these three WD repetition features to be any real number (positive or negative). Here {\ttfamily -\/1e20} represents -\/infinity. In addition, there are other repetition WD features you can use if you wish\+: see the keys of {\ttfamily W\+D\+F\+E\+A\+T\+U\+R\+E2\+U\+P\+D\+A\+T\+E\+FN} in {\ttfamily \hyperlink{controls_8py}{controllable\+\_\+seq2seq/controls.\+py}}.

{\bfseries Talk to the question-\/controlled CT model (with WD repetition control)\+:} \begin{DoxyVerb}python projects/controllable_dialogue/interactive.py \
-mf models:controllable_dialogue/control_questionb11e10 \
-wd extrep_2gram:-3.5,extrep_nonstopword:-1e20,intrep_nonstopword:-1e20 \
--set-controls question:7
\end{DoxyVerb}


Here {\ttfamily question\+:7} means the \textquotesingle{}70\% questions\textquotesingle{} bucket. You can set this anywhere between 0 and 10.

To talk to the \char`\"{}z=10 (boost)\char`\"{} version mentioned in the paper\+: \begin{DoxyVerb}python projects/controllable_dialogue/interactive.py \
-mf models:controllable_dialogue/control_questionb11e10 \
-wd extrep_nonstopword:-1e20,intrep_nonstopword:-1e20 \
--set-controls question:10 --beam-reorder best_extrep2gram_qn
\end{DoxyVerb}


{\bfseries Talk to the specificity-\/controlled CT model (with WD repetition control)\+:} \begin{DoxyVerb}python projects/controllable_dialogue/interactive.py \
-mf models:controllable_dialogue/control_avgnidf10b10e \
-wd extrep_2gram:-3.5,extrep_nonstopword:-1e20,intrep_nonstopword:-1e20 \
--set-controls avg_nidf:7
\end{DoxyVerb}


Here {\ttfamily avg\+\_\+nidf\+:7} means the 7th specificity bucket (where higher is more specific). You can set this anywhere between 0 and 9.

{\bfseries Talk to the specificity-\/controlled WD model (with WD repetition control)\+:} \begin{DoxyVerb}python projects/controllable_dialogue/interactive.py \
-mf models:controllable_dialogue/convai2_finetuned_baseline \
-wd extrep_2gram:-3.5,extrep_nonstopword:-1e20,intrep_nonstopword:-1e20,nidf:4
\end{DoxyVerb}


Here {\ttfamily nidf\+:4} means using the N\+I\+DF WD feature with weight 4. You can use any real number as a weight (positive or negative).

{\bfseries Talk to the response-\/relatedness WD model (with WD repetition control)\+:} \begin{DoxyVerb}python projects/controllable_dialogue/interactive.py \
-mf models:controllable_dialogue/convai2_finetuned_baseline \
-wd extrep_2gram:-3.5,extrep_nonstopword:-1e20,intrep_2gram:-1e20,intrep_nonstopword:-1e20,partnerrep_2gram:-1e20,lastuttsim:5
\end{DoxyVerb}


Here {\ttfamily lastuttsim\+:5} means using the response-\/relatedness WD feature with weight 5. You can use any real number as a weight (positive or negative).

Note that this this feature can take a while to load, especially the first time you run it. This is because we have to load the Glo\+Ve vectors from file.

{\bfseries Giving the bot a persona}\+: If you want the bot to have a persona when you talk to it, select one of the lines in {\ttfamily data/controllable\+\_\+dialogue/personas\+\_\+validation.\+txt} and prepend it to your first utterance. Alternatively you can write the persona yourself -\/ but make sure to use the same format.

{\bfseries Viewing top 10 beam search candidates\+:} If you want to see the top 10 candidates produced by beam search (rather than just the top 1), add the flag {\ttfamily -\/-\/verbose True}.

\subsection*{Train a CT model}

{\bfseries To train a CT model from scratch\+:} \begin{DoxyVerb}python projects/controllable_dialogue/train_controllable_seq2seq.py \
-mf /path/to/your/modelfile \
--control-vars avg_nidf
\end{DoxyVerb}


Here we are training a specificity-\/controlled CT model.

{\bfseries To change control embedding size\+:} The CT control embedding size will default to 10, but you could include e.\+g. {\ttfamily -\/-\/control-\/embeddingsize 15} if you wanted to change it.

{\bfseries To change number of buckets\+:} For {\ttfamily avg\+\_\+nidf}, the number of buckets will default to 10. If you want to use a different number of buckets, first you need to figure out what the N\+I\+DF lower bound should be for each bucket. Suppose you want 8 buckets. First run\+: \begin{DoxyVerb}python projects/controllable_dialogue/get_bucket_lowerbounds.py \
--num_buckets 8 \
--control-vars avg_nidf
\end{DoxyVerb}


and then copy and paste the provided lower bounds into {\ttfamily \hyperlink{controls_8py}{projects/controllable\+\_\+dialogue/controllable\+\_\+seq2seq/controls.\+py}}, similarly to the existing {\ttfamily A\+V\+G\+\_\+\+N\+I\+D\+F\+\_\+10\+B\+U\+C\+K\+E\+T\+\_\+\+L\+BS}. Then you can train a model with {\ttfamily -\/-\/control-\/num-\/buckets 8}.

{\bfseries To train a CT model on {\itshape multiple} controls\+:} \begin{DoxyVerb}python projects/controllable_dialogue/train_controllable_seq2seq.py \
-mf /path/to/your/modelfile \
--control-vars avg_nidf,question
\end{DoxyVerb}


Here we are training a model conditioned on specificity and question-\/asking.

{\bfseries To take an existing non-\/\+CT model and finetune it as a CT model\+:} First, run this command (in this example, taking the Conv\+A\+I2-\/finetuned baseline and adding specificity control)\+: \begin{DoxyVerb}python projects/controllable_dialogue/train_controllable_seq2seq.py \
-mf /path/to/your/modelfile \
--init-model models:controllable_dialogue/convai2_finetuned_baseline \
--add-control True \
--control-vars avg_nidf
\end{DoxyVerb}


This command will take the parameters saved in {\ttfamily -\/-\/init-\/model}, load them in the new model (which has randomly initialized weights for the new CT parameters), and then save that model to the given modelfile ({\ttfamily -\/mf}). It should be quick. Once that\textquotesingle{}s done, run this command\+: \begin{DoxyVerb}python projects/controllable_dialogue/train_controllable_seq2seq.py \
-mf /path/to/your/modelfile \
--add-control False \
--control-vars avg_nidf
\end{DoxyVerb}


You should see your new CT model training. Note\+: this is how the models in the paper were trained.

\subsection*{Look at generated output and automatic metrics}

Once you have downloaded the data, you will find a directory {\ttfamily wordstat\+\_\+files} in {\ttfamily data/controllable\+\_\+dialogue}. The json files in this directory contain the generated output computed on the Conv\+A\+I2 validation set, plus the corresponding automatic metrics. Each json file corresponds to a different model configuration.

Run the following\+: \begin{DoxyVerb}cd projects/controllable_dialogue
jupyter notebook
\end{DoxyVerb}


and then open up {\ttfamily inspect\+\_\+wordstats.\+ipynb}. Where it says {\ttfamily models\+\_\+dir}, enter the path to your {\ttfamily wordstat\+\_\+files} directory. You will be able to recreate the table of automatic metrics from the paper (Table 6), and explore the models\textquotesingle{} generated output.

\subsection*{Save generated output and automatic metrics to file}

If you want to generate json files like those in the previous section, run a command like this\+: \begin{DoxyVerb}python projects/controllable_dialogue/eval_wordstat.py \
-mf models:controllable_dialogue/control_questionb11e10 \
-wd extrep_2gram:-3.5,extrep_nonstopword:-1e20,intrep_nonstopword:-1e20 \
--set-controls question:7
\end{DoxyVerb}


This will create a json file containing the output and automatic metrics for the provided model configuration (here, question-\/controlled CT model with z=7 and WD repetition control). The script {\ttfamily eval\+\_\+wordstat.\+py} always places the json file in the same place as the model file. The script can take a while to complete -\/ so you can set e.\+g. {\ttfamily -\/-\/num-\/examples 512} to generate output on a smaller number of examples.

{\itshape Note\+: Due to changes in Parl\+AI, there might be some small differences between the json file created via this method, and the json files downloadable in the previous section.}

\subsection*{Human Evaluation code, logs, and analysis}

Human evaluation logs should be downloaded automatically after following the download instructions above. You\textquotesingle{}ll find them in the {\ttfamily evaluation\+\_\+logs/} folder.

A Jupyter notebook which generates the graphs and tables for the human experiments is available in the \href{https://github.com/facebookresearch/ParlAI/tree/master/projects/controllable_dialogue}{\tt project folder}. The notebook should be launched from the Parl\+AI root directory.

The code for running your own mechanical turk evaluations is also available in the corresponding \href{https://github.com/facebookresearch/ParlAI/tree/master/projects/controllable_dialogue/mturk}{\tt mturk folder}. You will probably want to make changes to the {\ttfamily model\+\_\+config.\+py} and {\ttfamily run.\+py} to change which models are being evaluated, and then you can launch the experiment with\+:


\begin{DoxyCode}
python parlai/mturk/tasks/controllable\_dialogue/run.py -r 0.9 --count-complete --hobby --max-resp-time 1200
       --max-connections 20 -nc 1200 --sandbox
\end{DoxyCode}
 Change it to {\ttfamily -\/-\/live} if you\textquotesingle{}re prepared to spend actual currency. The output must be lightly postprocessed to use it with the analysis tools released. If you intend to do this, please file an issue on the \href{https://github.com/facebookresearch/ParlAI/}{\tt Parl\+AI Git\+Hub}. 