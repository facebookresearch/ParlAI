\hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer}{}\section{parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer Class Reference}
\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer}\index{parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer@{parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer}}


Inheritance diagram for parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=235pt]{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=235pt]{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer__coll__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_aca18372fe25dbf5132ebcb2e63de31e3}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, attn\+\_\+type, hiddensize, embeddingsize, bidirectional=False, attn\+\_\+length=-\/1, attn\+\_\+time=\textquotesingle{}pre\textquotesingle{})
\item 
def \hyperlink{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a1c840562c4d3d0a715b4d5ddb1b8bafe}{forward} (self, xes, hidden, attn\+\_\+params)
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a8582ed1bb09a75d9a54e812a1b4ae648}{attention}
\item 
\hyperlink{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a38e130fe29ef52f03f78ba3c9a2ce4b9}{attn\+\_\+combine}
\item 
\hyperlink{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_ac886c3ee00cb9163fed352af8aa53e0c}{max\+\_\+length}
\item 
\hyperlink{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a5bbb3d93ee56f0a41562141beb302da7}{attn}
\item 
\hyperlink{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a2b5f04f72da768b45739b7e9f1dd6705}{attn\+\_\+v}
\end{DoxyCompactItemize}


\subsection{Detailed Description}
\begin{DoxyVerb}Computes attention between hidden and encoder states.

See arxiv.org/abs/1508.04025 for more info on each attention type.
\end{DoxyVerb}
 

Definition at line 551 of file modules.\+py.



\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_aca18372fe25dbf5132ebcb2e63de31e3}\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_aca18372fe25dbf5132ebcb2e63de31e3}} 
\index{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{attn\+\_\+type,  }\item[{}]{hiddensize,  }\item[{}]{embeddingsize,  }\item[{}]{bidirectional = {\ttfamily False},  }\item[{}]{attn\+\_\+length = {\ttfamily -\/1},  }\item[{}]{attn\+\_\+time = {\ttfamily \textquotesingle{}pre\textquotesingle{}} }\end{DoxyParamCaption})}

\begin{DoxyVerb}Initialize attention layer.\end{DoxyVerb}
 

Definition at line 565 of file modules.\+py.



\subsection{Member Function Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a1c840562c4d3d0a715b4d5ddb1b8bafe}\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a1c840562c4d3d0a715b4d5ddb1b8bafe}} 
\index{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}!forward@{forward}}
\index{forward@{forward}!parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{forward()}{forward()}}
{\footnotesize\ttfamily def parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+forward (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{xes,  }\item[{}]{hidden,  }\item[{}]{attn\+\_\+params }\end{DoxyParamCaption})}

\begin{DoxyVerb}Compute attention over attn_params given input and hidden states.

:param xes:         input state. will be combined with applied
            attention.
:param hidden:      hidden state from model. will be used to select
            states to attend to in from the attn_params.
:param attn_params: tuple of encoder output states and a mask showing
            which input indices are nonzero.

:returns: output, attn_weights
  output is a new state of same size as input state `xes`.
  attn_weights are the weights given to each state in the
  encoder outputs.
\end{DoxyVerb}
 

Definition at line 600 of file modules.\+py.



References parlai.\+agents.\+seq2seq.\+modules.\+R\+N\+N\+Decoder.\+attention, parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Decoder.\+attention, parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v1.\+R\+N\+N\+Decoder.\+attention, parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+attention, parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v1.\+Attention\+Layer.\+attention, parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer.\+attention, parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+attn, parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v1.\+Attention\+Layer.\+attn, parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer.\+attn, parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+attn\+\_\+combine, parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v1.\+Attention\+Layer.\+attn\+\_\+combine, parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer.\+attn\+\_\+combine, parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+attn\+\_\+v, parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v1.\+Attention\+Layer.\+attn\+\_\+v, parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer.\+attn\+\_\+v, parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+max\+\_\+length, parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v1.\+Attention\+Layer.\+max\+\_\+length, parlai.\+agents.\+legacy\+\_\+agents.\+seq2seq.\+modules\+\_\+v0.\+Attention\+Layer.\+max\+\_\+length, and parlai.\+agents.\+tfidf\+\_\+retriever.\+build\+\_\+tfidf.\+type.



Referenced by transresnet.\+modules.\+Transresnet\+Model.\+choose\+\_\+best\+\_\+caption(), transresnet\+\_\+multimodal.\+modules.\+Transresnet\+Multimodal\+Model.\+choose\+\_\+best\+\_\+response(), transresnet.\+modules.\+Transresnet\+Model.\+eval\+\_\+batch(), and transresnet.\+modules.\+Transresnet\+Model.\+train\+\_\+batch().

Here is the caller graph for this function\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=350pt]{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a1c840562c4d3d0a715b4d5ddb1b8bafe_icgraph}
\end{center}
\end{figure}


\subsection{Member Data Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a8582ed1bb09a75d9a54e812a1b4ae648}\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a8582ed1bb09a75d9a54e812a1b4ae648}} 
\index{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}!attention@{attention}}
\index{attention@{attention}!parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attention}{attention}}
{\footnotesize\ttfamily parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+attention}



Definition at line 568 of file modules.\+py.



Referenced by parlai.\+agents.\+transformer.\+modules.\+Transformer\+Encoder\+Layer.\+forward(), parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+forward(), controllable\+\_\+seq2seq.\+modules.\+R\+N\+N\+Decoder.\+forward(), controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer.\+forward(), and parlai.\+agents.\+transformer.\+polyencoder.\+Poly\+Encoder\+Module.\+score().

\mbox{\Hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a5bbb3d93ee56f0a41562141beb302da7}\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a5bbb3d93ee56f0a41562141beb302da7}} 
\index{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}!attn@{attn}}
\index{attn@{attn}!parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attn}{attn}}
{\footnotesize\ttfamily parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+attn}



Definition at line 591 of file modules.\+py.



Referenced by parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+forward(), controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer.\+forward(), and parlai.\+agents.\+transformer.\+modules.\+Basic\+Attention.\+forward().

\mbox{\Hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a38e130fe29ef52f03f78ba3c9a2ce4b9}\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a38e130fe29ef52f03f78ba3c9a2ce4b9}} 
\index{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}!attn\+\_\+combine@{attn\+\_\+combine}}
\index{attn\+\_\+combine@{attn\+\_\+combine}!parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attn\+\_\+combine}{attn\_combine}}
{\footnotesize\ttfamily parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+attn\+\_\+combine}



Definition at line 583 of file modules.\+py.



Referenced by parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+forward(), and controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer.\+forward().

\mbox{\Hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a2b5f04f72da768b45739b7e9f1dd6705}\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_a2b5f04f72da768b45739b7e9f1dd6705}} 
\index{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}!attn\+\_\+v@{attn\+\_\+v}}
\index{attn\+\_\+v@{attn\+\_\+v}!parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{attn\+\_\+v}{attn\_v}}
{\footnotesize\ttfamily parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+attn\+\_\+v}



Definition at line 595 of file modules.\+py.



Referenced by parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+forward(), and controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer.\+forward().

\mbox{\Hypertarget{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_ac886c3ee00cb9163fed352af8aa53e0c}\label{classparlai_1_1agents_1_1seq2seq_1_1modules_1_1AttentionLayer_ac886c3ee00cb9163fed352af8aa53e0c}} 
\index{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}!max\+\_\+length@{max\+\_\+length}}
\index{max\+\_\+length@{max\+\_\+length}!parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer@{parlai\+::agents\+::seq2seq\+::modules\+::\+Attention\+Layer}}
\subsubsection{\texorpdfstring{max\+\_\+length}{max\_length}}
{\footnotesize\ttfamily parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+max\+\_\+length}



Definition at line 589 of file modules.\+py.



Referenced by parlai.\+agents.\+seq2seq.\+modules.\+Attention\+Layer.\+forward(), and controllable\+\_\+seq2seq.\+modules.\+Attention\+Layer.\+forward().



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
parlai/agents/seq2seq/\hyperlink{parlai_2agents_2seq2seq_2modules_8py}{modules.\+py}\end{DoxyCompactItemize}
