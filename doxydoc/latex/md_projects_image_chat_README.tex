Kurt Shuster, Samuel Humeau, Antoine Bordes, Jason Weston

Please see \href{https://arxiv.org/abs/1811.00945}{\tt Shuster et al. (2018)} for more details.

\subsection*{Abstract}

To achieve the long-\/term goal of machines being able to engage humans in conversation, our models should be engaging. We focus on communication grounded in images, whereby a dialogue is conducted based on a given photo, a setup that is naturally engaging to humans (Hu et al., 2014). We collect a large dataset of grounded human-\/human conversations, where humans are asked to play the role of a given personality, as the use of personality in conversation has also been shown to be engaging (Shuster et al., 2018). Our dataset, Image\+Chat, consists of 202k dialogues and 401k utterances over 202k images using 215 possible personality traits. We then design a set of natural architectures using state-\/of-\/the-\/art image and text representations, considering various ways to fuse the components. Automatic metrics and human evaluations show the efficacy of approach, in particular where our best performing model is preferred over human conversationalists 47.\+7\% of the time.

\subsection*{Dataset}

The Image-\/\+Chat dataset can be accessed via Parl\+AI, with {\ttfamily -\/t image\+\_\+chat}.

Additionally, the Parl\+AI M\+Turk tasks for data collection and human evaluation are \href{https://github.com/facebookresearch/ParlAI/tree/master/parlai/mturk/tasks/image_chat}{\tt made available} in Parl\+AI.

\subsection*{Leaderboards for Image-\/\+Chat Task}

\tabulinesep=1mm
\begin{longtabu} spread 0pt [c]{*{3}{|X[-1]}|}
\hline
\rowcolor{\tableheadbgcolor}\textbf{ Model }&\textbf{ Paper }&\textbf{ Test R@1  }\\\cline{1-3}
\endfirsthead
\hline
\endfoot
\hline
\rowcolor{\tableheadbgcolor}\textbf{ Model }&\textbf{ Paper }&\textbf{ Test R@1  }\\\cline{1-3}
\endhead
Trans\+Res\+Net M\+M-\/\+Sum, Res\+Ne\+Xt-\/\+I\+G-\/3.\+5B &\href{https://arxiv.org/abs/1811.00945}{\tt Shuster et al. (2018)} &50.\+3 \\\cline{1-3}
Trans\+Res\+Net M\+M-\/\+Sum, Res\+Net152 &\href{https://arxiv.org/abs/1811.00945}{\tt Shuster et al. (2018)} &40.\+6 \\\cline{1-3}
Trans\+Res\+Net M\+M-\/\+Sum, No Images &\href{https://arxiv.org/abs/1811.00945}{\tt Shuster et al. (2018)} &35.\+4 \\\cline{1-3}
\end{longtabu}


\subsection*{Pretrained Models}

We provide our best model trained with Res\+Net152 image features. To evaluate the model, specify the following command\+: \begin{DoxyVerb}  python examples/eval_model.py \
      -bs 128 -t image_chat
      -mf models:image_chat/transresnet_multimodal/model
      -dt test
\end{DoxyVerb}


Which yields the following results\+: \begin{DoxyVerb}  {'exs': 29991, 'accuracy': 0.4032, 'f1': 0.4432, 'hits@1': 0.403, 'hits@5': 0.672, 'hits@10': 0.779, 'hits@100': 1.0, 'bleu': 0.3923,
  'first_round': {'hits@1/100': 0.3392, 'loss': -0.002001, 'med_rank': 3.0},
  'second_round': {'hits@1/100': 0.4558, 'loss': -0.002001, 'med_rank': 2.0},
  'third_round+': {'hits@1/100': 0.4147, 'loss': -0.002001, 'med_rank': 2.0}}
\end{DoxyVerb}


Additionally, we provide an interactive script that you can use to view outputs of our pretrained model. Simply run the following command\+: \begin{DoxyVerb}  python projects/image_chat/interactive.py \
  -mf models:image_chat/transresnet_multimodal/model
\end{DoxyVerb}


\subsection*{Model Examples}



\subsection*{Citation}

If you use the dataset or models in your own work, please cite with the following Bib\+Text entry\+: \begin{DoxyVerb}@article{DBLP:journals/corr/abs-1811-00945,
author    = {Kurt Shuster and
           Samuel Humeau and
           Antoine Bordes and
           Jason Weston},
title     = {Engaging Image Chat: Modeling Personality in Grounded Dialogue},
journal   = {CoRR},
volume    = {abs/1811.00945},
year      = {2018},
url       = {http://arxiv.org/abs/1811.00945},
archivePrefix = {arXiv},
eprint    = {1811.00945},
timestamp = {Thu, 22 Nov 2018 17:58:30 +0100},
biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1811-00945},
bibsource = {dblp computer science bibliography, https://dblp.org}
}\end{DoxyVerb}
 