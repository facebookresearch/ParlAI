\hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer}{}\section{parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer Class Reference}
\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer}\index{parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer@{parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer}}


Inheritance diagram for parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=231pt]{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer__inherit__graph}
\end{center}
\end{figure}


Collaboration diagram for parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=231pt]{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer__coll__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a091dbfa93db9f2dfdaae3779c24aecb0}{\+\_\+\+\_\+init\+\_\+\+\_\+} (self, kwargs)
\item 
def \hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a479b83909bafee8db3f60c9d8fe69635}{tokenize} (self, text)
\end{DoxyCompactItemize}
\subsection*{Public Attributes}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a6d3d9c1c8632f0b1ec559b3bbef9fe60}{annotators}
\item 
\hyperlink{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a7124470b153c9ed25f1c7a1404dd8e1d}{nlp}
\end{DoxyCompactItemize}


\subsection{Detailed Description}


Definition at line 17 of file spacy\+\_\+tokenizer.\+py.



\subsection{Constructor \& Destructor Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a091dbfa93db9f2dfdaae3779c24aecb0}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a091dbfa93db9f2dfdaae3779c24aecb0}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}!\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}}
\index{\+\_\+\+\_\+init\+\_\+\+\_\+@{\+\_\+\+\_\+init\+\_\+\+\_\+}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}}
\subsubsection{\texorpdfstring{\+\_\+\+\_\+init\+\_\+\+\_\+()}{\_\_init\_\_()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer.\+\_\+\+\_\+init\+\_\+\+\_\+ (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{kwargs }\end{DoxyParamCaption})}

\begin{DoxyVerb}Args:
    annotators: set that can include pos, lemma, and ner.
    model: spaCy model to use (either path, or keyword like 'en').
\end{DoxyVerb}
 

Definition at line 18 of file spacy\+\_\+tokenizer.\+py.



\subsection{Member Function Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a479b83909bafee8db3f60c9d8fe69635}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a479b83909bafee8db3f60c9d8fe69635}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}!tokenize@{tokenize}}
\index{tokenize@{tokenize}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}}
\subsubsection{\texorpdfstring{tokenize()}{tokenize()}}
{\footnotesize\ttfamily def parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer.\+tokenize (\begin{DoxyParamCaption}\item[{}]{self,  }\item[{}]{text }\end{DoxyParamCaption})}



Definition at line 33 of file spacy\+\_\+tokenizer.\+py.



References parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer.\+annotators, parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+corenlp\+\_\+tokenizer.\+Core\+N\+L\+P\+Tokenizer.\+annotators, parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+simple\+\_\+tokenizer.\+Simple\+Tokenizer.\+annotators, parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+regexp\+\_\+tokenizer.\+Regexp\+Tokenizer.\+annotators, and parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer.\+nlp.



Referenced by parlai.\+core.\+dict.\+Dictionary\+Agent.\+act(), parlai.\+core.\+dict.\+Dictionary\+Agent.\+span\+\_\+tokenize(), and parlai.\+core.\+dict.\+Dictionary\+Agent.\+txt2vec().

Here is the caller graph for this function\+:
\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=350pt]{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a479b83909bafee8db3f60c9d8fe69635_icgraph}
\end{center}
\end{figure}


\subsection{Member Data Documentation}
\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a6d3d9c1c8632f0b1ec559b3bbef9fe60}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a6d3d9c1c8632f0b1ec559b3bbef9fe60}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}!annotators@{annotators}}
\index{annotators@{annotators}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}}
\subsubsection{\texorpdfstring{annotators}{annotators}}
{\footnotesize\ttfamily parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer.\+annotators}



Definition at line 25 of file spacy\+\_\+tokenizer.\+py.



Referenced by parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+entities(), parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+lemmas(), parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+tokenizer.\+Tokens.\+pos(), and parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer.\+tokenize().

\mbox{\Hypertarget{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a7124470b153c9ed25f1c7a1404dd8e1d}\label{classparlai_1_1agents_1_1tfidf__retriever_1_1tokenizers_1_1spacy__tokenizer_1_1SpacyTokenizer_a7124470b153c9ed25f1c7a1404dd8e1d}} 
\index{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}!nlp@{nlp}}
\index{nlp@{nlp}!parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer@{parlai\+::agents\+::tfidf\+\_\+retriever\+::tokenizers\+::spacy\+\_\+tokenizer\+::\+Spacy\+Tokenizer}}
\subsubsection{\texorpdfstring{nlp}{nlp}}
{\footnotesize\ttfamily parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer.\+nlp}



Definition at line 31 of file spacy\+\_\+tokenizer.\+py.



Referenced by parlai.\+agents.\+tfidf\+\_\+retriever.\+tokenizers.\+spacy\+\_\+tokenizer.\+Spacy\+Tokenizer.\+tokenize().



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
parlai/agents/tfidf\+\_\+retriever/tokenizers/\hyperlink{spacy__tokenizer_8py}{spacy\+\_\+tokenizer.\+py}\end{DoxyCompactItemize}
