This directory contains several implementations of a ranker based on a pretrained language model B\+E\+RT (Devlin et al. \href{https://arxiv.org/abs/1810.04805}{\tt https\+://arxiv.\+org/abs/1810.\+04805}). It relies on the pytorch implementation provided by Hugging Face (\href{https://github.com/huggingface/pytorch-pretrained-BERT}{\tt https\+://github.\+com/huggingface/pytorch-\/pretrained-\/\+B\+E\+RT}).

\subsection*{Content}

This directory contains 3 Torch Ranker Agents (see \hyperlink{torch__ranker__agent_8py}{parlai/core/torch\+\_\+ranker\+\_\+agent.\+py}). All of them are rankers, which means that given a context, they try to guess what is the next utterance among a set of candidates.
\begin{DoxyItemize}
\item Bi\+Encoder\+Ranker\+Agent associates a vector to the context and a vector to every possible utterance, and is trained to maximize the dot product between the correct utterance and the context.
\item Cross\+Encoder\+Ranker\+Agent concatenate the text with a candidate utterance and gives a score. This scales much less that Bi\+Encoder\+Ranker\+Agent at inference time since you can not precompute a vector per candidate. However, it tends to give higher accuracy.
\item Both\+Encoder\+Ranker\+Agent does both, it ranks the top N candidates using a Bi\+Encoder and follows it by a Cross\+Encoder. Resulting in a scalable and precise system.
\end{DoxyItemize}

\subsection*{Preliminary}

In order to use those agents you need to install pytorch-\/pretrained-\/bert (\href{https://github.com/huggingface/pytorch-pretrained-BERT}{\tt https\+://github.\+com/huggingface/pytorch-\/pretrained-\/\+B\+E\+RT}). If you have not installed, running the model will prompt you to run\+: 
\begin{DoxyCode}
## Basic Examples

Train a BiEncoder BERT model on ConvAI2:
```bash
python examples/train\_model.py -t convai2 -m bert\_ranker/bi\_encoder\_ranker --batchsize 20
       --type-optimization all\_encoder\_layers -vtim 30 --model-file /tmp/bert\_biencoder\_test --data-parallel True
\end{DoxyCode}


Train a Cross\+Encoder B\+E\+RT model on Conv\+A\+I2\+: 
\begin{DoxyCode}
python examples/train\_model.py -t convai2 -m bert\_ranker/cross\_encoder\_ranker --batchsize 2
       --type-optimization all\_encoder\_layers -vtim 30 --model-file /tmp/bert\_crossencoder\_test --data-parallel True
\end{DoxyCode}
 